{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "import multiprocessing as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medium_scraper import main,get_last_day_in_year,daterange,get_links_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "year = 2019\n",
    "tags=['data','python']\n",
    "idx = []\n",
    "start_date = date(2019,12,29)\n",
    "end_date = get_last_day_in_year(start_date)\n",
    "for tag in tags:\n",
    "    for a in daterange(start_date,end_date):\n",
    "        idx.append([tag,a])\n",
    "articles = []\n",
    "for tag,single_date in idx:\n",
    "    articles.append(get_links_articles(tag,single_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'author': 'Eden Au',\n",
       "   'link': 'https://towardsdatascience.com/4-numpy-tricks-every-python-beginner-should-learn-bdb41febc2f2?source=tag_archive---------0-----------------------',\n",
       "   'title': '4 NumPy Tricks Every Python Beginner should Learn - Towards Data Science',\n",
       "   'claps': 342,\n",
       "   'text': 'NumPy is one of the most popular libraries in Python, and almost every Python programmer has used it for arithmetic computation given its advantages. Numpy arrays are more compact than Python lists. This library is also very convenient with many common matrix operations implemented in a very computationally efficient manner.\\nAfter helping with colleagues and friends with their numpy problems, I have come with 4 numpy tricks that a Python beginner should learn. Those tricks would help you write more neat and readable codes.\\nBefore learning numpy tricks, make sure you are familiar with some Python built-in features in the following article.\\nFor an array arr, np.argmax(arr), np.argmin(arr), and np.argwhere(condition(arr)) return the indices of maximum values, minimum values, and values that satisfy a user-defined condition respectively. While these arg functions are widely used, we often overlook the function np.argsort() that returns the indices that would sort an array.\\nWe can use np.argsort to sort values of arrays according to another array. Here is an example of sorting student names using their exam scores. The sorted name array can also be transformed back to its original order using np.argsort(np.argsort(score)).\\nIts performance is faster than using built-in Python function sorted(zip()), and is arguably more readable.\\nBroadcasting is something that a numpy beginner might have tried doing inadvertently. Many numpy arithmetic operations are applied on pairs of arrays with the same shapes on an element-by-element basis. Broadcasting vectorizes array operations without making needless copies of data. This leads to efficient algorithm implementations and higher code readability.\\nFor instance, you can use increment all values in an array by 1 using arr + 1 regardless of the dimension of arr. You can also check whether all values in an array is larger than 2 by arr > 2.\\nBut how do we know if two arrays are compatible with broadcasting?\\nEach dimension of both arrays have to be either equal, or one of them is 1. They do not need to have the same number of dimensions. These rules are illustrated in the example above.\\nThe syntax for slicing a numpy array is i:j where i, j are the starting index and the stopping index respectively. For example, as mentioned in the previous article — 5 Python features I wish I had known earlier, for a numpy array arr = np.array(range(10)), calling arr[:3] gives [0, 1, 2].\\nWhen dealing with arrays with higher dimensions, we use : for selecting the whole indices along each axis. We can also use ... can select all indices across multiple axes. The exact number of axes expanded is inferred.\\nOn the other hand, using np.newaxis as shown above inserts a new axis at a user-defined axis position. This operation expands the shape of an array by one unit of dimension. While this can also be done by np.expand_dims(), using np.newaxis is much more readable and arguably more elegant.\\nDatasets are imperfect. They always contain arrays with missing or invalid entries, and we often want to ignore those entries. For example, measurements from a weather station might contain missing values because of sensor failure.\\nNumpy has a submodule numpy.ma that supports data arrays with masks. A masked array contains an ordinary numpy array and a mask that indicates the position of invalid entries.\\nInvalid entries in an array are sometimes marked using negative values or strings. If we know the masked value, say -999, we can also create a masked array using np.ma.masked_values(arr, value=-999). Any numpy operation taking a masked array as an argument will automatically ignore those invalid entries as shown below.\\nThank you for reading. If you are interested in Python, the following articles might be useful:\\nOriginally published at edenau.github.io.\\nWritten by\\n'},\n",
       "  {'author': 'Dominik Vacikar',\n",
       "   'link': 'https://medium.com/@chichikid/the-guys-who-will-solve-the-private-market-682058dbc6ae?source=tag_archive---------1-----------------------',\n",
       "   'title': 'The Guys Who Will Solve the Private Market - Dominik Vacikar - Medium',\n",
       "   'claps': 28,\n",
       "   'text': 'Two weeks ago, I was sitting in a coffeeshop with my co-founder Xavi. It was late at night. We’ve had a few drinks. And we started talking about all the crazy things we’ve done in the past.\\nI think the one story that really stuck with me was — when back in the day, Xavi couldn’t find a decent wifi connection in Japan. The only place in the neighbourhood which had a good internet connection was a porn shop that would only allow you to use their computer, if you rented 2 movies. Apparently there was no way around it. So if you wanted to use their computer, you would have to get 2 weird-ass movies, and only then you would be allowed to enter an even weirder cubicle which contained the computer.\\nMost folks would probably tap out at this point, but Xavi was working on an important project at the time. This was the only way to get it done. So he would go to the shop every day, get the two goddamn movies, enter the cubicle and start coding.\\nNow, you might be wondering what the hell does a Japanese porn shop got to do with private market. Quite a lot actually. Bear with me.\\nAt some point during the night, Xavi looked at me and in his typical nonchalant fashion asked me — “so what’s the vision, man?”\\nI was pretty tired at that point, so I replied — my #1 task as the CEO is to empower my team to do great things. So I gather talented people around me and generate opportunities. I don’t want to manage a huge team, because I firmly believe that tech-teams operate at their best when you consciously keep them small. Only if you look at how much we’ve built in the past few months in 3–4 people, it’s pretty nuts.\\nXavi’s reply was — “that’s not good enough.” I still tried to convince him of my tribe vs. company theory, but somewhere deep down I knew he was right.\\nRecently, I got a hold of the book ‘The Man Who Solved the Market’. It’s a fantastic write-up of the remarkable success of Jim Simons and Renaissance.\\n/quick summary/ the unbelievable story of a secretive mathematician who pioneered the era of the algorithm — and made $23 billion doing it.\\nThis book spoke to me on many different levels. So I forced everyone on the team to read it, because I firmly believe that what Renaissance did for the public market is what we are trying to do at Specter for the private market.\\nNow to put things into perspective, we are a 9-months old company, we don’t manage $100BN, and we are very very very far from what Renaissance managed to achieve.\\nHowever, so far it has been one hell of a ride. Our clients include most of the top-tier VC and PE funds around the world. For instance, half of the Forbes European Midas List is already using Specter. I would even argue that we are running one of the biggest data operations for the private market. NOT in terms of team-size, but in terms of how much data we actually have.\\nI’ve been obsessed with this topic for almost 10 years now. I’ve been ridiculed for it. I’ve been fired for it. And I’ve been rejected literally by hundreds of experienced investors who told me it’s a stupid idea that will never work. But somehow I kept going. Same as Xavi kept going to the porn shop every day, because there was something more important at stake than what others might think.\\nThe thing I learned over the years is that nobody is going to believe you, until you actually do it.\\nNobody really believed in Renaissance either. For almost 30 years.\\nYou see, the problem with solving the market is that it’s of course incredibly difficult. And to prove you have solved it, you have to produce a big enough outcome. People don’t care about the precision of your algorithms, but people do notice when you walk away with $23BN like Jim Simons.\\nThe second problem with solving the market is that you can’t do it without getting really scrappy. I mean you have to gather so much data at a scale that 99.9% of investment funds give up. That is also why firms like Specter are thriving right now.\\nSame as in Renaissance case —in order to solve the market, you have to pass exactly four levels:\\nLEVEL 1 : GET THE DATA - literally all the data you can get your hands on\\nLEVEL 2 : USE THE DATA- apply it in funky, many times completely irrational ways\\nLEVEL 3 : BREAK EVERYTHING (A 100 TIMES)- you can’t read a ten-step how-to article, so you will have to break things, learn from it and rebuild from scratch\\nLEVEL 4 : PRODUCE A MEANINGFUL OUTCOME- a tangible and undeniable result\\nThe issue that many people and investors don’t get is that in a single day you can fall from Level 4 all the way to Level 1. So you have to have the stamina and passion to pass these levels continuously. And sometimes it really hurts.\\nSecond (and the biggest issue) is that most folks underestimate how much does it take to pass Level 1. Especially for the private market. Every day there are roughly 143,000 newly launched businesses, 147,000 newly registered websites, and 6,100 newly launched apps. So a dataset of 10,000 companies won’t cut it, same as a dataset of 1,000,000 won’t cut it.\\nIt’s one hell of a task. Trust me, I know it. Everyone at Specter knows it. Same as we know that we haven’t even passed Level 1 yet. Even though, we track tens of millions of companies and websites.\\nSo coming back to Xavi’s question — what’s the vision, man?\\nXavi, we are going to solve the private market, man. And I don’t care if we have to do it from a Japanese porn shop. But we will be the guys who will solve the private market.\\n🎤🕳️\\nWritten by\\n'},\n",
       "  {'author': 'Farida Adamu',\n",
       "   'link': 'https://medium.com/@faridaadamu/hello-guys-good-whatever-time-of-day-it-is-in-your-part-of-the-world-and-happy-holidays-334b28f7f79e?source=tag_archive---------2-----------------------',\n",
       "   'title': 'Introduction to Machine Learning - Farida Adamu - Medium',\n",
       "   'claps': 114,\n",
       "   'text': 'Hello guys, good whatever-time-of-day it is in your part of the world. and happy holidays 😊\\nSo, early in December, I did a session on Machine Learning at a DevFest event m and I thought it will be nice to helpful my slides with you.\\nIt’s an introduction to machine learning for people with interests in machine learning or those wondering what it is all about. So, if you are a pro already, still read 😃\\nBefore we define ML and its applications, one thing is constant; Without Data there is no Machine Learning. Data is the beginning and infinity of Machine Learning.\\nNow that we have written that in stone, let’s get into it.\\nWhat is Machine Learning?\\nIt is simply the technology that lets computers write their own programs. The machine receives data as input and works towards an already known output, or predicts an output.\\nHow do you teach a machine to learn? Simply (or not so simply) feed it with data. This happens in different stages as we will see below.\\nRemember our first constant? There’s no ML without data. The first stage of any Machine learning project is gathering and preparing data i.e cleaning like you would do in a data analytics process. The quality of data gathering and preparation is a very crucial part of doing ML because what you feed the machine is what you get.\\nThe second stage of ML is choosing a model. Now, there are many kinds of of machine learning models but we will look at the two common ones;\\ni. Supervised learning Model; where an algorithm learns the relationship between given input (x)and a given output(y). It can be likened to a teacher supervising a student in test, the teacher knows the answer but expects the student to walk their way towards it.\\nii. Unsupervised Models; when an algorithm learns from unlabeled data and tries to find hidden patterns or structures, and make an inference on its own since it no output is given. For instance, trying to understand the spending patterns of a certain demography from their shopping records.\\nAfter choosing what model you want to use, training which is the most important part of the ML process happens, where the algorithm learns from the data. After training the algorithm, you evaluate the performance by testing with a different set of data to be sure it is working as it should. If there are any issues, you go on to tune the data and then evaluate again. After all of these is done, the algorithm is able to make inference based on everything it has learned from the data you provided.\\nThere are different machine learning methods to chose from. They all fall under the broader classifications of mL models. For instance, linear regression is a supervised model.\\nPeople are often conflicted about the difference between Artificial Intelligence and Machine Learning. ML is a kind of AI. it’s like Physics and the Laws of Motion; while AI is the field itself, ML is a specific way of solving AI problems.\\nThis is how far I can go for now, I hope this has helped you understand Machine Learning a little. I will continue to update the article as I gain more knowledge myself.\\nTell me in the comments if you think there’’s something I may have missed.\\nDon’t forget to share with your friends as well xoxo.\\nWritten by\\n'}],\n",
       " [{'author': 'Kim Larsen',\n",
       "   'link': 'https://towardsdatascience.com/fire-your-bi-team-c425f774bf6?source=tag_archive---------0-----------------------',\n",
       "   'title': 'Fire Your BI Team - Towards Data Science',\n",
       "   'claps': 149,\n",
       "   'text': 'The term “Business Intelligence” gained widespread popularity in the 1990s and was originally defined as “concepts and methods to improve business decision making by using fact-based support systems.” Clearly, this sounds like a strategically important function for any company.\\nSo why the harsh headline? While the description above may capture what BI teams did in the 1990s, that’s not what BI teams have been up to in the last decade. As adoption and sophistication of data analytics changed, BI teams became the bottom-feeders of the analytical ecosystem while data scientists received all the fun and glory. BI teams ended up being order-takers executing precise requests prescribed by people who do not do analytics for a living. This has severely reduced their impact.\\nBut it doesn’t have to be like this. And you don’t need to fire your BI team. We can restore BI’s place in the analytical ecosystem by changing their mandate and how they interact with stakeholders. This will lead to more impactful insights while ensuring that the analysts themselves feel professional ownership and purpose.\\nLet’s walk through why and how.\\nTo see what’s wrong with BI today just consider the common, ticket-based engagement model: The BI analyst receives a ticket, executes the request, returns the answer, gets feedback through more tickets, and the cycle continues. A conversation may not even take place.\\nNow, if you’re someone who works with BI teams you might think this type of engagement model is exactly what you want. Fair enough — it does sound efficient and organized.\\nBut trust me, it’s not what you want.\\nJust think about the nature of the work required to extract meaningful and actionable insights. Even if the goal is to keep the analysis simple, you always end up on an analytical journey that involves slicing the data by segments, creating baselines, constructing derived metrics, and often more. This requires ownership and creativity from the analyst.\\nBut in the BI engagement model, the analyst is merely following someone else’s train of thought. Ownership and creativity are lost.\\nSo what’s the solution here? First, always kick off any “BI” analysis or dashboard with these two questions: (1) what’s the problem we’re trying to solve? and (2) what actions are we looking to take? From there, the onus is on the analyst to seek the question behind the question and share insights as they surface. Tickets will be replaced with face-to-face meetings and a symbiotic partnership will be formed. And if people can’t find time for that, then the question wasn’t important to begin with.\\nIf you’ve gotten this far, you might think I’m suggesting to convert all BI analysts into data scientists. But that’s not the case. While ETL and data exploration are core to any team that turns data into cashflows, there are some key differences:\\nData scientists should (mainly) be focused on efforts that lead to “data products” based on advanced methods. This could be ML models that drive product recommendations, or pricing and matching algorithms.\\nThe focus of the analysts, on the other hand, might be to deliver a slide deck with strategic recommendations or a dashboard. In fact, I favor the term “strategic analytics” for these teams because it describes the purpose of their roles.\\nBoth functions — data science and strategic analytics — are tremendously important, have clear purposes, require different competencies, and offer rewarding career paths. Most companies need both teams, but they don’t need a BI team. So if you’re managing a BI team, it’s time to change their mandate and the stakeholder engagement model. And let’s retire the term “Business Intelligence” once and for all.\\nWritten by\\n'},\n",
       "  {'author': 'Dana Alibrandi',\n",
       "   'link': 'https://blog.dash.org/announcing-the-release-of-dash-platform-on-evonet-c5a94dee0e59?source=tag_archive---------1-----------------------',\n",
       "   'title': 'Announcing the Release of Dash Platform on Evonet - Dash Blog',\n",
       "   'claps': 459,\n",
       "   'text': 'Today, Dash Core Group is proud to announce our first release of Dash Platform on Evonet, the public testing environment for Evolution features. This release was made possible by the hard work and coordination of numerous members of Dash Core Group, and it’s my pleasure to introduce the capabilities of the platform on their behalf.\\nDash Platform functionality will be released in several phases to Evonet. The testing phases, along with what will be tested in each phase, is outlined later in this post. Our initial efforts will begin with the foundational components of the platform: those that applications and developers will access. Later phases will expand our testing to include light client support, enhanced security functionality, and masternode rewards / incentives. Note that Dash Platform is not a product with which most users will directly interact. Instead, users will interact through client applications that have their own user interfaces, such as the DashPay application. As such, participation in testing of Dash Platform will likely require some basic development skills.\\nNow, I will provide a brief overview of the platform, the value it provides to users, the feature set made available in this version, details about upcoming releases, how to help test Evonet, and a brief overview of our documentation.\\nDash Platform is a technology stack for building decentralized applications on the Dash network. Our goal for the platform is to facilitate frictionless value transfer while leveraging the strengths of the Dash network. These strengths include instantly confirmed transactions, stable project governance, and scalability. Now, thanks to the release of Dash Platform, businesses have tools to add metadata to payments, thereby allowing for more robust, user-friendly application experiences.\\nAdding metadata to payments is just one use case of the functionality that Dash Platform provides. For this initial release, as well as upcoming releases, Dash Platform’s core functionality is that of a decentralized storage platform. Application developers create data contracts, register them with the Dash network, store application data, and receive Dash network consensus on the state of that data. This results in a storage platform that is censorship resistant, fault-tolerant, and has zero downtime.\\nIn a broader sense, Dash Platform provides the missing data layer in the vision for a fully decentralized web. It is similar in functionality and goals to Ethereum Swarm and can be used in tandem with Ethereum dapps as a decentralized database. When compared with Swarm, Dash Platform stands out due to its fluid developer experience and fast block confirmation times on the Platform blockchain, which allow for changes to your application data to be confirmed and reflected on user interfaces in real time.\\nSince Swarm is only on testnet, there is no production-ready decentralized storage solution that provides developers with the functionality of a full featured database. In order to store smart contract data, developers typically turn to a technology like IPFS, which is designed primarily as immutable, content-addressed file storage. Due to its immutability, applications that use IPFS don’t benefit from consensus being performed against the state of their data. With Dash Platform, developers can now easily create user-friendly experiences, where users can mutate data while the platform simultaneously maintains a consensus-backed immutable record of those changes.\\nWithin the centralized web, it is most similar to a product like Firebase’s Cloud Firestore. The main benefits of using Dash Platform over Firestore are similar to the general advantages of decentralization: better security, ownership over data, reduced data silos, and enhanced transparency. Over time, Dash Platform intends to add more components to its stack so that developers can fully utilize the Dash network to facilitate trustless, disintermediated value exchange.\\nFor this initial release, Dash Platform will provide basic access to platform services and components. In doing so, our aim is to inspire new use cases, educate the public about capabilities, and generate momentum as we prepare upcoming iterative releases. The deliverables in this version consist of both higher-level and lower-level components. Higher-level components are aspects of the platform that developers will interact with directly. These include the following:\\nIn addition, lower-level components are underlying aspects of the platform that power its overall functionality. Most developers will not interact with these components directly, as they are abstracted away by DAPI and the Dash SDK. These components include:\\nThe flow of data between these components is illustrated by the following diagram:\\nThis release represents the first of a series of releases, each designed to incrementally introduce key components of the platform for testing. Once we’re confident in the performance of newly introduced components, we will move on to releasing the next set of components per the testing plan. The goal is to maximize the security, stability, and quality of the product that ultimately goes to mainnet, while starting a wider conversation between the Dash DAO and developers regarding how to best optimize the platform going forward. The features that will be tested in later phases include support for light clients, platform Proof-of-Service, masternode incentives, reward distribution, DAPI endpoints for Dash Core (Layer 1), DAPI SPV, and improved platform consensus. A summary of planned release phases is included in the next section.\\nBecause this release is focused on deploying several foundational components upon which developers may build test applications, an end user interface will not yet be available. However, in the coming weeks, we will provide a web interface by which users can test identities, names, and document submission through the browser. Ultimately, most end users will interact with platform functionality by using applications like the DashPay wallet.\\nTo foster trust with the wider Dash community and future platform developers, Dash Core Group is committed to iteratively releasing platform features leading up to a mainnet release. We have organized our remaining work into several phases, and each phase will contain one or more releases depending on the functionality we are delivering, as well as bug fixes required to complete testing in each phase. With Phase 1: Access to Platform Components being delivered alongside this announcement, the remaining release phases are outlined below:\\nThe goal of this phase is finalizing the security around storing and retrieving data across various environments. In that regard, the first course of action is to allow for 3rd party developers to safely register their application data schemas (i.e. data contracts), thereby opening up the platform for wider testing. Once that is done, we’ll add functionality for storing platform data in authenticated tree structures, providing proofs of data in DAPI responses, and using BLS for signing platform data. We will also add support for light clients and provide SPV functionality through DAPI.\\nNext, we will implement incentives that will encourage network participants (masternodes) to host platform components. This work includes enforcement of hosting requirements through a platform Proof of Service (PoSe) algorithm, collection of fees associated with data operations on the platform, and distribution of fees as rewards to masternodes. In addition, we will introduce recovery mechanisms for identities and establish the foundation for decentralized identities. In doing so, masternode owners will be able to better understand how rewards are accumulated and distributed in return for providing the platform as a service. Developers will be able to improve the UX of their identity implementations, and they will also be able to explore deeper use cases for identities beyond usernames.\\nLastly, we will optimize and polish the platform in preparation for a mainnet release. There will be improvements to the platform chain in order to handle chain halts and improve consensus efficiency. Without these improvements, the platform chain could stop producing blocks due to validator sets not being able to agree on the contents of a block. Feature flags will be added to smoothly introduce new features once the platform is live on mainnet, and we will conduct thorough security checks to ensure there are no vulnerabilities that put funds or data at risk.\\nAlongside Evonet, we are providing a new developer hub that contains documentation for building applications on Dash Platform. This documentation is broken down into several sections to maximize readability and encourage a speedy introduction to development. There are tutorials that are learning-oriented, where the outcome is the completion of some platform development task required for a successful application to function. There are also explanations that are understanding-oriented, where the goal is to obtain a deeper understanding of how platform components operate and interact with each other. For those looking for a deeper dive, we’ve also included our historical documentation, which contains details on how the Dash core protocol functions.\\nWith this release, developers will be able to test the following actions:\\nIn order to get started as quickly as possible, we’ve provided a Connect to Evonet tutorial which allows users to interact with the platform via DAPI. It will also be possible for developers to create their own devnets using platform components through the use of the Dash Network Deploy Tool. Privately created devnets don’t have any of the restrictions we’ve placed on Evonet, which means developers can create, register, and test any data contract beyond the DPNS and DashPay contracts.\\nWe encourage developers to experiment with all of the above and help improve the platform by reporting any issues. Issues can be reported on GitHub, in the Issue section for the associated repo. A comprehensive list of all platform repositories is included at the end of this announcement.\\nThis release to Evonet marks a significant milestone after years of concerted effort by Dash Core Group, as we designed, redesigned, and finally implemented the Evolution vision as it was originally conveyed. This release represents the early stages of a platform that aims to revolutionize value exchange and deliver on the promise of digital cash. The development team behind Dash Platform is driven to make development easy and accessible for everyone striving to realize the promise of blockchain. We look forward to your thoughts, comments, suggestions, and pull requests as the Dash DAO enters a new chapter in its history.\\nWritten by\\n'},\n",
       "  {'author': 'Elijah Meeks',\n",
       "   'link': 'https://medium.com/nightingale/2019-was-the-year-data-visualization-hit-the-mainstream-d97685856ec?source=tag_archive---------2-----------------------',\n",
       "   'title': '2019 Was the Year Data Visualization Hit the Mainstream',\n",
       "   'claps': 241,\n",
       "   'text': 'There’s always something going on in the field of data visualization but until recently it was only something that people in the field noticed. To the outside world, beyond perhaps an occasional Amazing Map®, Tufte workshop or funny pie chart, these trends are invisible. Not so in 2019, where data visualization featured prominently in major news stories and key players in the field created work that didn’t just do well on Dataviz Twitter but all over.\\n2019 saw the United States President amend a data visualization product with a sharpie. That should have been enough to make 2019 special, but the year also saw the introduction of a data visualization-focused fashion line, a touching book that uses data visualization to express some of the anxieties and feelings we all struggle with, as well as the creation of the first holistic professional society focused on data visualization.\\nWhen Donald Trump was elected, he framed and hung in the White House a map of the United States that implied he was elected by an enormous landslide. But as every frustrated data visualization expert pointed out, this map neglected to indicate that more people didn’t vote for Trump than did. The United States has had data-driven presidents before — Thomas Jefferson famously charted the crops the slaves of his plantation planted every year. But the United States has never had a president that cared more about the appearance of data than the data itself, until now.\\nThe critical thing to recognize is that it was the rhetorical value of the above geospatial data visualization and not its underlying dataset that was important to Trump. But it wasn’t the electoral map that cemented Donald Trump’s status as the first data visualization president because, critically, the map was representing the data (it was just doing so in a way that was misleading). It was this year in September when, confronted with an official map of the range of the effects of Hurricane Dorian — one that contradicted his claims about what states might be affected — he decided to draw onto the map an additional bit of range. The data didn’t support it and wasn’t even uncertain enough to allow it to be drawn with digital tools, but Trump knew if he could just change the visualization, that was all that mattered.\\nThis has been taken by many pundits as a sign that we live in a post-fact era but that’s short-sighted. Instead, public debates about the presentation of data increase the prominence of data visualization as a meaningful act. The previous way of looking at it, that you were just “showing the data” is naive and misleading and leads to products like Trump’s “Impeach This” map. The naive perspective that data visualization is just a final step to help people see the data ignores the importance of subtle steps like showing uncertainty as well as the necessity to design a product that engages the audience (something Trump does far better than many data visualization practitioners).\\nTrump is a sign of this, not a cause, and as we move forward in our practice we need to be more aware of how, for many people, the visualization is the data.\\nJust as there have been presidents before Trump who have shown charts, there have been books before Michelle Rial’s Am I Overthinking This that are filled with charts. But Michelle’s book, unlike the typical data visualization coffee table book, is not a collection of charts selected for their historical or design merits. Instead, she’s created a series of charts by hand in her inimitable style that highlights the contradictions, fears, and complexities of modern life in a way that text simply can’t.\\nData visualization as a way of exploring and expressing one’s feelings and traits has always been present in the margins of the field. Data-driven badges are always popular and Dear Data provided a nice model for thinking about one’s life and connecting with each other in a systematic way. Likewise, XKCD has often produced data visualization content. The Internet is littered with jokes like the pie chart made of a real pie, which have always proved popular with audiences. But Michelle’s book signals a fundamental shift toward the act of creating data visualization as a standalone way of imputing meaning, not as a gimmick or a one-off but fully engaged as the primary method for dealing with an increasingly data-intruded life.\\nThat’s probably why Michelle’s work is constantly shared without credit.\\nOur profession suffers from an implicit stratification that bubbles up into a disdain for those who use one method of doing data visualization over the other. Practitioners who use one tool think those who use another aren’t as good. Coders think people who rely on tools are less capable. People who write in one language or with one library think the others are worse. As a result, we see an overemphasis on learning technical skills over design.\\nExcept for Giorgia Lupi, who has throughout her career eschewed this entire line of reasoning to forge a path that touches on traditional data visualization, data art, design and data humanism. This year, Giorgia has tacked on two more significant achievements: She’s started a fashion line and joined Pentagram, the world’s largest independent design consultancy.\\nAs with the earlier themes, there have been examples of data visualization in fashion before this, most notably Rachel Binx’s continuing work to create jewelry and clothing based on data visualization. But it wasn’t until Giorgia’s efforts that this sort of thing began to be covered by mainstream media like Vogue.\\nIn 2019, SalesForce purchased Tableau for $15 billion and Google purchased Looker for nearly $3 billion. These are serious investments and are likely not the last of their kind. Both SalesForce and Google have already invested significantly in their own in-house data visualization tools but both realized that to compete they needed to rapidly expand their data visualization capacities and were willing to pay top dollar to do so.\\nData is increasingly important to all businesses, not just tech, and so much a part of our everyday lives that it makes sense that companies with strong data analysis, data science, and data engineering talent would feel the need to improve their data visualization capabilities. When data visualization is considered just a skill, it’s usually less important than the modeling, ETL design and analysis for the professionals. But we’re now seeing an acknowledgment that if you don’t have good data visualization then your insights are less apparent, resonate less with audiences and are harder to communicate among scientists.\\nAs its executive director, I recognize my bias, but the phenomenal success of the Data Visualization Society in 2019 clearly indicates a pent-up desire to bring together the profession and field in a holistic way. What was just an idea of three people back in February has grown to a 10,000 member organization with regular debates about important topics, an annual community survey, data visualization challenges, a thriving publication, social media influence and a growing slate of resources. Speaking from inside the process, I can say that it has been an enormous amount of fulfilling work.\\nData visualization is technically mature. The rise of new tools and technologies is less based on features around the core area of displaying graphics encoded with data attributes and more around improved UI/UX and, increasingly often, machine learning-based approaches to improve suggestions for which chart to use. As practitioners, this lets us shift our focus away from technical problems toward longstanding themes of underdevelopment of design and information modeling.\\nData visualization is becoming less of a tech company rarity and more a part of everyone’s everyday life. If you have a smartwatch, you see data visualization encoding your exercise routine and other details of your everyday life. And not just there, data visualization is all over in sleep tracking apps, weather reports that encode uncertainty, communications pieces like Spotify’s end of the year report, goal tracking apps, bullet journal habit tracking, diet/food tracking apps like fitness pal, bank statements and more. It will only continue to grow more common in the coming years.\\nData visualization is also its own standalone endeavor with more and more professional roles and whole organizations dedicated to it.\\nWhat we need to do next year is reexamine all our preconceived notions about what makes data visualization good and how to achieve it. We need to seriously rethink what we’re doing because what we’re doing has seriously changed. Modern data visualization is optimized for producing charts for busy executives. But that’s changing. Now, data visualization is personal stories, small businesses, data science, political campaigns, human resources, community building— in short, data visualization is becoming a part of the fabric that is modern culture. We need to throw away our old notions of data visualization and understand how this new data visualization is made, how it’s read and how it relates to itself.\\nElijah Meeks is the Executive Director of the Data Visualization Society and a Data Visualization Engineer at Apple. He’s the author of D3.js in Action and the creator of Semiotic, a React-based charting framework, as well as the Data Explorer found in the nteract notebook platform. He’s created data visualization products in academia and industry, including a stint at Netflix and also creating digital humanities works at Stanford.\\nWritten by\\n'},\n",
       "  {'author': 'Thomas K R',\n",
       "   'link': 'https://medium.com/swlh/the-ghost-in-the-machine-15-practical-steps-towards-becoming-anonymous-online-62f21cadc838?source=tag_archive---------3-----------------------',\n",
       "   'title': 'The ghost in the machine: 15 practical steps towards becoming anonymous online',\n",
       "   'claps': 184,\n",
       "   'text': 'Thumbing through social media? Checking out some new shoes online? Reading about President Cheeto’s latest failure? Nice. You’re screaming, at high-speed, through hundreds of connected computers - and you are being watched. In an economy that now considers your personal data more valuable than oil, the visibility of your online behaviour powers the entire earnings models of the biggest technology platforms on the planet — and well - business is booming.\\nSo, why is your personal data so important to businesses and governments around the world? A complicated question with long, and short answers.\\nThe most powerful businesses in the world are creating and leveraging a global storm of ongoing, precision, human-behaviour data in a bid to predict the future. Knowing exactly who, what and where people are online allows powerful, for-profit organisations to analyse mass human behaviour and profile entire slices of populations precisely — targeting them with the information and ideas that will resonate most deeply. For consumer brands, that might be a £4.99 phone case, or a £150.99 pair of shoes. For governments, it might be an entirely new political or economic system.\\nThis exhaustive set of personal data spans technical (devices), geographic (locations), economic (spending habits), emotional/personal (moods and public consensus), visual (image and identity), political (social and political affiliations and behaviours) — and more. This is a highly potent cocktail of often ‘private’ information that now lives in the hands of software companies and analytics companies looking to collate it and crunch it — and use the outcomes to manipulate human behaviour for their financial futures.\\nThis business model is what has become commonly (and chillingly) known as ‘surveillance capitalism’. It’s the jewel in the crown of our extraordinary digital revolution — and arguably, the concept that generates more technology revenue than anything else in the world.\\nShoshana Zuboff is likely the world’s foremost authority on digital surveillance and its role in society, and as an author and Harvard scholar, declares:\\n“Surveillance capitalism unilaterally claims human experience as free raw material for translation into behavioural data. Although some of these data are applied to service improvement, the rest are declared as a proprietary behavioural surplus, fed into advanced manufacturing processes known as ‘machine intelligence’, and fabricated into prediction products that anticipate what you will do now, soon, and later.\\nAnd further:\\n“The combination of state surveillance and its capitalist counterpart means that digital technology is separating citizens in all societies into two groups: the watchers (invisible, unknown and unaccountable) and the watched. This has profound consequences for democracy because asymmetry of knowledge translates into asymmetries of power.”\\nI highly recommend this excellent piece — which paints a picture of what surveillance capitalism is, how you’re part of it, and how crucial it’s become to digital models and political agenda.\\nYeah, pretty bad, probably the shiftiest business shit in history to be honest, apart from Boston Dynamics robots that are literally more athletic than most human beings will ever be, but at least they don’t track your location yet.\\nThe extent to which any individual can protect their privacy online is governed by a small number of important questions:\\nYou can control a very large amount of the data you transmit, and to who, but you can’t stop sending data. You will never be completely anonymous online. With even the best masking tactics, something is always trackable, somewhere — but you can pick and choose from a series of things that will completely overhaul your privacy game online.\\nTake a look at a few things you can do to help prevent your personal data falling into the wrong hands.\\nThere might already be a trail of public information about you on websites across the Internet. It’s a good idea to have a clear understanding of what that is, and how simple it is to find out about you online.\\nFor that reason, you’re going to need to start digging. This means:\\nIf there are prominent public images of you that identify you, you’ll also want to use Google Reverse Image Search (hit the camera icon) to find where else it’s stored online.\\nAn Alexa is quite literally a corporate listening device that you bought from the world’s richest technology company and records your life while it sits next to your fridge. Why don’t you just rent a bed in an Amazon warehouse like Big Daddy Jeffy wants.\\nOne of the most sophisticated technology projects in history, Facebook’s birth reimagined Google’s original lessons in data analysis and left an entire data economy in its wake - paving the way for some of the most flagrant data abuses of all time. Plus, it’s now widely recognised as an advertising platform that routes titanic amounts of commercial advertising and political propaganda through its network to its targetable user base of its 2.4 billion people—and is credited as a significant tool in the installation of authoritarian governments all over the world — Trump, and Johnson included.\\nSo it’s a fantastically powerful human indexing tool and you’re going to need to delete your account. And that includes their other products, too. Which means:\\nYou’ve just uninstalled WhatApp, which might mean you now have no way to contact anybody over the Internet. The good news is that not only do secure messaging options exist — there are some excellent ones available.\\nBoth Signal and Telegram are privacy-centric, popular communications platforms that have native privacy built-ins, such as self-destructing messages and secret chats. They’re pretty similar apps in essence, so it’s your choice as to what you prefer.\\nThis one is straightforward. The apps on your phone, tablet and computers run GPS software that uses the device’s hardware to trace and record their own locations. This is exploitable geographic data on your whereabouts for lots of different reasons (at the moment, usually used for super-targeted advertising). If you don’t like that, you’ll need to find where in those apps you can manage those settings and switch them off.\\nVirtual Private Networks are important parts of creating genuine online privacy. They create a private network from a public internet connection and are designed to mask your IP address — which is the public ‘home address’ of your computer, phone, or tablet - making your online actions much more difficult to trace back to your actual machine, and subsequently, your identity.\\nVPNs also persist secure, encrypted connections that provide much greater privacy from intrusion than a standard WiFi connection. Popular VPNs like NordVPN use military grade encryption, leak protection, no-logs policies and killswitches to protect your identity as thoroughly as feasibly possible. This is a lot of infosec jargon, but the theory is actually not very complicated - and there’s more at this great article.\\nA web browser is a piece of software you use to access the Internet and request data from the other computers connected to it (like Facebook’s servers, for example). When you’re using a browser, your computer is sending and requesting information between itself and lots of other machines — and that’s how the Internet moves information around. Chrome, Safari and Firefox are the most popular.\\nTor is the best known privacy-centric browser. It’ll let you connect to the Internet without doing any of the nefarious and overreaching stuff that the other commercial browsers do, and there’s good advice here.\\nIf you’re using Chrome (browser) and Google (search engine), it’s guaranteed that you’re leaking personal information all over the place about who you are, where you are, what you’re looking for and where you’ve been (geographically, and digitally).\\nPrivate search engines like Duck Duck Go are designed specifically to allow you to search the Internet without storing any of your personal data. If you want to stop Google, for example, threading together the data from Chrome, Google (Search), YouTube, its Ad network and more, you’ll need to abandon it for an anonymous browser.\\nProtonmail is a Swiss encrypted email provider by the CERN and MIT team — perhaps, the most technically capable ideas teams in the world. These sorts of products don’t have some of the bells and whistles of GMail, or others — but they’re free, secure and committed to better privacy standards than the commercial alternatives.\\nPart behavioural, part technical. Social media is intentionally addictive — but you may want to post as little as possible online, on any platform you use — particularly if it’s about you, your life or includes personally identifiable information.\\nIn the case that you do want to post images or media to the Internet and preserve some privacy, you’ll need to remove the metadata from each file. Simply put, metadata is automatically generated data about data (in this example, the latter is an image)— and includes things like the locations, dates and times of the ‘thing’ you’re posting. When downloaded by someone else, they can simply extract that data — which might not be what you want, if you don’t want to offer up where you’ve been, and at what time.\\nPut your photos through a platform like ExifRemove (although, there are lots of these services!) and it’ll wipe the metadata for you, before you do anything with the image.\\nIf you’re sharing files online, instead of using a consumer product like Dropbox, you may want to use a platform like OnionShare instead. It supports files of any size.\\nA ‘data breach’ is an instance of a hacker (or group), gaining unauthorised access to a computer system that your private information might be on. They’re common, but you don’t always find out about it, and if you’re using the same credentials for multiple sites, you’ve actually been breached elsewhere too.\\nHave I Been Pwned? is a website that catalogues data breaches and allows your to use your email address to find out if any of the accounts associated with it have been compromised. You’ll then know which of your passwords to change.\\nNow that you know if you’ve been hacked, you’ll know which passwords you’ll need to change. The truth is this though — in 2020, all of your passwords should be practically unhackable, unique, and managed through a third-party, like Lastpass. This means:\\nBeast mode:\\nIf you really want to, you can throw this process into hyperdrive by generating a temporary, burner email address for each service. It’ll expire quickly, and you’ll probably never be able to reset your password, but your personal email address won’t be tied to any of your Internet accounts and they won’t be able to reach you by email ever again, about anything, let alone track anything to your email identity.\\nThe greatest, and worst part of the Internet is its commerce model. Much like the physical world, purchase behaviours are easily traceable because they run through a highly documented ledger system of transactions and reconciliations. This is amplified enormously online — and along with your details, and the item, each transaction is accompanied by a staggering amount of metadata accumulated from your purchase. Often, your entire purchase cycle is recorded — from the first time you see an advert, until you eventually make the purchase - including where you were, what device you were on, and what else you might be interested in.\\nFor true anonymity, you need to completely stop shopping online, or pay for everything using a cryptocurrency like Bitcoin. But almost no mainstream providers accept Bitcoin, and its value is roughly as stable as my legs when I walk home from the bar at 2.45am having drunk 8 glasses of scotch to forget about the technocratic dystopia.\\nI’m not sure I’m prepared to stop doing either, really.\\nMost people can’t afford to replace their computers and phones, and wouldn’t if they could, because they aren’t a literal NSA agent. However, if you really wanna jam this baby into fifth, you can buy a dedicated secure phone, like a Blackphone (there are a bunch of these devices, available)— and a new hardware encrypted laptop and install either Tails or Whonix on it, rather than Windows or Mac OS.\\nYou can then do all of the above. It’s probably as close as you’re ever going to get to leaving no trace. Genuinely, good luck with that. You’ll need it, because it’s complicated.\\nThe best security and privacy policies are thorough, but practical. It’s for you to determine what you value and how important it is. Truthfully, doing all of this isn’t going to give you a particularly fun future Internet experience — but there might be certain tradeoffs worth making to elevate your level of privacy.\\nNot everyone values their data rights in the same way — but it’s important to think about the long-term effects of living highly publicly in a digital system that traces, records and capitalises on what can be permanent and deep digital footprints.\\nHave a day.\\nThomas K R\\nWritten by\\n'},\n",
       "  {'author': 'Brad Lindblad',\n",
       "   'link': 'https://codeburst.io/how-would-hemingway-write-code-cab7054560e0?source=tag_archive---------5-----------------------',\n",
       "   'title': 'How Would Hemingway Write Code? - codeburst',\n",
       "   'claps': 51,\n",
       "   'text': 'Probably with whiskey\\nHow many times have you looked back on code you wrote a few months back and thought, “what the hell was that?” I regularly scratch my head at code I wrote a few days prior, especially if I rationalized my spaghetti code away as a “scratch file.”\\nThere is objectively good code and bad code, in the same way there is good writing and bad writing. Writing good code isn’t all that different from writing good prose. Both good code and writing need to follow certain rules and best practices, as well as artfully solve many problems that lie outside the scope of rigid covenant.\\nGreat code is concise, glaring in its intent, and brief — following the DRY principle. There was no other writer who followed these coding practices to their natural conclusion in literature better than Ernest Hemingway. Hemingway wrote seven novels in the first half of the 20th century and won the Nobel Prize for Literature in 1954. He was the master of the simple sentence and the restrained narrative, developing this style as a young reporter.\\nMy thought experiment: how would Hemingway write code if he were a data scientist or a web developer? What can we learn from the techniques that crafted some of the best American literature? Let’s find out.\\nSamuelson: But reading all the good writers must discourage you. Hemingway: Then you ought to be discouraged.\\nIn the open-source world, we have free access to some of the best code ever written. All we have to do is open GitHub and browse the source code of our favorite projects.\\nIf Hemingway wrote R code like I do, he would have read all the Tidyverse “books” written by the phenomenal R scientist Hadley Wickham. These are the classics in the R community; many R developers regularly consult these repos when writing code for inspiration or ideas on how to solve a problem.\\nSince ancient times, most forms of art have had some kind of master-apprentice program. Rembrandt didn’t attend an online art school and immediately brush out The Nightwatch. He studied under a master to observe and absorb everything he could.\\nToday, the apprenticeship process is not so clearly defined but continues to exist in other forms. In 1934, an aspiring young Minnesotan named Samuelson hitchhiked to Hemingway’s casa in Key West to boldly knock on his door, putting his lot in Fate’s hands. Hem not only invited him over the next day, but he hired him to watch his boat for the next year, with a great deal of mentoring along the way.\\nAt one point Samuelson asked what he should read to become a great writer. Hem produced a long list of books which he pressed on Samuelson to devour.\\nSamuelson: Should a writer have read all of those?\\nHemingway: All of those and plenty more. Otherwise he doesn’t know what he has to beat.\\nOne of the reason’s Hem’s books are classics is because he wrote from his own experience. His time as an ambulance driver in Italy during the Great War fueled his masterpiece A Farewell To Arms. His love of the outdoors dripped onto the pages of The Old Man and the Sea. He didn’t doodle from imagination but from vivid memories and tactile experiences that the reader finds absolutely authentic.\\nBut how does a developer code from experience? Two words: business knowledge. If you’re designing an app to optimize freight deliveries for example, ride with a delivery driver for a week to gain a different perspective that your rubber duckie could never provide. Go out and learn the How and Why of what is desired, from the people who will actually use the thing.\\nHemingway: Whatever success I have had has been through writing what I know about.\\nHem wrote much of his 50,000+ word novels longhand, with pen and paper. Even though he could have written faster with a typewriter, he still crafted most of his first drafts the ancient way: applying carbon to pressed pulp.\\nModern science now tells us that writing longhand engages our brain more, helps us to be more creative, and increases comprehension — all benefits lacking from computer composition.\\nWhile the idea of writing code longhand may abhor you, especially if you lean a bit too heavily on your IDE’s intellisense, you’ll realize that you will Think before writing when the effort to write is greater, instead of thinking after typing.\\nI usually map out the salient chunks of my code on paper before even thinking of firing up my computer. I outline all the functions, map out which modules go where, etc., on whatever paper is handy. At that point, I’ll open my laptop and start carefully revising my ideas until I have something solid — a good first draft of the code. Next, I’ll work through one to three revisions, making changes and asking for feedback from peers. When I follow this practice, I definitely do NOT have the “WFT is this” feeling when reviewing it in the future.\\nIn the age of the full stack and 10x developer, we could take a cue from Hem on resting. In one letter he outlines his thoughts on what we now call the work-home balance:\\nHemingway: It is better to produce half as much, get plenty of exercise and not go crazy then to speed up so that your head is hardly normal.\\nHem would begin a new adventure after finishing a manuscript or go fishing in the Gulf of Mexico when he had writer’s block. He knew the importance of getting his head out of the game at regular intervals.\\nDevelopers take pride in the odd hours they keep or the late-night work sessions that empty their internal reservoirs. Hem knew that he only had at best a handful of productive hours in any one day, so he didn’t attempt to force work after the golden hours had waned.\\nDo you ever stop reading a book on a cliffhanger? Just for the excitement, so when you sit down to read again you’re already jacked to see what happens? Hem did the same thing when he was writing his books. His protégé Samuelson asked him when he should knock off writing for the day, to which he responded:\\nHemingway: The best way is always to stop when you are going good and when you know what will happen next. If you do that every day when you are writing a novel you will never be stuck...don’t think about it or worry about it until you start to write the next day, that way your subconscious will work on it all the time.\\nHow often do you carry your work home with you? intentional or not? Junior web developer Hemingway would discourage this behavior because he learned that if he left his work at work, the next morning his subconscious would have the answer he was looking for all ready for him.\\nHemingway: The first draft of anything is shit.\\nHow much of your code gets past the first draft? Maybe it’s missing a few comments, or large chunks are carelessly commented out. If you were a craftsman, creating furniture that will be around long enough for your grandchildren to use, would you skip the hand sanding, leaving rough corners and blemishes?\\nWhile it may be entertaining to imagine Hemingway resolving a merge conflict, we can learn much about the development process in general from the man.\\nFor further reading I suggest Hemingway’s For Whom the Bell Tolls as a good entry point, and Ernest Hemingway on Writing for more on his process.\\nOriginally published at https://technistema.com on December 30, 2019.\\nWritten by\\n'},\n",
       "  {'author': 'Jaime Goff',\n",
       "   'link': 'https://uxdesign.cc/design-and-data-how-to-humanize-data-32a03079311f?source=tag_archive---------8-----------------------',\n",
       "   'title': 'Design & Data: how to humanize data - UX Collective',\n",
       "   'claps': 17,\n",
       "   'text': 'A firefighter trying to get to a burning house on time hinges on the data system they are pulling from. A parent relies on teacher’s data to understand their child’s progress. Netflix looks at data to predict audiences for their next cringe-worthy holiday special.\\nData is any distinct information and is everywhere from Facebook to a local football game. Everyone lives in the data structures that underpin society. The design of the data structures, whether intentional or not, always impacts people.\\nThe people who live within the data structures and the decisions we make with data matter. Coherent products or services take into consideration people’s relationship to the data they use. As designers, we need to start thinking about data.\\nThe first step in working with data through a human-centered lens is to discover the humans who use the data. By exploring and pinpointing the needs of the people we can see how user needs can translate to data needs. The context around these needs should inform both the technical and design process.\\nIf you are working with an existing dataset, this looks like asking, “who uses this data?” Go and talk with the people who are getting the information. Be persistent in asking why they use the data and what their goals are with the data. Zoom out from specific data elements and understand the information people need and why.\\nCurrent data systems can be a starting point, but look for what kind of problems users have and the role data plays. Start uncovering what information do people need at the right time to achieve their goals. Give people a magic wand to imagine a world where they could have any information they wanted.\\nThis research will help understand what data people need in a more timely manner. Maybe even more important is capturing nuances around why they use it and the goals that they have.\\nAfter talking with users, it is helpful to visualize the stories you heard. The goal is to paint a picture of people impacted as a result of the data breakdowns. Humanize those affected by the unintended consequences of the design of the data.\\nData can be very siloed and sometimes political. It’s important to find a human-centered cause that all stakeholders call rally around. Visualize your research to not only show the human pain points, but also the business impacts. This intersection of business, human, and data needs will be a rallying point to gain buy-in.\\nThe idea of “data” can feel like a scary and overwhelming world to enter into as UX designers. Here are some practical ways to begin to engage with data:\\nThe first step with engaging with data is to advocate for the reality that design has a place in data. Designers must own that data shapes our lives. The decisions that we make and the data systems created will impact people.\\nAs designers, we have the privilege of shaping these design choices. When we sit in rooms with many voices, we have to be willing to advocate for the often silent voice in the room— that of the user. This means entering a world of design and data. A world full of ambiguity and not a lot of roadmaps. Our intention is doing no harm, helping each other and our users as best we can along the way.\\nDear Designers, we have to start thinking about data! Far too often, we’ve seen teams that assume data to be a technical issue reserved for someone else to figure out. This leads to technically-driven outcomes that don’t serve users. As designers, it’s our job to advocate for users, and that has to include data. Our digital worlds are swiftly evolving in new territory (it’s like the wild wild west out there!). We see in the news ethical questions swirling around data, but what do we do? Designers are equipped to help shape this data-driven future. How might we take a human-centered approach to data?\\nThanks for reading! I’m Jaime Goff and I’m Cara Tomko, we are UX Designers at ICF Next. We co-wrote this piece because we passionate about starting a conversation about the intersection of design and data.\\nWritten by\\n'},\n",
       "  {'author': 'Terence Shin',\n",
       "   'link': 'https://medium.com/swlh/predicting-life-expectancy-w-regression-b794ca457cd4?source=tag_archive---------9-----------------------',\n",
       "   'title': 'Predicting Life Expectancy w/ Regression - The Startup - Medium',\n",
       "   'claps': 56,\n",
       "   'text': \"Week 5 of 52\\nLinear regression is one of the most widely used approaches used to model the relationship between two or more variables. It can be applied anywhere, from forecasting sales for inventory planning to determine the impact of greenhouse gases on global temperatures to predicting crop yield based on rainfall.\\nIn this post, we’ll go over what linear regression is, how it works, and create a machine learning model to predict the average life expectancy of a person based on a number of factors.\\nAccording to Wikipedia, linear regression is a linear approach to modeling the relationship between a dependent variable and one or more independent variables. In simpler terms, it is the ‘line of best fit’ that represents a dataset.\\nBelow is an example of a line that best fits the data points. By creating a line of best fit, you can predict where future points may be and identify outliers. For example, assume that this graph represents the price of diamonds based on weight. If we look at the red dot, we can see that this particular diamond is overvalued because it costs much more given the same weight as other diamonds. Similarly, the green dot is undervalued because it costs much less than other diamonds with similar weights.\\nSo how do you find the line of best fit? Let's find out.\\nWe’re going to focus on simple linear regression. The line of best fit, or the equation that represents the data, is found by minimizing the squared distance between the points and the line of best fit, also called the squared error.\\nTo give an example, there are two ‘line of best fits’ shown above, the red line and the green line. Notice how the error (the green lines between the line of best fit and the plots) is much greater than the red line. The goal of regression is to find an equation in which the sum of the errors is minimized.\\nIf you want to know the math behind it, you can watch Khan Academy’s videos here, where they find the partial derivatives of m and b.\\nIf you want to use simple linear regression, you can use the LinearRegression class from the scikit-learn library.\\nSimple linear regression is useful when you want to find an equation that represents two variables, the independent variable (x) and the dependent variable (y). But what if you have many independent variables? For example, the price of a car is probably based on multiple factors, like its horsepower, the size of the car, and the value of the brand itself.\\nThis is when multiple regression comes in. Multiple regression is used to explain the relationship between a dependent variable and more than one independent variable.\\nThe image below shows a plot between income (y) and seniority and years of education (x). When there are two independent variables, a plane of best fit is found instead of a line of best fit.\\nWhat if you had a set of data where its line of best fit is not linear (like the image below). This is when you would want to use polynomial regression. Using Wikipedia again, it’s defined as a form of regression analysis in which the relationship between the independent variable x and the dependent variable y are modeled as an nth degree polynomial in x. In simpler terms, it fits a non-linear relationship between x and y.\\nWhen you want to use polynomial regression, a few extra lines of code are needed:\\nTo demonstrate how to build a regression model in Python, I used the ‘Life Expectancy (WHO) dataset on Kaggle here. My goal was to create a model that could predict the average life expectancy of a person in a given country on a given year based on a number of variables. Keep in mind that this is a very basic model — my next post will go through different methods to improve a regression model.\\nIn terms of prepping the data, I more or less followed the steps that I laid out in my EDA blog posts.\\nPart 1 here.\\nPart 2 here.\\nThere are a couple of new topics that I introduced in this model, like converting categorical data (countries) into dummy variables and evaluating the Variance Inflation Factor (VIF) of all variables. Again, I’m going to go through all of these new topics in my next blog post.\\nOne thing that I wanted to share was the correlation heatmap because there are some really interesting correlations here:\\nAfter cleaning the data, I executed the code above to create my polynomial multiple regression model with an MAE of 8.22 and a range of 44.4. In my next blog post, I’ll actually introduce several methods to improve a regression model (also applicable to most machine learning models) using the same dataset that I used here.\\nHappy Holidays! :)\\nWritten by\\n\"}],\n",
       " [{'author': 'Jonathan Hsu',\n",
       "   'link': 'https://medium.com/better-programming/stop-using-square-bracket-notation-to-get-a-dictionarys-value-in-python-c617f6ea15a3?source=tag_archive---------0-----------------------',\n",
       "   'title': 'Stop Using Square Bracket Notation to Get a Dictionary’s Value in Python',\n",
       "   'claps': 2200,\n",
       "   'text': 'A dictionary is an unordered set of terms and definitions. That means that:\\nTo define a dictionary, use curly braces and separate each term/definition pair with a comma.\\nThe traditional way to access a value within a dictionary is to use square bracket notation. This syntax nests the name of the term within square brackets, as seen below.\\nNotice how trying to reference a term that doesn’t exist causes a KeyError. This can cause major headaches, especially when dealing with unpredictable business data.\\nWhile we could wrap our statement in a try/except or if statement, this much care for a dictionary term will quickly pile up.\\nIf you come from a JavaScript background, you may be tempted to reference a dictionary value with dot notation. This doesn’t work in Python.\\nWhen you want to access a dictionary’s value, the safest way to do so is by using the .get() method. This method has two parameters:\\nWhen the term has previously been declared, .get() works no differently than a traditional square bracket reference. In the event that the term isn’t defined, a default value is returned that saves you from having to handle an exception.\\nThis default value can be anything you wish, but remember that it’s optional. When no default value is included, None— the Python equivalent of null — is used.\\nSometimes, not only do you want to protect from an undefined term in your dictionary, but you also want your code to self-correct its data structures. The .setdefault() is structured identically to .get(). However, when the term is undefined, in addition to returning a default value, the dictionary’s term will be set to this value as well.\\nIn the above example, we see that .setdefault() is the exact same as square bracket notation or .get() when the term exists. Additionally, it behaves the same as .get() when the term doesn’t exist, returning the passed default value.\\nWhere it differs from .get() is that the term and definition are now part of the dictionary, as seen below.\\nBoth .get() and .setdefault() are superior techniques when referencing dictionary values... it just may take some time breaking old habits and adopting the practice.\\nWhen you don’t want to alter the original data, .get() is your winner.\\nWhen you want to alter the original data, use .setdefault() and call it a day.\\nWritten by\\n'},\n",
       "  {'author': 'Eden Au',\n",
       "   'link': 'https://towardsdatascience.com/4-numpy-tricks-every-python-beginner-should-learn-bdb41febc2f2?source=tag_archive---------1-----------------------',\n",
       "   'title': '4 NumPy Tricks Every Python Beginner should Learn - Towards Data Science',\n",
       "   'claps': 342,\n",
       "   'text': 'NumPy is one of the most popular libraries in Python, and almost every Python programmer has used it for arithmetic computation given its advantages. Numpy arrays are more compact than Python lists. This library is also very convenient with many common matrix operations implemented in a very computationally efficient manner.\\nAfter helping with colleagues and friends with their numpy problems, I have come with 4 numpy tricks that a Python beginner should learn. Those tricks would help you write more neat and readable codes.\\nBefore learning numpy tricks, make sure you are familiar with some Python built-in features in the following article.\\nFor an array arr, np.argmax(arr), np.argmin(arr), and np.argwhere(condition(arr)) return the indices of maximum values, minimum values, and values that satisfy a user-defined condition respectively. While these arg functions are widely used, we often overlook the function np.argsort() that returns the indices that would sort an array.\\nWe can use np.argsort to sort values of arrays according to another array. Here is an example of sorting student names using their exam scores. The sorted name array can also be transformed back to its original order using np.argsort(np.argsort(score)).\\nIts performance is faster than using built-in Python function sorted(zip()), and is arguably more readable.\\nBroadcasting is something that a numpy beginner might have tried doing inadvertently. Many numpy arithmetic operations are applied on pairs of arrays with the same shapes on an element-by-element basis. Broadcasting vectorizes array operations without making needless copies of data. This leads to efficient algorithm implementations and higher code readability.\\nFor instance, you can use increment all values in an array by 1 using arr + 1 regardless of the dimension of arr. You can also check whether all values in an array is larger than 2 by arr > 2.\\nBut how do we know if two arrays are compatible with broadcasting?\\nEach dimension of both arrays have to be either equal, or one of them is 1. They do not need to have the same number of dimensions. These rules are illustrated in the example above.\\nThe syntax for slicing a numpy array is i:j where i, j are the starting index and the stopping index respectively. For example, as mentioned in the previous article — 5 Python features I wish I had known earlier, for a numpy array arr = np.array(range(10)), calling arr[:3] gives [0, 1, 2].\\nWhen dealing with arrays with higher dimensions, we use : for selecting the whole indices along each axis. We can also use ... can select all indices across multiple axes. The exact number of axes expanded is inferred.\\nOn the other hand, using np.newaxis as shown above inserts a new axis at a user-defined axis position. This operation expands the shape of an array by one unit of dimension. While this can also be done by np.expand_dims(), using np.newaxis is much more readable and arguably more elegant.\\nDatasets are imperfect. They always contain arrays with missing or invalid entries, and we often want to ignore those entries. For example, measurements from a weather station might contain missing values because of sensor failure.\\nNumpy has a submodule numpy.ma that supports data arrays with masks. A masked array contains an ordinary numpy array and a mask that indicates the position of invalid entries.\\nInvalid entries in an array are sometimes marked using negative values or strings. If we know the masked value, say -999, we can also create a masked array using np.ma.masked_values(arr, value=-999). Any numpy operation taking a masked array as an argument will automatically ignore those invalid entries as shown below.\\nThank you for reading. If you are interested in Python, the following articles might be useful:\\nOriginally published at edenau.github.io.\\nWritten by\\n'},\n",
       "  {'author': 'Jason Richards',\n",
       "   'link': 'https://towardsdatascience.com/python-efficiency-tips-old-and-new-tricks-for-the-aspiring-pythonista-6717dccd1a39?source=tag_archive---------2-----------------------',\n",
       "   'title': 'Python Efficiency Tips: Old and New Tricks for the Aspiring Pythonista',\n",
       "   'claps': 112,\n",
       "   'text': 'tqdm() is a wrapper used to instantly show a smart progress meter for your ‘for’ loops. It requires an install and an import, but if you ever had a loop that seemed like it was taking longer than it should, this could provide some needed insight.\\n2. Shift and Tab (Windows OS)\\nThis tip actually has saved me so many trips to Stackoverflow, yet I still forget to use it at times. Here are a couple of uses:\\n3. f-strings\\nFor those of you still utilizing str.format() or %d (replace with number), %s (replace with string), f-strings are more efficient. F-strings can be called anywhere within your code and can reference any previously instantiated variables. Here are a few examples of how to use them:\\n4. IPython Magic\\nPython has a built-in library called magic that allows the user to run specific commands just by leading with a % symbol. There are two forms of magic:\\nThere are many magic commands, with the most well known as %timeit (times your code) and %matplotlib inline. To get a full list of what commands can be called, just run the following cell:\\n5. Walrus operator (new to python 3.8)\\nFor those of you running 3.8 (notice: not all packages are ready for 3.8 as of this post) , here is a new operator that makes performing operations a little more efficient. The := operator, assignment operator, or walrus operator (as it looks like eyes with tusks) allows the user to assign a variable within the function or method specified. While there are several uses which can be exampled in the documentation, the simplest use case is exampled here:\\nThe hope here is that maybe you found something that will help make your Python a little more efficient or perhaps spark your interest enough to explore some more efficiency methods. Feel free to respond with any other efficiency tips. Happy holidays!\\nWritten by\\n'},\n",
       "  {'author': 'Rebecca Vickery',\n",
       "   'link': 'https://towardsdatascience.com/extracting-feature-importances-from-scikit-learn-pipelines-18c79b4ae09a?source=tag_archive---------3-----------------------',\n",
       "   'title': 'Extracting Feature Importances from Scikit-Learn Pipelines',\n",
       "   'claps': 281,\n",
       "   'text': \"Scikit-learn pipelines provide a really simple way to chain together the preprocessing steps with the model fitting stages in machine learning development. With pipelines, you can embed these steps so that in one line of code the model will perform all necessary preprocessing steps at the same time as either fitting the model or calling predict.\\nThere are many benefits to this besides reducing the lines of code in your project. Using the standard pipeline layouts means that it is very easy for a colleague, or your future self, to quickly understand your workflow. This in turns means that your work is more reproducible. Additionally, with pipelines, you can enforce the order in which transformations happen.\\nThere is however one drawback in that, although scikit-learn models have the benefit of being highly explainable. Once you embed the model into a pipeline it becomes difficult to extract elements such as feature importances that make these models so interpretable.\\nI have been spending some time recently looking at this problem. In the following article, I am going to present a simple method I have found to extract feature importances from a pipeline using the python library ELI5.\\nIn this article, I am going to be using a dataset from drivendata.org, a machine learning competition website. The dataset can be downloaded here.\\nFirst I’ll import all the libraries I am using.\\nI am then using the pandas library to read in the datasets that I have previously downloaded. The features and target labels are in separate CSV files so I am also using the pandas merge function to combine them into one data frame.\\nIf we inspect the data types we can see that there are a mixture of numerical and categorical data. We will, therefore, need to apply some preprocessing before training a model. A pipeline will, therefore, be useful for this dataset.\\nBefore constructing the pipeline I am dropping the ‘building_id’ column as it will not be needed for training, splitting the data into test and train sets, and defining some variables to identify the categorical and numerical columns.\\nI am going to construct a simple pipeline which will chain together the preprocessing and model fitting steps. Additionally, I am going to add an imputer for any missing values. Although the dataset I am using here does not have any missing data it is sensible to add in this step. This is because in the real world if we were deploying this as a machine learning application there is a chance that new data we are trying to predict on may have missing values. It is, therefore, good practice to add this as a safety net.\\nThe code below constructs a pipeline that imputes any missing values, applies a standard scaler to the numerical features, converts any categorical features into numerical and then fits a classifier.\\nWe can inspect the quality of the pipeline by running the below code.\\nWe can see that there is likely to be room for improvement in terms of the performance of the model. One area we would want to explore, besides model selection and hyperparameter optimisation would be feature engineering. However, in order to determine which new features to engineer we first need to have an understanding of which features are most predictive.\\nIt is not easy to extract feature importances from this pipeline. However, there is a python library that makes this very simple called ELI5. This library, named after the slang term “explain like I’m 5”, is a package that provides a simple way to explain and interpret machine learning models. It is compatible with most popular machine learning frameworks including scikit-learn, xgboost and keras.\\nThe library can be installed via pip or conda.\\nLet's use ELI5 to extract feature importances from the pipeline.\\nELI5 needs to know all feature names in order to construct feature importances. By applying one-hot encoding to the categorical variables in the pipeline we are introducing a number of new features. We therefore first need to extract these feature names and append them to the known list of numerical features. The code below uses the ‘named_steps’ function built into scikit-learn pipelines to do this.\\nTo extract the feature importances we then simply need to run this line of code.\\nWhich gives a nicely formatted output.\\nThe ELI5 library also provides the ability to explain individual predictions but this is not yet supported for pipelines. In this article, I demonstrated a simple method to extract features importances from a scikit-learn pipeline which provides a good starting point to debug and improve a machine learning model.\\nThanks for reading!\\nWritten by\\n\"},\n",
       "  {'author': 'Theresa Neate',\n",
       "   'link': 'https://medium.com/@theresaneate/my-2019-a-year-of-lows-and-highs-b5dc5a1128e7?source=tag_archive---------4-----------------------',\n",
       "   'title': 'My 2019, a year of lows, and highs - Theresa Neate - Medium',\n",
       "   'claps': 16,\n",
       "   'text': '2018: For reference, here’s what I concluded in 2018, including some of my commitments for the new year of 2019.\\nThis story begins at the beginning of 2019, takes a few detours back and forth and stops at some pitstops along the way, and ends with my trip overseas at the end of the year.\\nI started the year of 2019 in despair. My role of Senior Developer Advocate was not working out. I was not sleeping well. I was dreading going to work. I intensely loved the technical domain but I really struggled with how the role turned out to be, and with my experience with some key stakeholders.\\nThere is plenty of detail I am purposely not sharing here, but I will mention a highlight in my attempts to figure it all out: I was incredibly lucky to receive a 1:1 coaching session from the great Kelsey Hightower after I reached out to him with some questions on the Developer Advocate role:\\nYet, despite my efforts to the contrary, I was heading for a crash. Honestly, I loved the idea of the role, especially from the aspect of ‘developer relations’, but struggled with what I experienced in reality. I realised that I was behaving and feeling like the same early signs of a burnout I experienced just over 10 years ago.\\nI finally realised — driving home in tears after one particularly bad day — that I had become an easy target through my ‘caged dog’ behaviour, and thus that I needed to leave. I had really become a sitting duck. The ever-present-2019 lurgy started up then too, and hung around for most of the year.\\nSo, I started to look for other work.\\nPlease note: This is a reflection on my feelings, not so much on the minutiae of what people did or didn’t do. Some, although faulty in their execution from my perspective, really cared, and I still realised that through all the murkiness of it all. In respect to them and their intentions, I intend to not make this a smear campaign.\\nAs I was selectively and methodically interviewing externally but not yet finding something that put a glint in my eye, a role opened up internally at my employer. A role I had done before in another area of the company, and had done consistently since 2012: Lead QA. I thought I was done with this role but here it was being advertised in the area of the company that people routinely did well in, and on a bigger playing field, and still in the company that had been good to me on the whole.\\nMy manager, with whom I had been speaking regularly about my concerns, graciously accepted my reasons for seeking a change, and did not block my transfer request. He also gave me a positive review on my previous year, perhaps signalling that I had not in fact failed and that I really had tried my utmost, but possibly that the role implementation was at fault.\\nI applied in April and by May was interviewed for and offered — and accepted! — the role. I started my new role of Lead QA for the Consumer line of business, in June 2019.\\n(back to top)\\nI loved joining the Consumer Group (“Group”, as it was renamed from “Line of Business” after a September restructure) and I still love working there. It brought out in me, happy emotions again. Calmness, relief, a sense of belonging, people who asked for my opinion (and then didn’t disregard it) and feeling I could make a difference.\\nI felt SEEN. My manager made few to no assumptions about me and never tried to micromanage me; he asked a previous manager about me: how I work as employee and Lead QA, and then got to know me.\\nMy return to happiness was never about ‘returning to QA’ (which I never left, in my opinion), or the role of Lead QA. It was always about being seen and being appreciated. The management style, culture and the team I was working in, made all the difference.\\nPeople asked me occasionally about how I was doing and I replied for the first 3 months, that I was having a honeymoon period and my rose-tinted glasses would be biased, and they should ask again in 3 months’ time. But despite a stressful restructure in September, to this day I remain happy with the trust and treatment I am receiving.\\nBeing mildly superstitious and at the same time realistic after many years in the business, I don’t want to jinx this, and I know too that nothing lasts forever. But I will continue to enjoy what I can, and will use the experiences of this year as points of education towards improved wisdom.\\n(back to top)\\nThrough my inclusive and proactive DevOps Girls Co-Organiser, Franca (a platform engineer at SEEK), she let me know about her colleague Dr Pam O’Shea, Security Consultant and founder of haXX, who was going to run a Pen Testing workshop to women, over fortnightly Tuesday nights early in 2019. I quickly applied and a few weeks later, squealed with joy when I learned I was selected from the overcapacity applications.\\nI attended most of the 8 sessions from February to April, being fed delicious pizzas each night by SEEK who sponsored it; missing 1 due to the ever-present-2019 lurgy, and 1 due to attending the AWS Summit in Sydney. I was introduced to a whole new community, made several connections, and learned bucket-loads about security and pen testing, aligning neatly with my existing ops, networking and QA mindset. I was definitely awestruck with Pam herself, her awesome connections and their individual and collective knowledge.\\nHere’s my tweet from week 2 of 8, with an image of our curriculum:\\nI learned in December 2018 about the 0xCC training conference in April 2019, the first of its kind in Australia. Alannah, a pentester by day, organised this herself, enlisted other women in infosec to volunteer their time in running 2-day hands-on workshops over one weekend, which were offered free to other women in tech/infosec.\\nI enrolled in the “Snake Charming for Beginners” workshop, as it covered Security in Python, both topics I wanted to know more about. I missed the second half of day 2 as I was again taken down by the ever-present-2019 lurgy. This was still an incredible experience and I continue to have access to the materials and wish to do this again in 2020.\\nLidia had been my colleague at work since 2018 as Senior Security Consultant, and we got along great, often aligning on assessment of, and how to tackle problems as we saw them. Turns out, she is also the Melbourne organiser of the no-fuss BSides infosec conference.\\nAfter I enlisted her to speak at TConf in 2018 under the DevOps Girls banner (see TConf further below), we then teamed up DevOps Girls with BSides as partners for 2020. Although this is more appropriate for my 2020 reflection post, I want to call out Lidia for being awesome and already gifting DevOps Girls 5 free tickets (1 of which I gladly accepted) for the conference next year.\\n(back to top)\\nI really extended myself when I applied to speak, and was accepted for Agile Australia 2019. The topic was not my comfort zone, the audience & conference format neither. This was a business audience, several thousand attendees, with multiple tracks offered simultaneously. In June I spoke on the topic of developer experience and its place in and connection to CX. The talk went ok, not spectacularly but ok, and I am glad that I did this. (you can find these, and all my other slides and artefacts at theresaneate.com).\\nI have been a co-organiser of DevOps Girls since Dec 2016, and continue to be to this day. In 2019 I offered two workshops (in Aug & Oct), both geared towards a QA audience: DevOps Girls in Testing. I created the workshop content from scratch, and changed the format slightly from teacher-paced, to being self-paced. I think this really worked and I saw women stay on a topic for as long as they needed, and some women flying ahead into the optional material. I used this same material, tweaked slightly, for my TestBash workshop.\\nMy co-organiser Franca, in Feb & Nov 2019 also created and ran 2 workshops, first on AWS Lambdas (serverless) & then on Cloud Networking, the last being a subject close to my heart. Here’s Franca’s write-up on how the November event went: https://medium.com/seek-blog/seeking-networks-in-the-cloud-with-devops-girls-1d91d531a3fd.\\nFrom Aug-Nov, I also coached 5 first time women-in-tech speakers, to speak at TConf 2019 under the DevOps Girls banner, but more about that below in the section on TConf.\\nIn March this year I did a 1-hour webinar with Software Test Professionals, based in the US, with a wide international audience. Our host was Smita Pandey Mishra in Delhi, with my co-panellist being Hilary Weaver-Robb in Detroit.\\nThen, in August 2019, I did a 30-mins podcast with the Ministry of Testing in preparation for my TestBash talk and workshop (see below). I got to speak with Michael Lang from the UK, who was hosting his first podcast interview.\\nBoth of these were excellent experiences, despite crazy timezone issues.\\nMy ally and women-in-testing colleague and very famous testing person Anne-Marie Charrett invited me to apply to the most awesome TestBash conference, in Sydney. I did, and both my workshop and talk were — to my delight — approved. TestBash are the worldwide testing conferences run by the Ministry of Testing, a phenomenon of testing excellence and kindness and fairness. They also paid for my travel and accommodation, so again I say: kindness and fairness.\\nI really enjoyed teaching my workshop on Thursday morning, then attending a Visual Validation / Applitools workshop by the most excellent Angie Jones on Thursday afternoon, and doing my talk on DevOps Mythbusting on Friday. I met some super cool, super smart, kind people, who I hope to remain in touch with and cross paths with again in the not too distant future.\\nHere’s a pic of the fabulous workshop crowd:\\nI am a fan of TConf. Beginning initially 10 years ago with the MST meetup group, this community then grew into the TConf conference too. I spoke at both the 2017 and 2018 conferences (but didn’t apply to speak in 2019). Both organisers are Melbourne testing/QA veterans, respected by the community and people I like to call friends. Their meetups and conferences are wholesome, technical, and relevant. And both Ray and Scott are A+ smart and kind people, and I enjoy collaborating with them.\\nIn 2018, Scott reached out to me as DevOps Girls co-organiser, to ask me for advice on speaker gender equality, which he really wanted to achieve at TConf. After a bit of thought, I proposed to coach new (women) speakers to speak, and host them as first time speakers in the form of lightning talks. So, over 8 weeks, after first selecting them, I took completely new speakers and put them through coaching to gradually be ready to a deliver a ~7 minute talk to an audience of 400/500. There were rave reviews in the feedback we received afterwards.\\nSo we did this again in 2019. I put out a call for speakers in August, selected the 5 deserving women (based on their contributions to others) by end of August, and started the coaching. Again, I offered my services for free. 8 sessions of coaching from initial idea selection, to creating content, to delivering the talk, to consistency, to breathing, to how to cope with the inevitable screw-ups. This year’s coaching was harder for me than the previous year, mostly because of flaky commitment from the participants. I worked harder than anticipated, to get the show off the ground. If I do it again, I will set stricter rules on participation. On the day, they delivered their talk spectacularly well, to even my and their surprise! Here’s a pic we took after the talks’ conclusion:\\n(back to top)\\nI graduated my diploma, finally!\\nAfter 2 years of juggling part time day time study with full time work, forcing down my throat some outdated (and I daresay, unwanted) Win Server material, but still finding joy in other subjects and especially the Linux material, managing to ‘weasel’ my way into another class’ online subject so that I don’t have to wait for mine next year, applying for and being granted several credits, it was all done.\\nIn December 2019 I received notification that I had graduated with a GPA of 3.7. I would have had 3.8 if it weren’t for the one subject I scored under 80% in: my lowest score, still a near-high-distinction with 77% in “Security in Win Server 2012”.\\nSince my return to IT and thanks to the benefits of the Australian education system afforded to my permanent residency & subsequent citizenship, I have been studying part time non-stop since 2007. 3 diplomas, several short courses, half a bachelors, and 1 grad cert later, I might just take a break now from tertiary education. This last one was hard to swallow, there was too much material I had to ‘take on the chin’.\\nMaybe in the future I’ll do a Masters and/or PhD in Networking or Security or similar, but not right now. I am tired. Relieved, but tired, and slightly disappointed in the system. Now I will focus on my health & fitness again.\\nThanks again to my 3 managers and my employer over these 2 years. You really supported me in this quest. There will yet be a place for QA-Ops, or OpsQA, or full-OSI-continuous-QA.\\n(back to top)\\nEvery year I try to visit a new place in Australia, to get to know my adopted home better.\\nThis year it was Brisbane.\\nYes I had never actually visited Brisbane, beyond an airport transit once. I visited for 3 nights and was very pleasantly surprised how likeable it is. In my opinion, far nicer and bigger than Perth which I had seen several times (although still too humid for me, of course). Brisbane was simply beautiful, not too small, and really enjoyable. Here’s a pic from near my hotel over dinner with a friend:\\n(back to top)\\nAt the end of May I received an invitation to attend my mentee, Preeti’s, wedding in India to her fiance and all-round great guy and my former colleague, Miles.\\nI was honoured to be invited, and accepted.\\nAs a planner and researcher, I started planning. I renewed my passport, got a visa (what an experience), booked flights to and from India via Singapore, for migraine mitigation ensured I booked airport hotels too, bought my Christmas present to myself in the form of a stay at the Marina Bay Sands, booked a few nights either side of the wedding in India, and ...\\n... enrolled into Bollywood Dancing lessons!\\nI knew I had no inkling of what to expect at an Indian wedding, as famous as they are for their colour and festivities, least of which the dancing and music. So I researched and found an Indian dance studio right around the corner from work (who knew??).\\nI enrolled into Bhangra lessons for one term and bombed out quickly when I missed a few classes due to the ever-present-2019 lurgy, and work restructure stress. I went back after a few weeks but for more casual “Bollyfit” dance classes, and attended 10 of them until just before I departed for India in early December. I learned A LOT and even got some Bhangra moves (including dynamic squats) under my belt.\\nThen off to India I went. The trip was RAD. Every day was a firehose of education.\\nMy final destination via Delhi airport was on the east coast, in the state of Odisha (formerly Orissa), where I stayed for 5 nights. The wedding occurred on days 2–3 of my stay, with buffer days either side for shopping for outfits, and one day for sightseeing the temples.\\nDescribing the wedding deserves another post on its own, I cannot capture it here. I met some amazing people and wore some spectacular outfits, if I dare say so myself. I’ll leave you with a pic of the most complex, and beautiful, outfit:\\nI then treated myself to a “recovery stay” for a few nights in Singapore. This was my Christmas present to myself. In Singapore, I finally got some sleep as my adrenaline started coming down. I also failed to notice then (only later) that the ever-present-2019 lurgy was not there (anymore).\\nSingapore, and The Marina Bay Sands, were INCREDIBLE. Another topic that deserves a post on its own but I will definitely be back for a longer stay and more exploration at another time.\\nUpon my return to Australia near the end of the year I shared some pictures of my India outfits with my dance teacher Joshinder, who loved them. Once I have the video of our Sangeet (pre-wedding party) group dance, I’ll share that with her too. I think she’ll approve. We, the ~25 Aussies who danced as a group, did GREAT.\\n(back to top)\\nThat concludes the look at the year that was.\\nHere’s where I look back at where my head was at, end of 2018, and what I thought I’d be doing in 2019:\\nMy Python skills improved for sure. Have not achieved fluency yet, but found a study partner at work, and we caught up once a week since May 2019, doing 1 or more lessons per week, from the course “Python Programming for Developers”. This, plus the aforementioned “Snake Charming for Beginners” workshop, plus a full day workshop on “Python Web Test Automation”, helped my Python skills improve. Not fluent yet, but getting better. Unfortunately, I did not manage to fit in any AWS certifications, although I keep my skills semi-refreshed by creating the DevOps Girls workshops. I call this one, half accomplished.\\nThis got worse in 2019, especially towards the end. I got sick a lot more often with the ever-present-2019 lurgy, gained a few more kilos, didn’t run another 5K (despite registering for 2 races) — yes, due to sickness. I did make it back to ice hockey though, not playing great, but doing ok. Just very casual recreational women’s development league and training, not serious league. Being a little older than all my opposition players and team mates, I am now choosing to take it easier on my body. This one is deemed: not accomplished.\\nDefinitely achieved a new skill through Bollywood dancing lessons. I wanted to learn more dancing, and I did. Didn’t get to drawing or language lessons this time. But one new skill a year, is ok. So, this one is accomplished.\\nOK, I didn’t get to Iceland or Canada, top of my bucket list, or New Zealand again (which I love). But I did get to both Singapore and India for the first time (beyond just the Singapore transit lounge, for a change). So I call this one, well accomplished.\\nDone. Brisbane was lovely, and I’m definitely going back, preferably in June/July at the height of winter :)\\n(back to top)\\nMore or less in order of priority:\\nThat’s weight, fitness, migraines (reduction) and blood pressure. Sort it out, all of it. Nothing has higher priority. (Already turned the corner with a few kgs loss since my worst peak, and resting HR dropping. More to go. )\\nInstead of several speaking gigs, maybe only 1 — and be really selective about it. How about a keynote this time, or an international one with expenses paid? Yes, that selective. Instead of 3 workshops, and 8 weeks speaker coaching, and a webinar and podcast, do 1 or 2 only, on material I personally adore and want to get better in for my own benefit. Definitely no simultaneous or long-term study commitments this next year.\\nThis is the year, it’s the third time I am vowing to do it, this is my training priority this year. Solution Architect, I’m coming for you.\\nI definitely want to keep my Linux skills sharp, maybe through the Linux Foundation, starting here: https://training.linuxfoundation.org/training/introduction-to-linux/.\\nFinish the course I am currently doing, which I have just restarted a week ago over the holiday break. Once it’s finished, who knows. Maybe find another project after that, maybe not.\\nThanks to my contributions to the Ministry of Testing’s TestBash, I am now a proud holder of 1-year Dojo membership. I intend to do all the courses that appeal to me, because knowing what my fellow testers/QAs are doing and learning, keeps me current too.\\nI am the first person to discourage overuse and panacea treatment of test automation, but I believe it does have a place and should be architected intelligently and used wisely. I fear we have over-indexed on exploratory testing and manual checking at my workplace, making it worse with poor to no monitoring in production, no place more so than in the mobile space. It is my intention to get my head around “best” test automation practices (vs my current hacking skill set) for both web and mobile, through the courses offered on this portal.\\nI have been selected for “Women in Leadership” professional coaching at work. This starts in January and I am really looking forward to that. Also, my manager has bought his leadership team a few copies and shared with me the book “Elegant Puzzle — Systems of Engineering Management”, which I am currently reading and hope to finish in a few weeks.\\nI sense a change for me in my role in the coming year but nothing planned, it’s only a feeling. This may well be in the direction of engineering management, head of Quality (or Quality Engineering), or something in the direction of the leadership or architecting of the QA - Ops intersection. Hopefully with some Linux thrown in the mix.\\n(This remains the definition of my ideal role: https://twitter.com/TheresaNeate/status/1095090698527768576?s=20)\\nEither way, I want to be the best Lead QA I can be, treat others as I would like to be treated, continue to nurture the culture of quality ownership in all of Consumer, help the QAs be fulfilled in their own journeys, and fix a few key things along the way (as you guessed, some of it has to do with CI/CD).\\nI am an introvert and prefer fewer people to many, but since my long-term relationship ended a few years ago, it’s gone a bit far and my solitude has impacted me. I feel less awkward now than at first, but still feel it, especially at Christmas. Next Christmas I will either be travelling again, hopefully with company, or volunteering at a soup kitchen (which is surprisingly hard to locate, Google was no help in this! — now accepting ideas). I turned down offers of charity to join others’ gatherings, as charity is just not my thing and being a third wheel is no fun. So for next year’s Christmas (when I feel it the most), I’ll start thinking now.\\nI have already enrolled for 1 drawing course in Jan 2020, and am wait-listed for another. There is also a PADI level 1 diving course I might be doing in June in Cairns, but haven’t yet committed to. Either way, there will be new skills gained.\\nThere will be more travelling, at least 1 of which will be yet another new location in Australia (maybe Cairns for my planned diving course), and maybe another international destination. Watch this space.\\n(back to top)\\nSo — THAT was my 2019, and those are my hopes for 2020.\\nThere was so much to say, once I started writing. Thank you for reading, if you got this far!\\nUpon reflection, I had a lot to reflect on.\\nThe year of 2020 will be one of Health & Fitness (mental and physical), and Focus.\\nI want to recommend to all of you too, to read Deep Work by Cal Newport. I welcome hearing from you on your own journeys of Focus. Please tweet (or DM) me how you’re faring.\\nSee you on the other side.\\nI’ll close with pics of my furry babies:\\n(back to top)\\nWritten by\\n'},\n",
       "  {'author': 'Jonathan Hsu',\n",
       "   'link': 'https://medium.com/better-programming/how-to-iterate-over-two-or-more-lists-at-the-same-time-5f70b5c822ad?source=tag_archive---------5-----------------------',\n",
       "   'title': 'How To Iterate Over Two (or More) Lists at the Same Time',\n",
       "   'claps': 195,\n",
       "   'text': 'If you have had multiple data sets that needed to be jointly computed or displayed, then you probably have experienced the pain of iterating over multiple lists in parallel.\\nHere’s what that looks like traditionally...\\nTaking the length of one list and using a placeholder index variable, often named i, you manually access each position in the lists.\\nNotice the [i] in each list. This seems simple enough, not complex.\\nHowever, if you have a larger number of lists or more complex data that requires nested loops, then you’re setting yourself up to easily make mistakes.\\nThere has to be an easier way...\\nThe zip() function eliminates the guesswork and reduces the likelihood of an error. Taking a variable number of iterable arguments, the function will return a zip object that groups each item by its index.\\nNotice that the function returns a zip object, meaning you cannot print it right away. Fortunately, we can coerce it into the data type we want.\\nBe careful when coercing the zip object into a dictionary. Exactly two arguments are necessary, otherwise an error will be thrown.\\nWe can use this function inside a for loop as the iterable value. Below is the traditional parallel index technique refactored using zip().\\nWhat other uses for the zip() function have you found? Do you have another way of simultaneously iterating over multiple lists? Share your experiences below in the comments!\\nWritten by\\n'},\n",
       "  {'author': 'Nikita sharma',\n",
       "   'link': 'https://medium.com/@nikkisharma536/how-to-build-deep-neural-network-for-custom-ner-with-keras-9f3eda3b3bf2?source=tag_archive---------7-----------------------',\n",
       "   'title': 'How to build deep neural network for custom NER with Keras',\n",
       "   'claps': 54,\n",
       "   'text': 'In this post, we will learn how we can create a simple neural network to extract information ( NER) from unstructured text data with Keras.\\nNER is also known as entity identification or entity extraction. It is a process of identifying predefined entities present in a text such as person name, organisation, location, etc. It is a statistical model which is trained on a labelled data set and then used for extracting information from a given set of data.\\nSometimes we want to extract the information based on our domain or industry. For example : in medical domain, we want to extract disease or symptom or medication etc, in that case we need to create our own custom NER.\\nHere we will use BILSTM + CRF layers. The LSTM layer is used to filter the unwanted information and will keep only the important features/information and the CRF layer is used to deal with the sequential data.\\nBI-LSTM is used to produce vector representation for our words. It takes each word in a sentence as an input and produce a vector representation of each word in both directions (i.e; forward and backward) where forward direction access past information and backward direction access future. It is then combined with the CRF layer\\nCRF layer is an optimisation on top of BI-LSTM layer. It can be used to efficiently predict the current tag based on the past attributed tags. Here is a great post on why CRF layer is useful on top of BI-LSTM\\nFor this example I have used this Kaggle dataset. For our model, we need a data frame that contain ‘Sentence_id’/ ‘Sentence’ column, ‘word’ column and the ‘tag’ column.\\nWe will create list of list of tuples to organise our input data and to differentiate sentences from each other. After loading data, we will use the SentenceGetter class to retrieve sentences with their label.\\nOutput :\\nHere is how three sentences would look like:\\nKeras (and most other ML models) expect all the ids to be numeric, this is an optimisation to save memory. We will use word2idx dictionary to convert each word to a corresponding integer ID and tag2idx to convert tag to integer ID.\\nThe BI-LSTM layer expects all texts/sentences to be of the same length. We select the padding size to be the length of the longest sentence.\\nLet’s discuss in brief about different layers used to create our model.\\nThe input layer takes a shape parameter that is a tuple that indicates the dimensionality of the input data.\\nIt is basically a dictionary lookup that takes integers as input and returns the associated vectors.\\nIt takes three parameters :\\nIt takes five parameters :\\nIt is a wrapper that allow us to apply one layer to every element of our sequence independently. It is used in sequence classification to keep one-to-one relations on input and output.\\nWe have not applied any customisation to the CRF layer. We have passed the number of output classes to the CRF layer.\\nWe need to configure the learning process, before training a model.\\nIt takes three parameters :\\nIt is used to update/save the model weights to the model file, if and only if the validation accuracy improves.\\nIt takes five parameters :\\nIt will train the model for a fixed number of epochs.\\nIt takes seven parameters :\\nFull code link : https://www.kaggle.com/nikkisharma536/ner-with-bilstm-and-crf\\nThat’s all for this post. Stay tuned for more interesting blogs.\\nOriginally published at https://confusedcoders.com on December 29, 2019.\\nWritten by\\n'},\n",
       "  {'author': 'Jarosław Gilewski',\n",
       "   'link': 'https://medium.com/deepvisionguru/poseflow-real-time-pose-tracking-7f8062a7c996?source=tag_archive---------8-----------------------',\n",
       "   'title': 'PoseFlow — real-time pose tracking - DeepVision.guru - Medium',\n",
       "   'claps': 12,\n",
       "   'text': 'This story presents one of the methods for multi-person articulated pose tracking in video sequence called PoseFlow and its adaptation with the Detectron2 COCO Person Keypoint Detection Baseline.\\nDetectron2 is a robust framework for object detection and segmentation (see the model zoo). It allows us to detect person keypoints (eyes, ears, and main joints) and create human pose estimation.\\nThe person keypoints estimation is done on individual images and to fully understand human behaviour and be able to analyse the full scene, we need to track the person from frame to frame. The person tracking opens the possibility for action recognition, person re-identification, understanding human-object interaction, sports video analysis and much more.\\nWe will use the source code based on my previous story:\\nI encourage you to read it first! If you followed it just run the command below in the project directory:\\nor if you prefer to start from the beginning follow with:\\nTo check what the pose estimation is all about, run the command:\\nWe will get the following results on the screen:\\nAs you can see Detectron2 gives us the bounding box of the human and their keypoint estimations thanks to available COCO Person Keypoint Detection model with Keypoint R-CNN.\\nThis model is based on Mask R-CNN, which is flexible enough to extend it to human pose estimation. The keypoint’s location is modelled as a one-hotmask and Mask R-CNN is adopted to predict K masks, one foreach of K keypoint types (e.g., left shoulder, right elbow).\\nIt’s a top-down method where we first detect human proposal and then estimate keypoints within each box independently.There is also a bottom-up approach which directly infers the keypoints and the connection information between keypoints of all persons in the image without a human detector.\\nAnother very popular, alternative estimators are:\\nThere is a good article Human Pose Estimation with Deep Learning summarizing different approaches to human pose estimation.\\nIf you are perceptive enough, you will notice on the result above that the human poses are already tracked at least with the colour of the person bounding box. It’s a very naive heuristics to assign the same colour to the same instance for visual purpose based on intersection over union (IoU) of boxes or masks (see video_visualizer.py).\\nUsing bounding box IoU to track pose instances will most likely fail when an instance moves fast thus the boxes do not overlap, and in crowed scenes where boxes may not have the corresponding relationship with pose instances.\\nMulti-person articulated pose tracking in unconstrained videos is a very challenging problem, and there are a lot of solutions for that.\\nThe solution I would like to present is PoseFlow described in the paper PoseFlow: Efficient Online Pose Tracking by Yuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, Cewu Lu. The source code for the solution is available on GitHub, and it is also included in the AlphaPose repository.\\nIn my opinion, the paper is quite hard to read, and the correspondence to the implementation code is a little bit blurry. Even though the method is called online pose tracking, you are not able to use it directly on a video stream. It’s an academic research code presenting the idea and preparing results for PoseTrack Challenge validation set.Looking at the PoseTrack 2017 Leaderboard we can see that it is in 13th place in multi-person pose tracking challenge.\\nWith the idea of the processing pipeline in mind, I’ve moved the necessary part of the PoseFlow code to the detectron2-pipeline repository (pipeline/utils/pose_flow.py, pipeline/libs/pose_tracker.py) and added pipeline/track_pose.py pipeline step.\\nFrom the code perspective, the algorithm could be simplified to the following steps (see pipeline/libs/pose_tracker.py, line 59–87):\\nLet’s see it in action:\\nThe final result depends on a lot of factors like:\\nThe Detectron 2 person keypoint detection model is not the best one for a robust pose tracking. There are specialized frameworks like already mentioned AlphaPose or OpenPose. You should experiment with both while creating your pipeline, replacing Detectron2 with the chosen model.\\nHappy coding!\\nWritten by\\n'},\n",
       "  {'author': 'Sarah Noles',\n",
       "   'link': 'https://medium.com/swlh/python-guide-to-reading-excel-sheets-9132d81cbedd?source=tag_archive---------12-----------------------',\n",
       "   'title': 'Python Guide to Reading Excel Sheets - The Startup - Medium',\n",
       "   'claps': 88,\n",
       "   'text': 'Recently I worked on an automation project which required loading data from an Excel spreadsheet into a MSSQL database. I find myself always Googling how to read from an Excel sheet so I’m making this guide to document how to do it as well as work through some of the common problems I run into.\\nFor this example, I’m using a simple Excel spreadsheet named readme.xlsx with a sheet in it named data. The sheet contains a list of dogs and information about them including their name, their breed (if known), and a Boolean value indicating whether they are a puppy or not.\\nReading from an Excel sheet on it’s own is pretty easy using the Pandas library. First we install Pandas and xlrd, a library used for interacting with Excel files. Then, you just call the read_excel function with the workbook and sheet names.\\nTo install:\\nThen, in python:\\nIf we view the resulting Pandas data frame, we can see the results. Note that I am using PyCharm here because I like how they display data frames in their debugger.\\nAwesome! We are able to see the data in python. Now we can iterate through it and store it in the database.\\nHowever, if you look closely, you’ll notice some data didn’t come through quite as nicely as we would have hoped.\\nThe unknown values in the “breed” column which we left empty came across as nan, which will definitely cause problems when loading the data to a database. Fortunately, this is easy to fix!\\nTo fix this, we will tell Pandas to locate all nan values and replace them with another value. This is done with the fillna function. In this example, we are going to change the nan values to an empty string to return the data to it’s original state.\\nAfter running this function, we see that the the nan values are gone and replaced with an empty string. Success!\\nHowever, what happens if we have nan in multiple columns? What if we don’t know the breed for one dog and whether another dog is a puppy? We don’t want to replace the Boolean value with an empty string as that will certainly cause problems.\\nTo fix this problem, we can tell Pandas to fill nan values in certain columns with different values. Here’s an example.\\nOur test data now looks like this:\\nIf we run the same command as above, our data frame shows nan values in both the breed and puppy columns.\\nIf we run the same fillna(‘’) command as above, it will give us an empty string in the “puppy” column, which will cause problems on the database side.\\nTo fix this, we will use the same fillna function but instead of passing it a single value, we will pass it a dictionary which maps a column name to the value we want to replace nan with in that column. In the “puppy” column, we want to replace it with False and just assume the dog is not a puppy. Our updated command is:\\nThe resulting data frame shows different values in each column where the nan values used to be.\\nIt’s worth noting that that we can also exclude certain nan values from being replaced with this method. If we provide a column-specific dictionary and exclude a column from the dictionary, any nan values in the excluded column will not be replaced.\\nThe example below shows the breed nan values being replaced, but the puppy nan value remains.\\nOne more weird thing happened with our data when we had an empty value in the puppy column. The data went from displaying as True/False in the data frame to displaying as a float value (either 1.0 or 0.0). In this case, Pandas is giving it’s best guess on the data type, but we can change the import method to force Pandas to import the data as Boolean values.\\nTo fix this, we use the dtype parameter when calling the read_excel function and pass it a dictionary where we map a column name to a data type as shown below.\\nNow when we view the data frame, all the values in the puppy column show up as True/False again. Note that when we do this, it defaults the nan values to True in the new Boolean column. This is something to be aware of and may lead you to not force the value.\\nTo try this out yourself, check out my GitHub: https://github.com/SarahLN/PythonGuides/tree/master/Reading_Excel\\nCover photo by Émile Perron on Unsplash\\nWritten by\\n'},\n",
       "  {'author': 'Sam Danilov',\n",
       "   'link': 'https://medium.com/@senya.danilov/docker-on-a-diet-f4ce1b05a862?source=tag_archive---------13-----------------------',\n",
       "   'title': 'Docker on a diet - Sam Danilov - Medium',\n",
       "   'claps': 352,\n",
       "   'text': 'Docker containers are extremely useful when it comes to CI and CD. As a developer, I was amazed with its possibilities and ease of use. What I didn’t know when I started using Docker is that I did it all wrong.\\nAfter couple of months building and pushing containers to our private Nexus Registry, our devops approached me and said “Hey, your images took all the disk space of the registry, might want to check if they are a little bit oversized?”.\\nI had two docker images, one for Python application and one for Java. Their total size was whopping 6.5Gbs without applications themselves. After all enhancements it’s 1.4 Gb for Python and 660Mb for Java, applications’ binaries included.\\nIn this article I will cover tips and tricks for optimizing docker images size, which will save you registry disk space and (re)deployment time.\\nFirst thing to do would be extracting common stuff from your images.\\nAll the common libraries, files should be in a base image, so they would be cached inside docker layers. For my case I moved Apache Thrift and custom log rotation utility (which is build from sources) to baseline image, instantly saving 350Mb worth of space.\\nEach docker image consist of series of layers. These layers are changes made to the image with commands like FROM, RUN, COPY, you can think of it as a commits in a git repository. Docker makes use of union file systems to combine these layers into a single image. Union file systems allow files and directories of separate file systems, known as branches, to be transparently overlaid, forming a single coherent file system.\\nLayers come in handy, because they can be reused during build or deploy, but they also can make image significantly bigger.\\nImagine creating image for nodejs application:\\nLooks something like we would write in a bash-script, right? Totally wrong for docker though. Size of our new image is 248Mb. Let’s look into docker history command:\\nOutput suggests that we have 67.2Mb of data twice. That’s because we unzipped in one layer and copied in another and rm in the last RUN command had zero impact on previous layers. So the correct way to optimize this example would be:\\nNotice that there is also a rm -rf /var/lib/apt/lists/* command which will wipe the cache of aptitude. Let’s look at docker history again.\\nThe COPY command is used to put data from host machine inside the image during the build process.\\nWhat’s bad about COPY is that it creates a separate layer, so the copied data will reside in the image forever. Copying archives or some temporary data is really a bad idea. Prefer wget (or another tool) over COPY if it’s possible or...\\nMulti-stage builds is a relatively new feature requiring Docker 17.05 or higher. This added the ability to create temporary image and then use it’s filesystem during the build of final image. The basic idea is to install all the dependencies and compile all the sources in intermediate image and copy the result to final image. Let’s have a look at the example:\\nLooks like we have a lot of build-time only dependencies:\\nNow we add another build stage and copy binary from the “compiler” image:\\nSmall change and a nice reward:\\nBe careful though and don’t remove runtime dependencies of a package!\\nHave you heard of jlink? It’s awesome! You can create your own JRE with only modules you need, which presumably will be smaller than any “slim” image. I suggest using AdoptOpenJDK image as jlink provider. The idea is to create JRE with jlink and then COPY it using multi-stage build:\\nLet’s look through docker images\\nUsing jlink we achieved image two times smaller than the smallest AdoptOpenJDK image. Not to mention the ease of integration between custom JRE and baseline image (a simple COPY instruction).\\nMost of the projects use Maven or Gradle for dependency management. And sometimes we have a lot of dependencies. In our case it was 400Mb worth of jars! Build tools put all the dependency jars inside the main jar file and given the fact that dependencies are not being update very often, we will have a lot of images with the same dependencies. Looks like like a perfect candidate for moving to another layer!\\nFirst, you will need to extract dependencies from you project and if they were updated, push them to your registry.\\nLet’s see how to do it with Maven and SpringBoot. Add excludeGroupIds parameter to your builder plugin configuration:\\nAnd use this little bash script:\\nNow, how do we know if dependencies were updated and we need to push them? And how do we use that “dependencies image” as a baseline for application image?\\nI made this hacky approach where I calculate the md5-hash of pom.xml and try to pull docker image with such hash in version from registry. If it’s not there — create it and push it. This is how I did it:\\nNext, we pass dependencies hash to application’s dockerfile via ARG command and use it in version of FROM-image:\\nNote, that now there is a maven/gradle plugin jib that does job similar to what I’ve written above (not applicable to our case, though, because our Gitlab runner clear cache after every build).\\nI used the same approach as for Java container. I calculate the hash of requirements.txt file and use it as a version for intermediate container.\\nInside a dockerfile you should use multi-stage build. In first stage install all the build dependencies and then do pip install --no-cache-dir. In the result stage copy /opt/miniconda3 dir from previous stage.\\nLet’s look at python-dependencies.dockerfile. Don’t mind the apt-get install section, these are just packages that I needed for my python dependencies.\\nWhat’s left to do is create the final python image base on dependencies image. I just copy all python scripts (and a bash script for starting python).\\nFound this article useful? Follow me (Sam Danilov) on Medium! Please 👏 this article to share it!\\nWritten by\\n'},\n",
       "  {'author': 'Collin Ching',\n",
       "   'link': 'https://medium.com/@collindching/how-to-maximize-claps-on-your-medium-articles-ce427cf327ff?source=tag_archive---------14-----------------------',\n",
       "   'title': 'Earning more claps on your Medium stories with regression',\n",
       "   'claps': 226,\n",
       "   'text': 'There’s something about writing article headlines in sentence case that feels more personal than using title case. While I prefer to write my titles in sentence case, I occasionally wonder whether this is to the detriment of my reader engagement.\\nIn this project, I wanted to explore how title capitalization affects the number of claps a Medium article receives. And more broadly I wanted to use data to discover how content and formatting decisions impact a Medium article’s claps, with the goal of using these insights to help me maximize claps on my own articles.\\nTo this end, I scraped a year of data science articles from Medium, parsed those articles for features, and developed several regression models to predict claps.\\nThis post covers 1) data science workflow and 2) action items for Medium writers. In part 1), I explain data acquisition, machine learning workflow, modeling strategy, and error analysis methodology. In part 2) I detail potential strategies that Medium writers can use to potentially increase article claps.\\nWhile I included select code snippets in this post, you can access the full code here. If you’re just interested in the takeaways, you can skip to the bottom.\\nFor this project, I scraped two types of pages — archive pages and article pages. First, I collected article links and metadata from Medium’s data science article archive. I collected information on all articles from 2018.\\nAfter scraping the archive, I navigated to the scraped article links to collect further data and article content.\\nScraping method at a high level: I used ChromeDriver in Selenium to load dynamic webpage content. Then, I passed the rendered HTML to BeautifulSoup to parse and retrieve data. I added a three-second delay between HTML requests to avoid overloading Medium’s servers and getting blocked.\\nScraping culminated in over 15,000 articles and 21 text and metadata columns.\\nFull scraping code here.\\nWith the data scraped, these were some personal questions I wanted to explore.\\nIt is always useful to visualize the distribution of the target variable. I plotted a quick histogram below.\\nThis is heavily right-skewed. This will cause linear regression to disproportionately learn coefficients for low-clap articles and predict high-clap articles less effectively. I will apply a log transformation to the target variable to improve skew and, most importantly, improve the normality of my residuals.\\nAlmost half of all data science articles received 10 or fewer claps, while the most popular article received 50,000 claps (by Michael Jordan). Any article that received 120+ claps, outperformed 75% of data science articles published last year.\\nOutliers: Defining an outlier is problem-dependent. I used the IQR method for this data because it is robust to skewed. According to IQR, articles with over 301 claps were considered outliers.\\nTo balance a) modeling articles with a range of claps and b) ignoring outliers to limit skew, I analyzed articles that received between 0 and 1,000 claps, then applied a log transformation.\\nWith this done, I set aside 20% of articles to use as test data.\\nTo create model features, I visualized the data, feature engineered, and plugged new features into StatsModels’ OLS regression to check for statistical significance and predictive value. This process allowed me to eyeball the predictive power of newly created features.\\nAfter feature engineering, I used regularized regression in Scikit-Learn (lasso, ridge, and elastic net) to reduce model features.\\nThen, I conducted error analysis and decided whether to make adjustments to the data or the model.\\nFeature engineering was a four-step process, with a focus on low-hanging fruit first. I extracted basic features, then extracted categorical features, then engineered text features, and finally created polynomial features.\\nStep 1 — Basic features: These were no-brainer features that counted different text assets. Some features I extracted in this round were title lengths, number of bold text assets, and number of pictures. I also extracted date features.\\nStep 2 — Categorical features: For each categorical feature, I out the n most common categories and encoded them as dummy variables. For example, for publications, I took the 10 most frequent publications and encoded them as dummy variables. To do so, I defined a custom transformer called TopPublicationEncoder. Code below.\\nSee here for my custom transformer code.\\nStep 3 — Text features: These were text-based features, with minimal NLP involved (that was beyond this project’s scope for me). Some features I created were: whether the title was in title case or sentence case, the top 30 title keywords, the number of digits used in the article, readability score (in grade level), and title sentiment.\\nI used the Flesch-Kincaid readability test to approximate the school grade difficulty of an English-based text. Note: this process can take a while on large corpuses. The textstat package has a fast Flesch-Kincaid implementation.\\nTo analyze sentiment of article titles, I used the vaderSentiment package, which gives sentiment was on a scale of -1 to 1.\\nStep 4 — Higher-order features: I didn’t do too much here, but did decide to use the ratio of pictures to paragraphs as a feature.\\nAt the end, this is what the features looked like.\\nDue to time constraints, I didn’t use NLP in this project even though NLP features would have high predictive power. Features I would have considered adding were bag of n-grams, TF-IDF, document similarity, and document clustering.\\nThis is what article readability looked like.\\nThe x-axis represents the grade-level difficulty of a text, and you’ll notice that many articles have a grade level of 200+. This is an obvious fluke. In fact, it’s unlikely that any article even has a reading difficulty greater than 30.\\nWith a quick inspection, it turns out many of these articles were written in foreign languages, or had indecipherable non-English characters. This is what one article looked like.\\nClearly, these articles are confusing the readability algorithm. To handle this, I wrote a helper functio, is_English(), to identify non-English content.\\nUsing is_English() on titles and paragraphs, I was able to handle 500 non-English articles.\\nWith features and OLS regressions built, it’s time to add regularization to control for overfitting.\\nI used three regularized regression models: lasso, ridge, and elastic net regressions. Lasso enforces an L1 penalty which aids in feature selection and ridge enforces an L2 penalty that penalizes large beta coefficients. Elastic net is a just a weighted combination of the L1 and L2 penalties. Read more here.\\nHere were my model scores on validation data.\\nAs you can see, these regularized models are all pretty consistent, and result in a 30% error improvement over a naive baseline. Still, an average error of 80 claps is quite high.\\nA classic method for evaluating linear regression is the residual plot, where the difference between true and predicted values are on the y-axis and the predicted value is on the x-axis. Below is the residual plot for elastic net regression.\\nThis plot suggests that claps do not have a linear relationship with the beta coefficients of this elastic net model. The most glaring clue is that the slope of the residuals is negative — a violation of the OLS assumption of conditional errors having mean equal to zero. What this tells me is that my model is biased.\\nAt this point, there are a couple ways to achieve better residuals: either improve the quality of data inputs or use more suitable models for the data. Specific options considered are below.\\nA) Transform the features: I used a Yeo-Johnson transformation to improve normality, but this did not improve the linearity between my coefficients and the outcome. As a note, Yeo-Johnson transformations also complicate the interpretation of regression coefficients, which is a topic for a future article.\\nB) Add better features (polynomial, NLP): Creating more meaningful features through NLP would probably have a powerful effect in improving model performance. Unfortunately, creating polynomial and NLP features was beyond the scope of this project for me.\\nC) Use nonlinear regression: Circumvent the linearity assumption altogether by using a nonlinear regression method. Examples include generalized linear models, generalized additive models, local regression, or tree-based methods.\\nWith project scope and time constraints in mind, I ran with option C) and used a random forest as my final model.\\nEdit: After finishing the project write-up, I realize that since the Poisson regression is used to model non-zero count data and may be a good option for modeling claps. I will explore Poisson regressions for modeling Medium claps and publish the findings as a follow-up post to this project. Source.\\nA random forest is suitable for this prediction task not only because it is a non-parametric model, but also because it cannot extrapolate beyond the training data. Its predictions are bounded, in this case, between 0 claps and 1000 claps, whereas an OLS model could have predicted negative claps.\\nBelow is the code I used to tune my random forest model with 5-fold cross-validation.\\nMy final estimator, accessed through gridsearch.best_estimator_, had max_depth = 12, max_features = 40, min_samples_leaf = 2, and min_samples_split = 2.\\nRandom forest performed negligibly worse than elastic net, while allowing me to circumvent linearity. The compromise in model performance is worth it.\\nNext, we can analyze the feature importances using the SHAP (SHapley Additive exPlanations) package. The Shapley value is a concept taken from game theory: in a game involving a group of players and a payout, the Shapley value is the average marginal contribution of a particular player to that payout.\\nHow does this concept apply to random forests? Each decision tree in a random forest uses a different permutation of data and predictors to arrive at a set of predictions. The SHAP library calculates each predictor’s average marginal contribution to a prediction across all of these permutations. You can use Shapley values to peer into a random forest, and gain insight into which features it uses — and how it uses them — to make predictions.\\nDeciphering the SHAP summary plot: Broadly, this plot shows relationships between key predictors and the target variable. First, predictors are displayed in order of greatest to least feature importance along the y-axis, so tag_Education has the highest feature importance in this model. The x-axis corresponds to the magnitude and direction (positive or negative) of a feature’s impact on model predictions. Color represents the feature’s value.\\nPutting this all together for the first feature, this plot tells us that articles that have the education tag (pink dots) tend to have significantly fewer claps that articles without the tag (blue dots).\\nIf you want to learn more about SHAP plots, I found this article very helpful.\\nHere were the five questions I listed at the beginning of this post. This section is dedicated to answering them using data visualizations.\\nCapitalization structure doesn’t seem to affect the claps an article receives, so you’re free to capitalize however you want.\\nArticles that have a read time between 8 and 25 minutes generally receive a higher amount of claps. Claps dip for articles longer than 25 articles. Longer articles don’t seem to be a turnoff as long as the article is substantive.\\nArticles published under a publication perform significantly better than articles published independently — by a difference of about 150 claps.\\nBased on the plot above, I recommend that you publish with Towards Data Science or Analytics Vidhya unless you’re writing for a more specific audience.\\nTo measure this, I counted the number of bold and italicized text bodies per article. These charts show that text formatting tends to have a positive correlation with claps.\\nArticles written at a grade level of 12 to 14 performed the best. This is based off of the Flesh-Kincaid grading system, in which articles with more words per sentence and more syllables per word are considered more difficult (less readable).\\nI explored the data a little bit more, and arrived at some findings that could further inform writing strategy.\\nAny more than that, and you’re getting diminishing returns on your effort in choosing images. This is a general suggestion, but it doesn’t take into account the length of the article as well, and it wouldn’t make sense for a 300-word article to have six pictures. In this scenario a ratio would be more appropriate.\\nWe can use a new measure — words per picture — to gauge how many pictures you should include in your articles.\\nThe chart above tells us that the optimal ratio of words to pictures is somewhere between 100-to-1 to 200-to-1. That probably translates to a picture for every medium-sized paragraph or for every few short paragraphs.\\nWhen you publish a Medium article, you have the option to give your article up to five category tags. If you haven’t been doing so, max out your tags!\\nThis chart suggests that using fewer numbers in titles (three or less) tends to correspond to be better article performance, in contrast to what the SHAP summary plot suggested. Question your findings!\\nThe key issue that I ran into with this project was that this data could not be described linearly. In the real world, data will probably be pretty messy, so this is a scenario that I expect to run into more often. In order to overcome this, it will be helpful to learn about extensions of linear models that allow us to relax OLS assumptions. I plan to research this topic next and do some write-ups in future posts.\\nThanks for reading! If want to see how I implemented this analysis , here is the project’s GitHub repo. I’m always open to constructive feedback — if you have follow-up ideas for this analysis, comment it below or reach out via LinkedIn.\\nThanks to Harrison Jansma for inspiring this project with his Medium clap project.\\nWritten by\\n'},\n",
       "  {'author': 'Hasitha Chandula',\n",
       "   'link': 'https://medium.com/@hasitha.chandula/restful-api-with-python-and-flask-6e65e79a5e15?source=tag_archive---------24-----------------------',\n",
       "   'title': 'RESTful API with Python and Flask - Hasitha Chandula - Medium',\n",
       "   'claps': 176,\n",
       "   'text': 'Python is one of the most popular Programming Language. There are many frameworks and libraries in python. Flask is one of them. In this article, I’m going to show you how to build a simple RESTful API using Flask.\\nBefore start, you need to install python 3 on your computer. You can download and install python 3 by clicking here. After that, you can verify that by type python — version on your terminal or cmd in windows.\\nTo build Flask App First we need to install Flask by type pip install flask in terminal and press enter. After flask installed successfully now we can build our RESTApi.\\nFirst, we need to create a server using flask. To do that we need to create a new python file. I named it app.py and the code looks like bellow.\\nIn here what I did was import the Flask from flask and create our server in port 5000. Now we can access our server in http://localhost:5000. Now in the project directory open the terminal and enter python app.py and press enter. Now if you see something like bellow we are good to go.\\nWe can create routes using our app variable. I will show you how to create GET, POST, PUT, DELETE routes.\\n“/users” is our endpoint so we can get the response if we goto “http://localhost/users” using postman and request type is get.\\nwe can pass parameters like this. We need to put our parameter’s type the rute path.\\nIn post requests, we need to import request from flask.\\nThen we can access the request body. And if we need to return data as JSON then we need to convert data as JSON. We can do the convert using “jsonify”. First, we need to import that too.\\nNow we can do post request and return data as JSON to client. Code looks like bellow.\\nPut Request is more likely POST Request. Code is like this,\\nIn Delete Request, we pass parameters and find the element by the parameter and delete that item. Code like this,\\nNow I show you a full CRUD App using flask. In this example, my all data save in a variable but in the real world, we need to save data in a database.\\nIn My Example, I will create users with name, age and job and display all users, display user by their name, if user not found display error messages, edit users by their name, and finally delete users by their name. Here is my Full Example.\\nsource code here.\\nThank you.\\nWritten by\\n'}],\n",
       " [{'author': 'Jason Jung',\n",
       "   'link': 'https://medium.com/better-programming/how-i-built-and-deployed-my-first-web-application-with-django-in-5-weeks-e9728480a8dd?source=tag_archive---------0-----------------------',\n",
       "   'title': 'How I Built and Deployed My First Web Application with Django in 5 Weeks',\n",
       "   'claps': 276,\n",
       "   'text': 'I am a programmer, machine learning practitioner, and automation enthusiast, originally from Korea. Even though I do not have a CS degree, I fell in love with coding after college and have been learning new things on my own ever since.\\nIn July 2019, I launched my first web application, Salary Ninja, a tool for salary insights. It took me a total of five weeks while working full-time to learn Django, code, and deploy this application on a server.\\nAs part of becoming a full-stack engineer/problem-solver, I wanted to up my game by tapping into the front-end world. My idea was to build a website where you can search a database and return results with insights. Because I knew nothing about building a web application, I researched different options, tools, and frameworks. I want to share about the process I went through, as it might shed some light for those who want to build one themselves.\\nRough timeline:\\nEverything starts with an idea. You can turn your idea into a reality with grit, perseverance, and the belief that you can achieve anything to which you set your mind. What I love about the software world is that once you have an idea, most likely, you can embody it. All you have to do is sit down and find the right resources to carry it out and not give up.\\nI’ve had this idea to build a website application in the back of my mind for a while. It was when I read this article by Andrey Azimov that I was inspired to kickstart my project by first buying a domain name so that I would be more committed. Working at the world’s largest domain registrar, I had no excuse not to buy one. That’s how I ended up with https://salary.ninja.\\nBecause I had spent my precious money to buy the domain name, I was definitely more committed to solving this problem as soon as possible.\\nI’ve used some HTML and CSS before but I had no idea where to begin in terms of having a database. I literally started this project by simply searching on Google “how to build a website with search and database.” It was not the best way to search. Regardless, I ended up with two of the most popular options: Django and Node.js. In a brief summary, Django is a Python-based web framework and Node.js is Javascript based. Honestly, it seemed like Node.js was a bit more popular and was also more intuitive since Javascript was already widely being used for the front-end world. The chart below confirms the popularity of Node.js over Django.\\nHowever, this did not mean that Django was a bad option. I was surprised to learn that many prominent companies, like Pinterest, Dropbox, and Instagram, were using Django. I reasoned that those companies must have done enough research to justify using Django over Node.js. Also, Django was a big plus for me since I was already comfortable with Python and wanted to build my website as soon as possible. So I decided to go with Django.\\nThere are two main resources I used to learn Django: official documentation and YouTube.\\nI spent my first two weeks going through the official tutorial. I encourage you to do the same. I went through the tutorial twice. The first time I just went through all the examples and codes, and the second time, I started to tweak the code to create my website. The method is simple—after you learn the fundamentals, you try and fail until you get it the way you want it. At this stage, I just made sure I had a properly working search function and a database like below:\\nIf you want to read more about the differences between Django and Node.js, here are some links (in no particular order):\\nNow that I have a website with a basic structure, it is time to make it look pretty. I definitely did not want a crappy looking website. In the beginning, I didn’t know any better, so I started to style manually with HTML and CSS. I had a lot of trouble trying to figure out consistent styling and a mobile-friendly interface. That is when I came across Bootstrap, and it saved my life.\\nAccording to its official website, Bootstrap is the “world’s most popular front-end component library for building responsive, mobile-first websites.” Its mobile-first philosophy implies that the components are built for the mobile experience first and desktop versions are simply larger versions of the mobile site.\\nIn the past, I had created a desktop version and a separate mobile version, and discovered this to be very inefficient. Bootstrap, however, takes care of this issue for me. Being mobile-friendly is important because many people browse the web using their smartphones these days. Even for Salary Ninja, there are almost twice as many mobile visitors compared to desktop visitors, so I knew I had better focus on mobile user experience.\\nBootstrap provides many predefined classes/styles that you can easily use. For example, I used its template for navigation bars, input form, tables, footer, and more. Please check out its official documentation:\\nStill, you need to know basic HTML, CSS, and Javascript to use Bootstrap, so I suggest you learn the basics online. Perhaps check out Code Academy:\\nBefore I move on, I must mention other popular front-end libraries/frameworks: Angular, React, and Vue. I cannot comment on them too much since I do not have much experience with them. I didn’t feel the need to use them for my simple website. But if you want to be serious in becoming a front-end developer, you should definitely look into them. Below is a simple trend comparison.\\nReferences if you want to read further:\\nLike the image above, I wanted my search result to display an interactive plot. My first thought was using D3.js, but it turned out to be too complex for what I needed. Chart.js was a better option for me for a more simple plot. I recommend D3.js if you want to plot something more custom and complicated. I was able to create the chart above by using Chart.js’s simple template. Please learn more about them in the links below:\\nNext up, I wanted my table to have some features such as pagination, display options, search bar, and sorting. Bootstrap table provides some functions/stylings, but it was not enough for me. Then I came across DataTables, which “adds advanced interaction controlsto your HTML tables.” I ended up combining both Bootstrap and DataTables to get the best of functions and styling. Please check out the references below:\\nNow that I have a functioning, beautiful (?) website, I need to make it accessible from the internet so that anyone can reach the site. My criteria was:\\nThere were three options I looked into: Amazon Web Services (AWS), Google Cloud Platform (GCP), and Digital Ocean.\\nChoose:\\nDigital Ocean was an easy choice for me because it seemed to have really good documentation/tutorials and only cost me $5/month. GCP and AWS will cost you around $20/month for the cheapest options. Here’s a comparison of Digital Ocean and AWS.\\nThere are a few more tutorials you will have to go through to set up your server on DigitalOcean, connect your custom domain name, and set up SSL. DigitalOcean provides good tutorials on how to do these. Check below:\\nNow that you completed your website, there is a business side to your project. You can use Google Analytics to understand more about your audience, Google Adsense to monetize your website and add a Paypal Donate Button to get donations from your fans. I encourage you to check them out.\\nAgain, I encourage you to check out the links to learn more about them. Each of them is relatively simple to set up and implement.\\nHey, thanks for reading this article! Every time I finish a project, I try to write an article about it so I can document and share my process. I have no idea what the fate of my website will be, but I want to continue to make it better. I think the area that needs the most work is the query speed. If you have any tips, please let me know in the comment section!\\nI recently added a new company ranking feature. You can see where your company stands in terms of average salary.\\nThere is much more front-end skill that I want to learn and explore, such as React and Node.js. I hope I get to them one day. I hope you found this article informative and learned something new. Thank you again for reading!\\nFeel free to check out my previous projects/articles:\\n(LinkedIn, Twitter)\\nWritten by\\n'},\n",
       "  {'author': 'Egemen Zeytinci',\n",
       "   'link': 'https://towardsdatascience.com/predicting-hotel-cancellations-with-machine-learning-fa669f93e794?source=tag_archive---------1-----------------------',\n",
       "   'title': 'Predicting Hotel Reservation Cancellations with Machine Learning',\n",
       "   'claps': 60,\n",
       "   'text': 'As you can imagine, the cancellation rate for bookings in the online booking industry is quite high. Once the reservation has been cancelled, there is almost nothing to be done. This creates discomfort for many institutions and creates a desire to take precautions. Therefore, predicting reservations that can be cancelled and preventing these cancellations will create a surplus value for the institutions.\\nIn this article, I will try to explain how future cancelled reservations can be predicted in advance by machine learning methods. Let’s start with preprocessing!\\nFirst of all, I should say that you can access the data used in my repository that I will share at the end of my article. I would also like to share that this is the subject of a thesis. [1]\\nWe have two separate data sets, and since we’re going to do preprocessing for both, it makes sense to combine them. But during the modeling phase, we’re going to want to get to these two sets of data separately. So, to distinguish the two, I created the id field.\\nHere are the preprocessing steps for this project:\\nWith the code above, we convert string NULLand Undefined values to np.nan values. Then, we print the count of NULL values for each column. Here’s what the result looks like,\\nWe can delete NULLvalues in country, children, market_segment, distribution_channel, because there are few NULLvalues in these fields.\\nThere are a number of rules specified for the data. [2] For example, values that are Undefined/SC mean that they are no meal type.Since we have previously replaced Undefined values with NULL, we can fill the NULLvalues in the meal field with SC.\\nThe fact that the agent field is NULL means the reservation didn’t come from any agency. Therefore, these reservations can be considered as purchased directly by the customers, without any intermediary organizations such as agencies and etc. That’s why we’re not deleting NULLvalues, we’re throwing a random value like 999 instead. The same goes for the company field.\\nMore detailed information can be found in the document in the second link in the references.\\nThe ADR field refers to the average price per night of the reservation. Therefore, it is not normal for it to take a value smaller than zero. You can use df.describe().T to see such situations. We delete values that are smaller than zero for the ADR field.\\nFor integer and float fields, we determine the lower and upper points using the code below. If there is equation between the lower point and the upper point, we do not do any filtering. If not equal, we remove observations larger than the upper point and observations smaller than the lower point from the data set.\\nThe lower and upper points of the fields seem to be below,\\nFinally, we’re going to talk about multivariate outlier detection. [3] This is a special inference in which we work a little more and does not apply to every business. Charges such as $5 or $10 for a 1-night stay can be met normally, but that’s not normal for 10-night stay. Therefore, removing these values, which are considered contrary, from the data set will help our model to learn. So I’ve tried the LocalOutlierFactor and EllipticEnvelope, I’m only going over EllipticEnvelope because it yielded better results, but if you want to check out both, you can look at my repository.\\nThe chart is as follows. The red dots show outlier values.\\nAs you can see, it would make sense to leave small values outside the data set, especially after 6 nights. By applying this process, we can save the data set.\\nAnother important issue that has to be done before building up a model is feature engineering. Adding or removing features may be more efficient for our model.\\nFirst, I’m going to convert categorical data to integer using LabelEncoder, and then I’m going to look at the correlations. [4] The following code does this,\\nThis code gives us a correlation matrix like the one below,\\nIn this matrix, there appears to be a negative high correlation between reservation_status and is_canceled features. There is also a high correlation between total_nights and stays_in_week_nights and stays_in_weekend_nights fields. So, we remove reservation_status and total_nights features from our data set. Since there is a relation between reservation_status_date and reservation_status, we will remove this feature.\\nMachine learning models requires numerical data to operate. So before we can model, we need to convert categorical variables into numerical variables. There are two methods we can use to do this: Dummy variables and LabelEncoder .With the code you see below, we create features both using LabelEncoder and using dummy variables.\\nThen we build a logistic regression model with dummy variables and examine the classification report as a first look at the data.\\nThe accuracy score appears to be 0.8584, but the accuracy for reservations that have been cancelled is very low when looking at the classification report. Because our data contains 23720 successful cases and 8697 canceled cases. In such cases, it is preferred to dilute the weighted class or to increase the number of samples for the fewer sampled class. We will first select features with feature selection algorithm and then compare the dummy variables and label encoder using diluted data.\\nFeature selection is one of the most important issues for feature engineering. Here we will use SelectKBest, a popular feature selection algorithm for classification problems. Our scoring function will be chi2. [5]\\nWith the above function, we select the best features for both LabelEncoder and dummy variables.\\nThen we compare these features in a simple way.\\nThe comparison results are as follows,\\nWe select these fields because the features we create with dummy variables give better results.\\nWith the code above, we have equalized both the number of success reservations and the number of canceled reservations by 8697 and divided our data set into train and test. We will then measure the performance of our models by creating the following class.\\nLet’s go to the last step and compare our models!\\nMany models have been tried here, you can see them in my repository. But here I’m going to share the results of the top 2 models and some code that shows how we do hyperparameter tuning. Here’s how it goes,\\nXGBoost results are as follows:\\nIf we replace the XGBoost with the GBM using the codes above, the results are as follows:\\nFirst, I wanted to emphasize the importance of preprocessing and feature selection steps in model building processes in this article. The way to create a successful model is to get clean data.\\nThe optimization of the model established afterwards and especially the problem of classification should not be overlooked the importance of recall values. The accuracy by class is one of the most critical points of classification problems.\\nHopefully, it has been a useful article!\\nThank you for reading! If you’re curious about more and want to see the results for the H2 file, please visit my repository!\\n[1] Nuno Antonio, Ana de Almeida and Luis Nunes, Predicting hotel booking cancellations to decrease uncertainty and increase revenue (2017)\\n[2] Nuno Antonio, Ana de Almeida and Luis Nunes, Hotel booking demand datasets (2019)\\n[3] Christopher Jose, Anomaly Detection Techniques in Python (2019)\\n[4] Vishal R, Feature selection — Correlation and P-value (2018)\\n[5] Feature selection using SelectKBest (2018)\\nWritten by\\n'},\n",
       "  {'author': 'Dhanesh Budhrani',\n",
       "   'link': 'https://towardsdatascience.com/decorating-functions-in-python-619cbbe82c74?source=tag_archive---------2-----------------------',\n",
       "   'title': 'Decorating functions in Python - Towards Data Science',\n",
       "   'claps': 71,\n",
       "   'text': 'In this post, I’ll be explaining what Python decorators are and how you can implement one. Let’s get started!\\nA decorator is no more than a comfortable way to call a high-order function. You’ve probably seen it many many times already, it’s about those “@dostuff” strings that you find from time to time above a function’s signature:\\nOk, but now, what is a high-order function? A high-order function is just any function that takes one (or more) function as an argument and/or returns a function. For instance, in the above example, “@dostuff” would be the decorator, “dostuff” would be the name of the high-order function and “foo” would be the decorated function and the parameter of the high-order function.\\nAll clear? Great, let’s start implementing our first decorator!\\nIn order to start implementing our decorator, I’ll introduce you to functools: Python’s module for high-order functions. Specifically, we’re going to be using the wraps function.\\nLet’s create a high-order function that will print the execution time of the decorated function: we’ll call it “timeme”. In this way, every time we want to compute the execution time of a function, we’ll just need to add the decorator “@timeme” above the signature of the target method. Let’s start defining the signature of “timeme”:\\nAs mentioned before, a high-order function takes another function (the decorated one) as its argument, so we’ve included “func” in its signature. Now, we need to add a wrapper function that will contain the timing logic. For this purpose, we’ll be creating a “wrapper” function that will be wrapped by functools’ wraps function:\\nNote that “timeme” is returning the function “wrapper”, which in turn, besides printing the execution time, will return the result of the decorated function.\\nNow, let’s complete the wrapper by implementing the timing logic:\\nNote that the decorated function “func” is being executed with its positional and keyword arguments. I’ve added some print messages for you to observe the execution order. Alright! Let’s try it out, I’ll create a simple function with a print message, decorated by “timeme”:\\nIf you run it, you will see something like the following:\\nAs you can see, the first print message was placed in the high-order function. Then, we are calling the decorated function and printing its own message. Finally, we compute the execution time and print it.\\nWritten by\\n'},\n",
       "  {'author': 'Dario Radečić',\n",
       "   'link': 'https://towardsdatascience.com/4-steps-to-break-into-data-science-in-2020-4750418c726c?source=tag_archive---------3-----------------------',\n",
       "   'title': '4 Steps to Break Into Data Science in 2020 - Towards Data Science',\n",
       "   'claps': 101,\n",
       "   'text': 'Nothing wrong with that, but you probably know that it’s very easy to make an exhaustive list of near-impossible, time-consuming goals that will only make you feel overwhelmed, and very likely not motivated — because there’s so much to do.\\nIf you’re planning to enroll in data science in the next year, I’d say you’ve made a great decision. The field is widely accepted, there are jobs everywhere, salaries are great, and even the management is slowly figuring out why data science is needed.\\nBut before we start, let me to slightly demotivate you (yes, it’s necessary) — one year isn’t enough to learn the entire field.\\nDon’t get me wrong, 1 year is enough for you to land your first job, but the chances are you won’t go from 0 to data science team lead in a year (if you manage to do so please share your story in the comment section).\\nWith that being said, let’s explore all the skills you’ll need and how to learn just enough of them to get you started.\\nYou’ve most likely heard of harsh math prerequisites of data science. The amount of math you’ll need to know will vary much depending on the job role, but as a general answer to how much math you will need to get started, I would say: less than you think.\\nThe reasoning will follow. It’s tempting to dive deep into every somewhat related field — like calculus, linear algebra, probability, or statistics — but you need to know when it’s time to stop.\\nDon’t get me wrong, if you have all of the time in the world be my guest, become an expert in the above-mentioned fields, but otherwise, make sure you’re not wasting your time. To break into the field as a junior-level data scientist you’ll need to know math, but more on the intuitive level. You’ll need to know what to do in certain situations — that’s where the intuition part comes in — but I wouldn’t spend much time solving complex math tasks by hand.\\nIf you’re good on the intuitive level and know how to code — that’s enough. There’s plenty of time to get deeper into math after you get a job — no need to learn everything beforehand.\\nIf you don’t have an advanced math degree already I wouldn’t suggest you spend more than 2–3 months brushing up the math skills.\\nYes, coding skills are essential to data science. If you get a job in the industry and your coding skills are lacking, most likely you will know what you need to do, but you won’t know how to do it. It’s also likely you’ll suffer from SOCPS (Stack Overflow Copy Paste Syndrome), possibly even without reading the questions and answers.\\nThere’s nothing wrong with looking for more elegant solutions online, but you should know how to write a basic solution by yourself.\\nIf you’ve never written a line of code before, start small, read a book on Python or R and their role in data science (to get a complete picture). Then dive deeper into syntax. Don’t worry about memorizing everything, just make sure to know where to look when you get stuck.\\nIf you’ve already read a book or finished a course on programming and you know the syntax, but don’t know how to approach the problem, spend some time learning algorithms and data structures. Also go through most common coding interview questions, as those will get your creative juices flowing (or piss you off).\\nYou’re satisfied with your programming skills? That’s awesome! Now spend some time with analysis libraries — like Numpy and Pandas.\\nHow much time you’ll spend on coding will also vary much. It won’t be the same for complete beginners, or the ones who just need library knowledge. I’d say 3–4 months will be enough for complete beginners, and around 1 month if you’re learning the analysis libraries only.\\nIt’s highly likely that data you’re analyzing will come from some sort of a database. That’s where a typical work environment gets different from books or online courses — you won’t get a nicely formatted CSV file. More often than not you’ll need a domain knowledge (or someone with domain knowledge), and also a good amount of SQL knowledge.\\nIf you’ll be doing analysis in programming languages like Python or R, then don’t spend too much time learning SQL analytic functions, PLSQL/T-SQL and all of that more advanced stuff. Your SQL work, in this case, will rely mostly on joining a couple of tables on which you can perform the analysis.\\nHow much time you’ll spend here depends on the way you’ll use them and on the prior knowledge, but for starters don’t spend more than a month here.\\nIf you’ve followed each step from above and you don’t have some prior knowledge than it’s probably August or September of 2020. A lot of time has passed by, but you have all the prerequisites needed to land your first job.\\nWell, not all to be precise.\\nYou’re looking for a job in data science, and we’ve only been covering prerequisites so far. I would suggest that for the next 2 months you get comfortable with the basic data analysis and visualization libraries, like:\\nThat is if you haven’t already (you probably have since learning the prerequisites without a clear indication of why you need them can be boring).Don’t just go over tutorials, download some datasets from the web and perform a solid analysis. Then go online and see what others have done on the same dataset to see where you can improve.\\nIn the same 2 month period you should also get acquainted with some of machine learning algorithms, like:\\nMaybe you won’t use some of them in practice, but they will provide you with a base for learning more advanced algorithms like XGBoost and Neural networks later on.\\nLike with the analysis libraries, make sure to not follow tutorial by tutorial, but to do good quality work by yourself. If you feel like it, try implementing the algorithms from scratch in Numpy — but that’s not mandatory.\\nWith only a couple of months left in 2020, create a GitHub account a there upload 3–5 of your finest analysis/ML pieces for potential employers to see. Also, make a nice looking resume and cover letter.\\nIf you really feel like it, document your learning journey in the form of an online blog. Online presence can only help you in career development, that is if you don’t publish nonsense content on a daily basis — but I trust your judgment.\\nAnd that’s it, start sending your resume to the companies you want to work for — there’s nothing else you can do.\\nI sincerely hope 2020 will be your year. Go crush it.\\nWritten by\\n'},\n",
       "  {'author': 'Alina Yurenko',\n",
       "   'link': 'https://medium.com/graalvm/graalvm-in-2019-year-in-review-9ae272816e47?source=tag_archive---------4-----------------------',\n",
       "   'title': 'GraalVM in 2019: Year in Review - graalvm - Medium',\n",
       "   'claps': 274,\n",
       "   'text': '2019 is coming to an end. It was an extremely exciting, eventful, and productive year for the GraalVM project and community. Read some of the highlights below!\\nThis year in May our team reached a major milestone: we released the first production-ready version of GraalVM, GraalVM.19.0. Originating in Oracle Labs, an Oracle R&D organization, GraalVM was heavily researched for almost 8 years, and our team is very proud of reaching the production-ready status.\\nWe are happy to see an active and vibrant community around GraalVM contributing to the project, building new things on top of GraalVM, and extending the ecosystem to help others benefit from what GraalVM has to offer. Check some of the highlights below.\\nGraalVM Open Source Projects. GraalVM Community Edition is built from 3.6 million lines of code developed by the GraalVM team and collaborators, plus additional millions of lines of code from projects we depend on like Java, Node.js and others. This year our core GitHub repository passed the 10,000 stars milestone, and we added a new Open Source page, listing all our open source projects and some examples of how GraalVM advances the open source ecosystem.\\nGraalVM Community Workshop. This year we organized our first community workshop as a way to meet our community members in person, discuss the current project state and roadmap, and talk about ways we can improve our collaboration. It went great and we are excited to share the results — coming up in the next blog post!\\nJavaFX support with Gluon Substrate and GraalVM Native Image. The Gloun team introduced Gluon Substrate, a framework that leverages GraalVM Native Image to convert JavaFX applications into native executables. At the time of writing, Gluon Substrate can create executables for Linux and MacOS, with Windows support expected to be added soon.\\nQuarkus framework quickly became one of the hottest topics in the Java community. Leveraging GraalVM Native Image technology it offers impressive startup time and runtime memory savings. Learn how to get started with Quarkus on GraalVM.\\nSpring Boot applications as GraalVM native images. The Spring Boot team introduced initial support for GraalVM Native Image and a roadmap for extending this support in the immediate future. You can try different Spring Boot apps that already support GraalVM Native Image, or watch the talk by Sébastien Deleuze, a Spring Framework committer, to learn more about the current support state and plans.\\nNew language implementations by the community. The community open-sourced GraalSqueak — a Squeak/Smalltalk implementation for the GraalVM. Also, we continue to see valuable language contributions from individuals and organizations, such as TruffleRuby contributions by Shopify.\\nMicronaut framework actively extended support for GraalVM, most recently in their new data access toolkit — Micronaut Data, compatible out-of-the-box with GraalVM native images. Follow this great guide to build your first GraalVM-native Micronaut app.\\nHelidon, Netty-based microservice framework also added GraalVM Native Image support.\\nPicocli is a one-file framework for creating rich Java command line applications. Check out their updated guide to create native images for Picocli applications with extremely fast startup and convenient packaging!\\nIn 2019 we published a version roadmap to make it easier for developers and related projects to plan version updates. New major versions are released every three months and the last major release of the year is supported for the next full year. We also follow Oracle’s Security Policy and produce Critical Patch Updates (CPU) releases accordingly.\\nThe next major release — GraalVM 20.0 — will go live on February 18, 2020.\\nSome of the major features released in 2019 include the following.\\nIntroducing libgraal. One of the interesting characteristics of GraalVM is its ability to internally reuse components, multiplying the benefits. A perfect example of this is libgraal — a precompiled version of the GraalVM compiler, created using GraalVM Native Image. This has several advantages, such as improving startup times and completely avoiding interference with the heap usage and profiling of application code.\\nSpeeding up the compilation. Thanks to using profile-guided optimizations when creating libgraal and improving heuristics of our optimizations to make them less computationally intensive, we’ve been able to speed up compilation almost 2x. As a result, in the linked example, compilation required 30% less CPU time and the workload was faster by 13%, which makes a big difference for short and medium-length programs.\\nNew tools support. GraalVM 19.2 brought a bunch of new tools: GraalVM VisualVM now supports reading Flight Recorder snapshots, we introduced the GraalVM LLVM toolchain, and also added an extension for working with GraalVM programs in VS Code.\\nTracing agent for native images. Ahead of time compilation used to build native images requires additional configuration to use functionality like reflection, JNI, resources, and proxies. To simplify this, we introduced a tracing agent, that records the behavior of a Java application running and produces the configuration files automatically.\\nNative image class initialization updates. Since GraalVM 19.0, application classes in native images are by default initialized at run time and no longer at image build time. This provides a better out-of-the-box experience for users while still leaving the possibility to optimize startup time by running selected class initializers while creating a native image.\\nImproving profile-guided optimizations. PGO is an important optimization technique which allows GraalVM Native Image to achieve peak performance comparable to that of dynamically executed code, in addition to fast startup and low memory footprint. Until GraalVM 19.2, the way to collect profiling data was by creating and running an instrumented binary. This is now simplified — you can collect this data while running your program in JIT mode.\\nEarly adopter Windows support. Windows builds, introduced in 19.0, include the JDK with the GraalVM compiler enabled, Native Image capabilities, GraalVM JavaScript engine, and developer tools.\\nGraalVM on JDK11. Version 19.3 introduced long-awaited JDK11 support. We’ll continue providing JDK 8 builds, but if you’ve been looking to trying GraalVM on Java 11, now is the time to give it a go.\\nThe WebAssembly support. Our idea for GraalVM is to create a universal virtual machine, so adding new languages and platforms is one of our primary goals. We are particularly happy to add support for WebAssembly, a feature highly requested by the community. GraalWasm implements the WebAssembly MVP (Minimum Viable Product) specification, and can run WebAssembly programs in binary format, generated by tools such as Emscripten. It’s still an early implementation and we working on extending it, so feedback and contributions are very welcome!\\nBelow are some of the interesting use cases of GraalVM we learned about this year.\\nOracle Cloud Infrastructure migrated their services to GraalVM, resulting in 10% performance improvements and reducing garbage collection time by 25%.\\nThe Dutch National Police use GraalVM to enhance their Scala applications with the R data science capabilities.\\nNVIDIA added integration of CUDA into GraalVM languages like Python, JavaScript, R, and Ruby, meaning you can now define and launch GPU kernels from GraalVM applications.\\n2019 was a terrific year for GraalVM. We’ve released many new features and defined plans for the next year, had productive collaboration with the community, and saw more companies embracing GraalVM. We are looking forward to what 2020 brings and as always are happy to get your feedback and feature requests via GitHub or Twitter.\\nEnjoy the end of the decade and have a great 2020!🎉🚀\\nP.S. Many thanks to Doug Simon, Oleg Šelajev and Shaun Smith for helping with this blog post!\\nWritten by\\n'},\n",
       "  {'author': 'PRANOY DEV',\n",
       "   'link': 'https://medium.com/innovation-incubator/rest-api-performance-comparison-python-vs-golang-dc4decbd0543?source=tag_archive---------6-----------------------',\n",
       "   'title': 'REST API Performance Comparison Python vs Golang - Innovation Incubator - Medium',\n",
       "   'claps': 176,\n",
       "   'text': 'In this post, we’re going to explore building APIs with Python and GO. We’re going to put these two languages to the test and grade their performance based on response time. I’ll be using Postman for API testing. REST APIs in Python is powered by Flask and Mux for Golang\\nWe are going to perform CRUD operations onto a PostgreSQL instance running on localhost using python and go and rate them across each category.\\nWhat is CRUD? It stands for Create, Read, Update and Delete, these are the basic operations performed on a database. Read more about it from the below reference.\\nFirst, we have to set up the PostgreSQL instance and create a table onto which we can perform the operations.\\nhere we create a simple table called data with a single text field. We’re going to perform the CRUD processes on this table inside the Postgres database.\\nThe code repo can be found at\\nThe APIs have been built in a way that is similar to the following architecture.\\nNow let’s Analyse the response times. Let’s fire up Postman and checkout the response times for\\nPython:\\nGolang:\\nWinner: Golang, while python was averaging 20 to 30 ms, Golang was doing 15 to 20 ms winning by a margin of 5 -10 ms\\nPython:\\nGolang:\\nWinner: Golang, while python was averaging 15to 20ms, Golang was doing 12 to 15ms winning by a margin of 3 - 5 ms\\nPython:\\nGolang:\\nWinner: Golang, while python was averaging 20 to 25ms, Golang was doing 15 to 20ms winning by a margin of 5 ms\\nPython:\\nGolang:\\nWinner: Golang by a small margin of 2 to 3ms when compared to Python\\nGO is faster than Python, duh! but brownie points to Python for being soo easy to get up and running. But the goal of this experiment was to judge the impact that migrating to Golang would bring from a business perspective. Assume that 1 million API calls are made, Say python takes 20ms per request and Golang takes 15ms per request, Total processing time in Python would take around 5.55 hours and Golang would take 4.16 hours. The difference a few milliseconds make is huge. Since most businesses use AWS for building and deploying APIs, Golang will help save a huge sum of money as AWS charges per time taken for execution.\\nWritten by\\n'},\n",
       "  {'author': 'girija reddy',\n",
       "   'link': 'https://medium.com/@girijareddy937/important-python-frameworks-for-developers-in-2020-67df3c16b334?source=tag_archive---------7-----------------------',\n",
       "   'title': 'Important Python Frameworks for Developers in 2020 - girija reddy - Medium',\n",
       "   'claps': 65,\n",
       "   'text': 'In simple words, A web framework is a very useful tool for developing different web applications easily and effortlessly. It allows you to get rid of small tasks and problems associated with protocols, sockets and process/thread management while creating websites and web applications.\\nAs there is no such thing as a “perfect framework”, many developers use Python frameworks. It doesn’t matter whether you are a beginner or a more experienced developer, you can easily join the Python Online Course to learn how to work with this framework and simplify the development process. Choosing the Learn Python Programming Toronto and right Python Framework helps you to get a successful career. Therefore, we have compiled a few Important Python Frameworks in this article. Have a look.\\n1. Tornado\\nTornado is a nice web framework for building web applications based on python. It has the asynchronous network library, originally created for the FriendFeed aggregator. With the help of non-blocking I/O, Tornado can scale to tens of thousands of open connections, which makes it perfect for long polling, web sockets and other applications that need a long connection to each user. With its proper use, you can confidently cope with tens of thousands of tasks and create amazing web applications.\\n2. Twisted\\nIt is a free, open-source and event-driven network engine written in Python and released completely free under the MIT license. It works on the basis of Python 2 & a regularly developing tool is on Python 3. As this framework is based on deferred, it helps asynchronous architecture works very well.\\n3. Flask\\nWith Flask, you can create an unlimited number of unique web applications based on Python. the framework is suitable for those web developers who want to use the best practices, achieve rapid prototyping and create stand-alone applications in quick succession. It is a MicroFrame, which takes much less time to set up and installation. There will be fewer levels of abstraction between you and the HTTP functions.\\n4. Django\\nDjango is a high-quality Python web framework that allows you to create amazing web applications with simple and clean code. It is highly suitable for the creation of E-commerce websites. It comes with a number of features.\\nSo, you get almost everything that is needed to creating customer-centric, highly functional and professional web applications. It easily supports main databases — MySQL, SQLite, PostgreSQL, and Oracle. Its authentication, URL routing, pattern engine, object-relational mapping (ORM), and database schema migration (Django v.1.7 +) makes it an excellent Python framework.\\n5. Pyramid\\nA pyramid is a completely “open-source” framework to create web applications based on Python. It makes the work of web developers easier and comfortable. It is compatible with Python version 3 and allows web developers to work with NoSQL databases, including MongoDB and CouchDB. It makes web application development work simple, minimalistic, fast, and flexible. It maintains a huge amount of documentation and best suited for those developing an API, prototyping, and CMS.\\n6. Bottle\\nThe bottle is an extremely useful framework and fits into just one file. Despite its minimalism, it offers ample opportunities and fully fit for small and medium-sized web development projects. It includes routing for blank URLs and its utilities provide convenient access to data forms, file uploads, headers, and other metadata related to HTTP.\\nIt comes with a built-in HTTP development server that supports the functions fapws3, Bjoern, GAE, CherryPy, and any other HTTP server that supports WSGI, which gives an excellent solution for creating simple applications, studying the organization of web frameworks and prototyping.\\n7. Kivy\\nKivy is an open-source Python library’s particularly developed for developing cross-platform GUI applications. It enables you to write GUI applications in pure Python that run on major platforms, such as Windows, Linux, MacOS, Android, IOS.\\nKivy is a great choice if you can work with a non-native set of user interface tools in your applications. It allows web development professionals to write mobile applications using your Python programming skills without having to learn another language for a specific platform.\\n8. Dash\\nDash is an open-source framework used to create analytical web applications because it has web servers that launch Flask and exchange data with JSON packets using HTTP requests. Their interface offers components using React.js and the best thing is that applications developed with Dash are displayed in a web browser and can be deployed on servers. It means that dash applications are inherently cross-platform and mobile. They can use a rich set of plug-ins to extend the capabilities of dash applications.\\n9. cherrypy\\nCherryP is an object-oriented HTTP framework and it makes web application development work easier and comfortable. Its applications run on all operating system that supports Python, Windows, MacOS, Linux, etc. With the help of CherryPy, you can easily run several HTTP servers.\\nThis framework has a very powerful configuration system and a flexible plugin system. If you choose a package with a fast process of work, you also get ready-made tools for caching, encoding, sessions, authentication, static content, which simplifies and minimizes time while creating professional web applications.\\nFinal Words\\nIt’s a universal fact that frameworks simplify the lives of developers greatly by offering a clear structure for the application development process. They help professionals to get rid of small tasks and problems and create amazing web applications easily and effortlessly. These are some important python frameworks. You can choose any one of them as per your needs.\\nWritten by\\n'},\n",
       "  {'author': 'Pradeep T',\n",
       "   'link': 'https://medium.com/@pradeeprajkvr/towards-python-3-8-9af2d83af078?source=tag_archive---------8-----------------------',\n",
       "   'title': 'Towards python 3.8 | Medium | Medium',\n",
       "   'claps': 43,\n",
       "   'text': 'Python...One of the marvellous name that a techie can experience in 21st century. It is one of the ruling king(may be queen 👸) in the kingdom of programming languages. Guido Van Rossum published the first version of Python code (version 0.9. 0) at alt. sources in February 1991. After that various versions of python have been released. Among that python 2.x has made its own space and popularity.But the time arrived... Bells are ringing...Python 2.x is coming to its end(😟). Yes, from Jan 1, 2020 , python 2.x will no longer be supported by its creators, the python software foundation. But no worries, Python 3.8 is waving at us. Give a big thanks to python 2.x for its countless helping moments and heartily welcome python 3.8 , a new dawn.. !In this blog I am introducing you some of the exciting features of python 3.8. Come lets have a look on this.\\nPython programming language is built up on 19 guiding principles. The Zen of python. They are,\\nDefinitely, Python 3.8 is also based on these 19 principles. Download the latest version of python from here and install it in your machine(Windows, Linux/UNIX, Mac OS X, and Other). Some of the exciting features of python 3.8 that I am going to describe are:\\nWalrus-operator is another name for assignment expressions.It is a way to assign variables within an expression using the notation NAME:= expr, according to the official documentation. It is called the walrus operator as it resembles the eyes and tusks of a walrus on its side.\\n“The walrus operator” is a new method that allows you to assign values as part of an expression to the variable. Consider the example of the following code:\\nExplanation: The value of variable var1 is assigned to 100. The assignment expression var2 := var1 assigns variable var2 to the value of var1 which is 100 (in the above case), and checks if the value is greater than 90 (it acts like a normal if loop condition).\\noutput:\\nThe value of var2 is 100 and is greater than 90.\\nThe ability to define positional-only arguments using the / marker in function definitions is one of the many new improvements to the Python language in the 3.8 version. This change to the syntax has performance benefits and enhances the functionality of the API design. Keyword-only arguments are already available in Python with the * marker, and addition of / marker for positional-only arguments will enhances the consistency of the language.\\nIn order to define arguments as positional only, a / marker should be inserted in the function specification after all these arguments. Consider the example of the following code:\\nThe following will happens:\\nF-strings were introduced in Python 3.6 with PEP 498. Python supports multiple ways to format text strings. These include %-formatting , str.format(), and string.Template. PEP 498 introduced to add a new string formatting mechanism: Literal String Interpolation. In this PEP, such strings are called “f-strings”, taken from the leading character used to denote such strings, and standing for “formatted strings”.\\nWith f-strings it is possible to evaluate an expression as part of the string along with inserting result of function calls and so on. This feature is already present in many languages like Perl, Ruby, JavaScript etc. A basic usage of this is given below.\\nOutput:\\nHello, pradeep\\nHello, Pradeep\\nf-strings simplifies a lot of scenarios where str.format and % style formatting is used. It is much easier to use f — strings when compared to other string formatting ways.\\nPython is strongly typed as the interpreter keeps track of all variables types. It’s also dynamic as it rarely uses what it knows to limit variable usage. It supports the use of type hints via the typing module to allow third-party tools to verify Python programs. Python 3.8 adds new elements to typing module to make more robust checks possible.\\nThe final qualifier instructs a static type checker to restrict sub classing, overriding, or reassignment. The typing.Final , a special typing construct is used to indicate type checkers that a name cannot be re-assigned or overridden in a subclass. And the typing.final ,a decorator to indicate to type checkers that the decorated method cannot be overridden, and the decorated class cannot be sub classed. Example,\\npi: Final[float] = 3.1415926536\\nLiteral types indicate that a parameter or return value is constrained to one or more specific literal values. It can be used to indicate to type checkers that the corresponding variable or function parameter has a value equivalent to the provided literal. See PEP 586 for more details.\\nIt is a simple typed namespace. At runtime it is equivalent to a plain dict. TypedDict creates a dictionary type that expects all of its instances(specify “total=False” to allow keys to be optional) to have a certain set of keys, where each key is associated with a value of a consistent type. This expectation is not checked at runtime but is only enforced by type checkers. See PEP 589 for more details.\\nThere are some other pretty cool features introduced in python 3.8. Come,lets have a look on those.\\nThrough this module, we can get the information about installed packages in our Python installation. Consider the example,\\nOutput:\\nPython 3.8 brought some new changes in python math module. math.prod() works similarly to the built-in sum(), but for multiplicative products. Another new function is math.isqrt(). We can use isqrt() to find the integer part of square roots. Consider the example,\\nOutput:\\nPython 3.8 warns you in a pretty way for your silly mistakes(But don’t repeat😕) . It helps you during coding and debugging . Example,\\nOutput:\\nNow the question arrives. Should I upgrade to python 3.8 now? And the clear answer is “there is no downside to trying out Python 3.8 for yourself now”. Yes,now the ball is in your court..!\\nOnce you’ve upgraded to latest version, you can start to experiment with features that are only in Python 3.8, such as assignment expressions and positional-only arguments. See the porting to python 3.8 document for more details. So buddy, don’t be late ,install python 3.8 and enjoy coding..!\\nWritten by\\n'},\n",
       "  {'author': 'Sadrach Pierre, Ph.D.',\n",
       "   'link': 'https://towardsdatascience.com/exploratory-data-analysis-of-the-fifa-19-dataset-in-python-24eb27de9e59?source=tag_archive---------9-----------------------',\n",
       "   'title': 'Exploratory Data Analysis of the FIFA 19 Dataset in Python',\n",
       "   'claps': 215,\n",
       "   'text': 'In this post we will perform simple explaratory data analysis of the FIFA 19 data set. The data set can be found on Kaggle. FIFA is the Fédération Internationale de Football Association and FIFA 19 is part of the FIFA series of association football video games. It is one of the best selling video games of all time selling over 260 million copies to date.\\nFor this analysis we will be using the python pandas library, numpy, seaborn and matplotlib. The dataset contains 89 columns but we will limit our analysis to the following ten columns:\\nLet’s get started!\\nFirst let’s import the pandas library and read the csv file into a pandas dataframe and print the first five rows for the first sixteen columns:\\nWe can filter the dataframe so it only includes the ten columns we want:\\nFirst thing we can do is generate some statistics on the height column. The values are given as strings so we will want to convert them to a number we can use in calculations. Here we convert the height strings into centimeter values. We do this by multiplying the first element of the string, for example ‘5’7’, by 12.0 (inches) and we add the third element of the string ‘7’ (which is in inches) and multiply the final result by 2.54 (centimeters):\\nFirst we initialize a list called ‘Height_cm’ and we iterate over the height string value in the dataframe. We insert the conversion statements in between try except clause since there are cases where the strings contain non-numeric values, specifically there are missing height values. Whenever we reach a missing value we append an np.nan (numpy ‘not a number’) value. Finally we defined the new column to be the list we just populated.\\nNext let’s remove the missing values from the dataframe and print the first five rows:\\nWe can look at the mean and standard deviation in Height in cm:\\nWe can also generate a histogram of the heights:\\nWe can also wrap this up in a function and resuse it for any numeric column:\\nwhich generates the same output as above.\\nWe can also convert the weight column to kilograms for consistent use of the metric system. We do this by taking the first three elements of each string values in the ‘Weight’ column, convert them into floating point values and the divide by the conversion factor between Kg and pounds (2.20462):\\nWe can call our generate_statistics function with ‘Weight_kg’:\\nWe can also look at a the frequency in Nationality of each player using the ‘Counter’ method from the collections module:\\nWe can limit the counter to output only the five most common nationalities:\\nAs you can see England is the most frequent nationality with 1,657 records, followed by Germany with 1,195 and Spain with 1,071. We can display this output in a bar chart:\\nAgain, we can define a function that let’s us plot the most common values for any categorical column:\\nIf we call the function with ‘Position’ we get:\\nNext let’s convert the value (which is the market value for the player) and the wage into numeric values we can use in calculations. First lets truncate the string values in the ‘Value’ column such that we remove the ‘Euro’ and the ‘M’. The we can then convert each string into a floating point and multiply by 1e6:\\nWe can do the same for ‘Wage’, but here we multiply by 1e3:\\nWe will also want to convert the string values in the ‘Age’ column into integers:\\nFrom here we can generate a heat map from the numerical columns which will show us how strongly correlated each variable is with the other. Let’s filter the dataframe so that it only includes numerical values and generate a heat map from the resulting dataframe:\\nWe can also use box plots to visualize the distibution in numeric values based on the minimum, maximum, median, first quartile, and third quartile. If you are unfamiliar with them take a look at the article Understanding Boxplots.\\nFor example the distribution in market value for England, Germany and Spain:\\nAnd the distribution in age:\\nThe distribution in weight:\\nAnd height:\\nI’ll stop here but feel free to play around with the data and code yourself. The code from this post is available on GitHub. Thank you for reading!\\nWritten by\\n'},\n",
       "  {'author': 'Christian Hubbs',\n",
       "   'link': 'https://towardsdatascience.com/multi-armed-bandits-and-reinforcement-learning-dc9001dcb8da?source=tag_archive---------10-----------------------',\n",
       "   'title': 'Multi-Armed Bandits and Reinforcement Learning - Towards Data Science',\n",
       "   'claps': 21,\n",
       "   'text': 'Multi-armed bandit problems are some of the simplest reinforcement learning (RL) problems to solve. We have an agent which we allow to choose actions, and each action has a reward that is returned according to a given, underlying probability distribution. The game is played over many episodes (single actions in this case) and the goal is to maximize your reward.\\nAn easy picture is to think of choosing between k-many one-armed bandits (i.e. slot machines) or one big slot machine with k arms. Each arm you pull has a different reward associated with it. You’re given 1,000 quarters, so you need to develop some kind of strategy to get the most bang for your buck.\\nOne way to approach this is to select each one in turn and keep track of how much you received, then keep going back to the one that paid out the most. This is possible, but, as stated before, each bandit has an underlying probability distribution associated with it, meaning that you may need more samples before finding the right one. But, each pull you spend trying to figure out the best bandit to play takes you away from maximizing your reward. This basic balancing act is known as the explore-exploit dilemma. Forms of this basic problem come up in areas outside of AI and RL, such as in choosing a career, finance, human psychology, and even medical ethics (although, I think my favorite proposed use case relates to the suggestion that, due to its richness, it be given to Nazi Germany during WWII, “as the ultimate form of intellectual sabotage.”).\\nWe introduce multi-armed bandit problems following the framework of Sutton and Barto’s book (affiliate link) and develop a framework for solving these problems as well as examples. In this post, we’ll focus on ε−greedy, ε−decay, and optimistic strategies. As always, you can find the original post here (which properly supports LaTeX).\\nTo get started, let’s describe the problem in a bit more technical detail. What we wish to do, is develop an estimate Qt\\u200b(a):\\nWhere Qt(a) is the estimated, expected reward (Rn\\u200b), when action An\\u200b is taken at step n. We’re going to iteratively build a model that will converge towards the true value of each action. We’re going to use a Gaussian (normal) distribution for all of the underlying probability distributions that we’ll explore so that the mean corresponds to the true value (after all, given enough samples, we would expect our rewards to converge to the mean of the selected action).\\nThe simplest way to proceed is to take the greedy action or take the action we think will maximize our reward at each time step. Another way of writing this is:\\nWe can denote this maximum expectation or greedy action as A*n. This is the exploit side of our aforementioned explore-exploit dilemma, and it makes lots of sense if the goal is to maximize our reward. Of course, doing this repeatedly only works well once we have a good sense of our expected rewards for each actions (unless we get rather lucky). So, we need to figure out an algorithm that explores enough of our search space so that we can exploit the best actions.\\nBefore jumping into this, there’s one last concept to introduce. In typical RL applications, we may need hundreds of thousands of iterations, if not millions or more. It quickly becomes very computationally intensive to run simulations of these sorts and keep track of all that data just to calculate the average reward. To avoid this, we can use a handy formula so that all we need to track are two values: the mean and number of steps taken. If we need to calculate the mean at step n, m_n, we can do it with the previous mean, m_n−1\\u200b and n as follows:\\nWith that, we can start to develop strategies for solving our k-bandit problems.\\nWe briefly talked about a pure-greedy method, and I indicated that on its own it won’t work very well. Consider if you implement a pure-greedy method, you take one action, A_n=a_1\\u200b, at n=1 and get a reward. Well, then this becomes your highest reward (assuming it is positive) and you simply repeat a_1 \\u200b∀ n (take action a_1\\u200b for all steps n). To encourage a bit of exploration, we can use ε-greedy which means that we explore another option with a probability of ε. This provides a bit of noise into the algorithm to ensure you keep trying other values, otherwise, you keep on exploiting your maximum reward.\\nLet’s turn to Python to implement our k-armed bandit.\\nWe’re going to define a class called eps_bandit to be able to run our experiment. This class takes number of arms, k, epsilon value eps, number of iterations iter as inputs. We\\'ll also define a term mu that we can use to adjust the average rewards of each of the arms.\\nFirst the modules:\\nIgnore the %matplotlib inline if you’re building this outside of Jupyter.\\nThere are plenty of different ways to define this class. I did it so that once we initialize our problem, we just call the run() method and can examine the outputs. By default, the average rewards for each arm are drawn from a normal distribution around 0. Setting mu=\"sequence\" will cause the rewards to range from 0 to k-1 to make it easy to know which actions provide the best rewards when evaluating the results and which actions were taken. Finally, you could also set your own average rewards by passing values to mu.\\nLet’s set up some comparisons using different values of ε\\\\epsilonε. For each of these, we’ll set k=10, run 1,000 steps for each episode and run 1,000 episodes. After each episode, we will reset the bandits and copy the averages across the different bandits to keep things consistent.\\nLooking at the results, the greedy function under performs the other two consistently, with ε=0.01 coming in between the two and ε=0.1 performing the best of the three here. Below, we can see the effect is clearer using the sequence argument, and can get a feel for how often the optimal action is taken per episode because the averages remain consistent across episodes.\\nViewing the average selection of the algorithms, we see why the larger ε value performs well, it takes the optimal selection 80% of the time.\\nPlay around with the different values of both ε and k to see how these results change. For example, decreasing the search space would likely benefit smaller values of ε as exploration would be less beneficial and vice versa. Additionally, increasing the number of iterations will begin to benefit the lower value of ε because it will have less random noise.\\nThe ε-greedy strategies have an obvious weakness in that they continue to include random noise no matter how many examples they see. It would be better for these to settle on an optimal solution and continue to exploit it. To this end, we can introduce ε-decay which reduces the probability of exploration with every step. This works by defining ε as a function of the number of steps, n.\\nWhere β<1 is introduced as a scaling factor to reduce the scaling rate so that the algorithm has sufficient opportunity to explore. In this case, we also include +1 in the denominator to prevent infinities from appearing. Given this, we can make a few small changes to our previous class of bandits to define an eps_decay_bandit class that works on the same principles.\\nNow running the code:\\nThe ε-decay strategy outperforms our previous best algorithm as it sticks to the optimal action once it is found.\\nThere’s one last method to balance the explore-exploit dilemma in k-bandit problems, optimistic initial values.\\nThis approach differs significantly from the previous examples we explored because it does not introduce random noise to find the best action, A*_n\\u200b. Instead, we over estimate the rewards of all the actions and continuously select the maximum. In this case, the algorithm explores early on as it seeks to maximize its returns while additional information allows the values to converge to their true means. This approach does require some additional background knowledge to be included in the set up because we need at least some idea of what the rewards are so that we can over estimate them.\\nFor this implementation, we don’t need a new class. Instead, we can simply use our eps_bandit class and set ε=0 and provide high, initial values for the estimates. Also, I like to initialize the pull count for each arm as 1 instead of 0 to encourage slightly slower convergence and ensure good exploration.\\nWe can see that, in this case, the optimistic initial value approach outperformed both our ε−greedy and the ε−decay algorithms. We can see too, the estimates the algorithm has for each of arms in the last episode.\\nThe estimates are far off the actual rewards in all cases except the one with 977 pulls. This highlights a lot of what we’ll be doing in reinforcement learning more generally. We don’t necessarily care about acquiring accurate representations of the environment we are interacting with. Instead, we intend to learn optimal behavior in those situations and seek to behave accordingly. This can open up a whole discussion about model-free versus model-based learning that we’ll have to postpone for another time.\\nThere are other bandit methods that we will explore such as the gradient bandits, upper confidence bound (UCB) methods, and nonstationary problems. Plus, many others like dueling bandits, cluster bandits, collaborative filtering bandits, spatially correlated bandits, distributed bandtis, adversarial bandits, and contextual bandits which are open to exploration. Starting to wonder if we haven’t been hit with intellectual sabotage...\\nWritten by\\n'},\n",
       "  {'author': 'Andrew Scott',\n",
       "   'link': 'https://medium.com/swlh/welcome-to-python-meet-the-dunders-41026b2a7e36?source=tag_archive---------11-----------------------',\n",
       "   'title': 'Welcome to Python, Meet the Dunders - The Startup - Medium',\n",
       "   'claps': 25,\n",
       "   'text': '“Dunder” method name is the common pronunciation for python’s built-in method names that start and end with double underscores. Since “Dunder” is easier to say than “double underscore”, the name stuck. These methods are also sometimes referred to as “Special Methods” or “Magic Methods”. However, the only thing magic or special about these methods is that they allow you to include built-in type behavior in your custom classes.\\nWhile we sometimes treat these methods as a special language feature, there’s really nothing special about them. An introductory understanding of how to use dunder methods, and which ones are available can help you create much more intuitive and easy to use classes.\\nIn this post we’ll look at several of the dunder methods used for object creation, representation, and comparison. In later posts we will explore other dunder methods such as emulating containers, emulating numeric types, and more.\\nIf you’ve written any python you’ve almost definitely used at least one dunder method, whether you realized it or not.\\n__init__ is a method you’ve almost certainly used before. It is the dunder method used to initialize a new object. You may not have been aware of this, but when creating a new instance of an object, the dunder method __new__ is actually called first to create a new instance of your class, then the __init__ method is called to initialize that newly created instance.\\nA few notes about __init__; if you’re working with a derived class, you’re derived class’ __init__ method must explicitly call the base class’ __init__ method using super.__init__(). You must also never return a non-None value from __init__, as doing so will raise a TypeError.\\nThere are two common, and one less common dunder methods used for representing objects. You’ve undoubtedly called these methods many times, even if you weren’t aware you were using them.\\n__str__ is the most common of the representational methods. The __str__ method will return an “informal” printable representation of an object and return an str type. The __str__ dunder method is called by str() as well as the built-in format() and print() methods. Meaning, that anytime you use print() you’re also calling the __str__ method on whatever object you’re trying to print. Interestingly enough, if __str__ is not defined for an object, it will default to calling __repr__.\\nYou may be less familiar with __repr__, but it is the second of the common representational special methods and it is used to return the “official” string representation of an object. If an accurate representation of the object cannot be achieved due to object complexity, returning a useful description of the instance is also acceptable. The __repr__ method is called by repr().\\n__bytes__ computes a byte-string representation of an object and returns an object of type bytes.\\nThe so-called “rich comparison” methods will return a bool type and are used implement the backing methods for comparison operators such as == , != , < , > and more. I won’t go over all of these methods here, since you probably see where this is going, but these are the dunder methods called when you compare objects.\\n__eq__ is the dunder method used for checking equality between objects. This is pretty self-explanatory for strings and numeric objects, but what about more complex objects? The __eq__ method goes hand in hand with the __hash__ method, which as its name might suggest, takes a hashed collection (set, dict, etc.) and returns an integer. If a class does not define an __eq__ method it should also not define a __hash__ method.\\nBeware of incomplete implementations of __eq__ and other comparison methods, otherwise you may get unexpected results.\\nIt’s also interesting that you don’t actually need to define the __ne__ in cases where you have already defined __eq__ as python will just assign __ne__ to the inverse of __eq__. The opposite is actually also true, if __eq__ is not defined but __ne__ is, python will assign __eq__ to the inverse of __ne__ — however strongly it’s advised to define both or __eq__ only to reduce confusion.\\nHopefully you enjoyed this quick look at some of the common dunder methods. In Part 2 we’ll look at attribute access, emulating container types, and emulating numeric types.\\nHey, I’m Andrew Scott, a software developer and the creator of Ochrona. Ochrona focuses on improving python security by providing insights into your project’s dependencies and doing so with a major focus on Developer Experience (DX). Sign up for our Alpha program now!\\nWritten by\\n'},\n",
       "  {'author': 'Raivat Shah',\n",
       "   'link': 'https://medium.com/better-programming/string-validators-in-python-84aca562fd25?source=tag_archive---------12-----------------------',\n",
       "   'title': 'String Validators in Python - Better Programming - Medium',\n",
       "   'claps': 110,\n",
       "   'text': 'Often times, we work with strings and want to quickly check if they are of a particular type or contain specific data. While we can do the same by either storing the characters of the string in a data structure or explicitly writing down statements to verify, string validators in Python can simplify our task. This can be useful when validating a password string or any other user input. String validators are essentially functions that allow us to check if a string meets certain criteria.\\nThis post is an introduction to string validators and assumes no prior knowledge of them. Basic prior knowledge of programming in Python, however, is expected. Let’s get started.\\nA string is alphanumeric if all its characters are either digits (0–9) or letters (uppercase or lowercase). The method str.isalnum() checks if the string str is alphanumeric. For example:\\nNote that the method checks if all the characters in a string are alphanumeric. Thus, it returns false even when it contains non-alphanumeric characters along with alphanumeric characters. Since an empty string doesn’t contain any digit or alphabet, str.isalnum() returns false when applied to it:\\nA string is alphabetic if all of its characters are alphabets (uppercase or lowercase). The method str.isalpha() checks if the string str is alphabetic. For example:\\nSimilar to str.isalnum() , str.isalpha() returns false when applied to an empty string. This is intuitive because the empty string doesn’t contain any alphabets.\\nThe method str.isdigit() checks whether all the characters of the string str are digits (0–9). For example:\\nThe method str.islower() checks whether all the cased characters of a given string str are lowercase. Thus, it returns false even if there is one uppercase letter and returns true if there is none. For example:\\nThe method str.isupper() checks whether all the cased characters of a given string str are uppercase. Thus, it returns false even if there is one lowercase letter and returns true if there is none. For example:\\nThis is a common scenario when building web forms. The method str.isspace() checks if the given string is all whitespace or not. For example:\\nIntuitively, we refer to this method as “is blank.”\\nThe method str.istitle() checks if the given string is in title case. In Title Case, all words should start with capital letters and all words should be separated by a whitespace character.\\nFor example:\\nThat’s it! We’ve introduced ourselves to string validators and seen how they work. You can now proceed to validate your knowledge (instead of a string) by solving the following problem from HackerRank:\\nI hope this article helped you. Happy hacking!\\nWritten by\\n'},\n",
       "  {'author': 'Jamie Bullock',\n",
       "   'link': 'https://medium.com/better-programming/simple-audio-processing-in-python-with-pydub-c3a217dabf11?source=tag_archive---------13-----------------------',\n",
       "   'title': 'Simple Audio Processing in Python With Pydub - Better Programming - Medium',\n",
       "   'claps': 126,\n",
       "   'text': 'Sometimes, in a script or app, we need to perform audio processing tasks.\\nThese may include:\\nAll of these can be achieved using Pydub, a simple, well-designed Python module for audio manipulation. Pydub is my go-to tool for basic audio scripts. In the words of the PyDub authors:\\n“Pydub lets you do stuff to audio in a way that isn’t stupid.”\\nPydub can be installed with pip, which comes with all recent versions of Python. Simply type:\\nAnd we are ready to use Pydub.\\nThe main class in Pydub is AudioSegment. An AudioSegment acts as a container to load, manipulate, and save audio.\\nTo create our first audio script, we need a test audio file, this can be any supported format such as WAV, MP3, or AIFF. For the purposes of this tutorial, we’re going to download a file as part of the script using urllib.request.\\nThis is what the result sounds like:\\nNow that we have some audio loaded, we can do various forms of manipulation:\\nNotice how, in the final line of code, we chain the fade_in() and fade_out() operations. This is because every operation returns an AudioSegment instance.\\nThis is what the result sounds like:\\nIn addition to manipulating individual audio segments, we can layer and mix different segments with different levels and pan settings.\\nNotice that, in the last line, we use the Python “slice” operator to slice beat by length milliseconds.\\nThis is because our beat AudioSegment is twice as long as loop2. Slicing by length means we mix the first half of beat with loop2.\\nThis is what the result sounds like:\\nAn alternative approach would be to supply the loop=True argument to overlay(). This automatically loops the shorter segment as many times as needed to align with the longer one.\\nIn the next example, we will bring everything together by applying pan, filter, and reverse effects to the audio.\\nIn our argument to the pan() method, we supply a value between -1 and 1 where -1 is left, 1 is right, and 0 is center.\\nThis is what the result sounds like:\\nAs a final step, we could save our new audio loop to a new file. This can be in any supported audio format:\\nAside from loading and processing audio files, Pydub can also synthesize new tones.\\nThese can be sine, square, or sawtooth waves, at any frequency. It can also generate white noise. Tones can be turned into AudioSegment and combined like regular audio files.\\nIn the following example, we’ll use the Sine class to generate sine tones for the first 15 intervals in the harmonic series.\\nNotice how we use the += operator to append each 200ms sine tone to the end of our empty segment.\\nThis is what the result sounds like:\\nPydub is fantastic for simple audio tasks. However, if you want to do more complex processing such as speeding up or slowing down sounds, changing pitch, or applying time-varying effects, it’s not the best option.\\nAlso, one of the advantages of Pydub is its pure Python implementation. This minimizes native dependencies.\\nHowever, a downside of this is that Pydub is not very efficient. For processing large numbers of files as fast as possible, SoX or PySoX may be preferred.\\nThe full code from this tutorial can be found in this gist.\\nThanks for reading. If you have any suggestions for good Python audio frameworks or nice applications of Pydub, I’d love to hear them in the comments below.\\nWritten by\\n'},\n",
       "  {'author': 'Andreas Stöckl',\n",
       "   'link': 'https://towardsdatascience.com/animated-information-graphics-4531de620ce7?source=tag_archive---------14-----------------------',\n",
       "   'title': 'Animated Information Graphics - Towards Data Science',\n",
       "   'claps': 53,\n",
       "   'text': 'Animated information graphics of various datasets are a popular topic on youtube, for example, the channel\\nhas almost a million subscribers. I will show in this article some examples of such infographics on my dataset of news articles from Austrian newspapers on politics, and how to create the animations with Python and Plotly (https://plot.ly/).\\nIn the article https://medium.com/datadriveninvestor/topics-in-online-news-before-the-austrian-elections-2019-d981facbd0dd, I analyzed the topics before the elections. In this post, I want to look back at the news from the second half of the year 2019, and use animated graphics to analyze the news over time. I collected and analyzed more than 22,000 news articles from 12 Austrian online daily newspapers.\\nThe first video shows day by day how often each party has been named up to that day. It is counted from July until the end of the year. In the end, the ÖVP is just ahead of the FPÖ and “NEOS” is clearly at the end.\\nIn the second video, the number of times each party was mentioned per newspaper per week is shown. This is also shown animated for the entire second half of the year. It is noticeable that individual media have a clear preference for some parties, and for example, NEOS was mentioned very little outside of ORF. The week of the election of the National Council is clearly visible in the increased number of reports and mentions. The period of coalition negotiations after the election can be seen in the increased naming of the “Greens”. Many other aspects can often only be seen after watching the video several times.\\nIn order not only to review the quantity of reporting I have also tried to show the dominant contents in further visualization. The most common words are shown in a “Wordcloud” day by day. The dominating personalities stand out clearly, but also the election period is clearly recognizable by the changing choice of words. Here, words such as “percent” or “election results” dominate.\\nI loaded a data frame with the articles and some packages for data frame manipulation and date handling.\\nAfter counting the mentions of the political parties in the articles the data frame was extended with a column for each party.\\nThen some lists for the political parties, the newspapers and the days to animate were created. For each party (and newspaper) the entries are summed up and a bar chart with “Plotly” is generated. These are labeled with the number of entries. To get smoother transitions of the animations, 15 intermediate steps are calculated between the bar charts of two days, which are interpolated linearly.\\nAfter some formatting and labeling of the graphics, they are saved as numbered PNG files. This sequence of images can then be assembled into a movie using software such as Adobe After Effects.\\nTo create the “word animation” I used the Python package “Wordclouds” (https://github.com/amueller/word_cloud). This package calculates a “word cloud” for each day and saves it again as a sequence of PNG files.\\nIn the end, I used After Effects to merge the videos shown above and added some captions.\\nWritten by\\n'},\n",
       "  {'author': 'Favian Hazman',\n",
       "   'link': 'https://medium.com/@vianhazman/simulating-jakartas-brt-real-time-bus-tracking-using-kafka-and-docker-3f09a96234ea?source=tag_archive---------15-----------------------',\n",
       "   'title': 'Simulating Jakarta’s BRT Real-Time Bus Tracking Using Kafka and Docker',\n",
       "   'claps': 118,\n",
       "   'text': 'Better utilization of technology has enabled cities to interact and provide better services for its citizens. One of the main problems of cities around the world, and of course Jakarta is mobility. By acquiring public transportation data at lowest possible latency such as location streaming and density, it would improve citizen’s experience in commuting and even better interoperability between public transportations. We built a real-time visualization platform for Jakarta’s very own bus rapid transit, Transjakarta (Thank you Trafi for the inspiration!).\\nOn my final year as an IS student, i took Big Data Management course. For the final project, we’re challenged to build a paper and proof-of-concept (PoC) of a big data use-case be it data engineering or analytics and prediction scope with a technology of our choice. Our team of 4 (kudos to Wikan, Usama and Edwin!) chose Apache Kafka as the main technology for our project.\\nAs an introduction, Apache Kafka is a distributed streaming platform build atop of a message queue mechanism of publish and subscribe. According to Apache Kafka’s official documentation, Apache Kafka is fully scalable, distributed and fault tolerant. It means that is is suitable for high volume yet high availability use-cases such as real time tracking.\\nThe whole architecture for the project looks like this\\nFor the streaming system, we run two brokers (instances) of Apache Kafka to proof the high availability and fault-tolerant aspect of the system.\\nSince we got the data as a REST API fetches, we have to simulate the “asynchronous” part as the producer. For this case, we use Apache Airflow; a scheduler plaform to fetch the endpoint and publish messages every five seconds. The simulator script divides the message into three topic streams:\\nOn the consumer side, we have two use cases. The first one is visualization and realtime bus following. The second one is data storing to simulate stream data ingestion on data warehouse.\\nFor the data storing use case, we only use SQLite3 as the database due to time constraint (we admit, we are deadliners at the time😅). However since the trigger is eventual, we haven’t handle the possible cases of data loss in the system. For future improvement, it is possible by commiting the message offset between the consumer and the broker.\\nWe had so much fun while building the consumer’s platform using Django since we apply so much new things such as threading, queuing and Kafka’s Python API and websocket. And also, Mapbox GL’s Javascript library is the MVP of this project🔥🔥🔥.\\nWith limited computing resource and development timeline, Docker helped us on building, provisioning and orchestrating the instances. We’ve managed to run Airflow, PostgreSQL, Zookeeper, multiple Kafka brokers, Django, Redis, Debezium (plus Kafka Connect) without any hassle (except hours of figuring out the docker-compose script and the network configuration)\\nWe have so much fun on this project from brainstorming to presenting the project. Please reach us out via the comment section if you have any feedback or anything to share about this project or data engineering. The source code for the producer can be found here, and here it is for the consumer part. Let’s collaborate and learn😀\\nRecommended readings and references:\\n[PDF E-Book] Kafka: The Definitive Guide\\n[Medium] Routing Kafka messages to browser over web socket\\nWritten by\\n'},\n",
       "  {'author': 'Jackson Gilkey',\n",
       "   'link': 'https://towardsdatascience.com/intro-to-graphs-in-python-using-networkx-cfc84d1df31f?source=tag_archive---------18-----------------------',\n",
       "   'title': 'Creating Graphs in Python using Networkx - Towards Data Science',\n",
       "   'claps': 104,\n",
       "   'text': 'If you’re interested in doing Graph Theory analysis in Python and wondering where to get started then this is the blog for you. We’ll start by presenting a few key concepts and then implementing them in Python using the handy Networkx Package.\\nThe first thing you’ll need to do is install the Networkx package on your machine. Using Pip its as easy as:\\nOnce installed import the package and Initialize a graph object\\nAdd the first two nodes and an edge between them\\nAt this point our graph is just two connected nodes\\nAdding edges one at a time is pretty slow but luckily we can also add lists of nodes and lists of edges where each edge is represented by a node tuple.\\nOur Graph should now look something like this\\nWe can see a list of nodes or edges by printing these attributes of our graph.\\nIt is also possible to define nodes as strings.\\nMost importantly each node can be assigned any number of attributes which are then stored in dictionaries.\\nTo make this data more manageable feed the output of nodes.data() into a dict() function to get it into a lovely nested dictionary where each node is a key.\\nIn order to find the max clique we’ll need to import the approximation package from networkx since it is not included in the default import\\nNow in order to test the max clique we’ll create a clique of size 3 in our current graph by adding edge (4,6)\\nSo the set of vertices {4,5,6} contain our max clique of size 3 and that vertex set is exactly what the max_clique function will return.\\nTo see the induced subgraph of that vertex set, we need to combine the above with the subgraph method\\nWhich will give us the following complete 3-vertex graph.\\nFinal Thoughts and Questions\\nThere are a number of graph libraries out there for Python but I chose Networkx for its readability, ease of setup and above all for its excellent documentation. If you have any further question or wish to explore the library more please refer to the official documentation.\\nSources\\nhttps://networkx.github.io/https://en.wikipedia.org/wiki/Induced_subgraphhttps://en.wikipedia.org/wiki/Clique_(graph_theory)\\nWritten by\\n'},\n",
       "  {'author': 'Trilok Chowdary Maddipudi',\n",
       "   'link': 'https://medium.com/byteridge/image-processing-87036b925208?source=tag_archive---------19-----------------------',\n",
       "   'title': 'Edit pictures like a PRO using Python - Byteridge - Medium',\n",
       "   'claps': 12,\n",
       "   'text': 'How do Developers with poor Photoshop skills edit pictures beautifully?\\nFollow the blog to learn\\nImage processing or “image filtering” refers to modifying an image. It’s a key topic in image editing and computer vision, where it may be used to reduce noise or enhance certain features, among other things. The basic unit of image filtering is the kernel, which is the topic of today’s post. Before we get into kernels proper, however, let’s first ensure that we have a good understanding of what an image actually is.\\nConsider the following photograph of a pastry,\\nA digital image consists of pixels. In a color image, like the one above, each pixel is usually described in terms of the RGB color space. In other words, each pixel is represented by three channels: red, green, and blue. The bit depth, or color depth, of an image determines the range of values each channel can take on. In an 8-bit image, each channel can take on 28 = 256 values, from 0 to 255. 0 indicates that the intensity of a particular color is zero (black), and 255 indicates that the intensity of a particular color is maximum. So, for example, [R=0, G=0, B=0] would be black, [R=255, G=0, B=0] would be bright red, [R=0, G=0, B=255] would be bright blue, [R=127, G=0, B=127] would be an intermediate shade of purple, and so on.\\nIn a grayscale 8-bit image, there’s only one channel, and its intensity is represented by a value from 0 to 255, where 0 is black, 255 is white, and there are 254 shades of gray in between (which happens to be 204 more than the title of a certain novel). This is what the image above looks like when converted to grayscale:\\nIf we zoom in on a 20 pixel x 20 pixel region of this image, we can discern the individual pixels that form the image, like so:\\nWhile we see each pixel as a hue of gray, remember that, as we just discussed, each pixel is actually represented digitally by a number from 0 to 255. That means the digital representation of the 20 x 20 selection of the image looks like this:\\nNotice how brighter pixels have higher values, corresponding to higher intensities. Conversely, darker pixels have lower values, corresponding to lower intensities. If we were looking at the color version of the image, each pixel would have three numbers (RGB) instead of just one. I chose the grayscale version as an example because it’s easier to visualize, but the RGB representation, or any other color space, for that matter, works essentially the same way, regardless of the number of channels.\\nA kernel is essentially a mask or a filter that modifies the value of a pixel based on the values of its surrounding pixels. These surrounding pixels are termed the central pixel’s “neighborhood pixels.” Let’s zoom in further and examine an arbitrary 3×3 square neighborhood from our previous selection:\\nHere, we’re examining the neighborhood of the central pixel, which has a value of 99. The kernel will utilize the values from each pixel in the neighborhood (including the central pixel) to determine the new value for the central pixel. So, what does the kernel actually look like? A kernel is a matrix of the same shape as the neighborhood, and the value of each element of the kernel represents the weight given to the corresponding pixel from the neighborhood. A 3×3 kernel has the following basic form:\\nwhere w1 through w9 are the weights given to each pixel in the neighborhood. The pixel in the upper left corner would be given weight w1, the central pixel would be given weight w5, and so on. Representing the neighborhood in a similar fashion with the letters “a” through “g”:\\nwhere e is the central pixel (corresponding to the pixel with a value of 99 from the example above) and the others are its neighborhood pixels. Then the convolution of the neighborhood with the kernel is written as:\\nwhere ∗ is the convolution symbol. The new value of the central pixel is the weighted sum of all the pixels in the neighborhood. In other words, the new value is given by the sum of the elements resulting from element-wise multiplication of the two matrices:\\nenew=w1a+w2b+w3c+w4d+w5e+w6f+w7g+w8h+w9i\\nGoing back to our example, let’s apply a standard 3×3 sharpen kernel. A sharpen kernel ignores the four corner pixels, subtracts the value of each pixel directly adjacent the central pixel, and multiplies the value of the central pixel by 5:\\nWriting this out: (−1)(134)+(−1)(83)+(−1)(80)+(−1)(119)+(5)(99)=79.\\nThis is done for every pixel of the source image to find the new value of each pixel in the resulting image. Here’s what applying a sharpen filter to an entire image looks like:\\nThe comparison doesn’t require much commentary. By increasing the intensity of each pixel relative to its neighbors, the resulting image appears considerably sharper than the original. What if we want to blur the image? This is what a standard “box blur” convolution looks like:\\nWith a box blur, we set the value of the central pixel equal to the average of all the pixels in its neighborhood (hence the division by 9). In essence, this “dilutes” each pixel. This is what it looks like applied to a whole image:\\nWhat if we want to blur the image more? Well, this is what happens when the size of the kernel (and the neighborhood) is increased to 5×5:\\nGenerally, increasing the size of the kernel/neighborhood amplifies the effect of the kernel.\\nWe’ve just barely scratched the surface. There exist numerous other kernels for performing a wide variety of image modifications. Fun fact: if you’ve ever used the filters (like sharpen or blur) in an image editor like Photoshop, these kernels and convolutions are what the software is running under the hood. In fact, programs like Photoshop and GIMP allow you to create custom filters by supplying your own kernel. Since kernels and convolutions are also ubiquitous in computer vision.\\nWritten by\\n'},\n",
       "  {'author': 'Magnimind',\n",
       "   'link': 'https://medium.com/@magnimind/what-is-the-python-programming-and-the-content-of-python-certification-5e116740bc77?source=tag_archive---------20-----------------------',\n",
       "   'title': 'What is the Python Programming and the content of Python Certification?',\n",
       "   'claps': 43,\n",
       "   'text': 'In technical aspects, Python is a high-level, object-oriented programming language that comes with integrated dynamic semantics primarily for app and web development. Though it’s often compared to JavaScript, Ruby etc, the major thing that sets apart Python programming language from others is its simplicity.\\nFirst released in 1991, Python has become one of the most popular programming languages across the globe.\\nThe huge popularity of Python programming language heavily relies on the unique features offered by it. Let’s have a look at the features of Python that set it apart from other programming languages.\\nPython is highly simple to get started with. It comes with a simple setup, easy to understand syntax and many practical applications that are heavily needed in web development. The syntax is relatively simple compared to other languages and a bunch of modules can be imported to make the code much shorter.\\nThere’re some straightforward, excellent tools available to work with Python code, particularly the interactive interpreter.\\nIt eliminates the need for learning special text editor, IDE, or anything else to begin using Python programming language. All you need to have a command prompt together with the interactive editor.\\nFor a beginner in the programming domain, starting with a language which is difficult to learn not only slows the pace of the learning but brings additional complexity also. With Python programming language, a beginner can be introduced to the fundamental concepts like procedures and loops quickly and probably can work with user-defined objects in his/her initial stage of learning.\\nPython’s hug syntactical simplicity lets a beginner use advanced or basic concepts, without much boilerplate code which is common in many other languages.\\nIf you’re familiar with programming languages like Java or C++, you’re probably aware that in order to run them, you’ve to compile them first. But in Python programming language, there’s no need to compile it. All you need to do is run the Python code without thinking about linking to libraries. Here, interpreted means the source code isn’t executed all at once, instead it’s executed line by line, which makes it easier to debug the code. In Python, the source code gets converted into ‘bytecodes’ which is then translated into the native language of a specific computer.\\nA programming language, which comes with the ability to model the real world, is considered object-oriented. It concentrates on objects and merges functions and data. Python has object-oriented features. Class mechanism of Python adds classes with a minimum number of new semantics and syntax. Here, multiple inheritances are also supported.\\nPython programming language is freely available which means it can be downloaded from the official site by anyone. Python being open-source means you can read its source code, modify it, use pieces of it, and distribute copies of it freely.\\nPython is a high-level language which means programmers don’t need to remember its system architecture or manage the memory. This feature makes Python highly programmer-friendly.\\nLet’s consider you’ve written code in Python programming language for a Mac machine and you want to run it on a Windows machine. There’s no need to make changes to perform it, meaning you can take a single code and run it on any other machine without having to write separate codes for separate machines. This portability is another key feature of this language.\\nPython programming language comes with extensive libraries which can be used to eliminate the need of writing own code for each and every single thing. There’re libraries for unit-testing, documentation-generation, regular expressions, databases, web browser, image manipulation, image, and a lot more.\\nSome of the Python code can be written in other languages such as C++ if required. This feature makes Python an extensible language which means you can extend it to other languages.\\nWe’ve already seen that Python programming language is surprisingly easy to learn and you can use it as your stepping stone into other frameworks and programming languages. Across the globe, Python is heavily used by some giant companies like Google, IBM, Nokia, Pinterest, Disney, Instagram, and many others. Since a lot of giant organizations depend on it, it’s extremely likely that you’ll never have a paucity of ways to earn good money using your Python skills.\\nIf you’re still feeling hesitant whether you should learn Python programming language, have a look at the following advantages that you’d be able to enjoy once you become a master of it.\\nThis is the biggest reason you should focus on learning Python programming now. Though R was considered perfect for data science tasks till sometime back, with lots of frameworks and libraries Python has become the most preferred language among data scientists across the globe these days. In addition, you can a lot more with Python than R. For instance, you can automate staff, create scripts, step into web development etc once you become a master in Python programming language.\\nThis is the second most important reason to learn Python now. The exponential growth of machine learning has attracted a huge number of businesses to leverage this technology. With algorithms becoming sophisticated day by day, we can expect to see more advanced and diverse implementations of machine learning in the technology domain. If you want to play around machine learning or do a pet project, Python is the only major language that makes it easy.\\nWith lots of good frameworks and libraries, Python programming language makes web development actually easy. For example, you can complete a task in minutes on Python, whereas it’ll take hours to be completed in PHP. There’s a huge number of web developers who’re using Python frameworks like Flask and Django to develop web applications in minimum time.\\nThis is another key reason to start your programming career with Python programming language. Presence, as well as the size of the community, can act as a great differentiator when you’re learning something new. You can find the answer to almost any of your queries related to Python in minutes with the help of its communities.\\nAnother key factor behind the huge popularity of Python is its multipurpose nature which means it’s not tied to only a single thing. For example, R is good on machine learning and data science but fails to add any value when it comes to web development. By learning Python programming language you’ll be able to do a lot of things: from doing data analysis to creating web applications to automating tasks to writing scripts.\\nWhen it comes to salary, Python professionals belong to some of the highest paid groups in the industry, especially those working in the machine learning, data science, and web development domains. While tech salaries greatly differ from one region to another, they stay in a very good range, anywhere between 70K and 150K based on the domain, location, and experience.\\nBased on popular job portals, it looks like having a Python programming certification under your belt can greatly help you get a decent job within a short time. In addition, the demand has increased exponentially these days because of the widespread implementation of data science and machine learning.\\nHaving a Python certification gives you a competitive advantage when you appear in front of the hiring managers. While competitions in the technology domain are rigorous, having such a certification can change the scenario completely for you.\\nA Python programming certification demonstrates that you not only have a robust understanding of the language but are determined to improve your skillset as well.\\nIn addition, having a certification greatly helps you move up the corporate ladder quickly. Lastly, by obtaining a Python programming certification you’ll be able to earn significantly more than those who don’t. If you’re interested to know about what you’ll learn by completing a Python certification course, here’s an overview of the content of Python certificate programs.\\nPython programming certification course can be taken by Project Managers and BI Managers, Software Developers, Big Data Professionals, Analytics Professionals, and anyone interested to build a career in Python. Beginners without any previous programming experience can also take up a Python certification course to step into the technology field quickly.\\nHowever, it’s extremely important to complete your Python programming certification course from a reputed institute. Things you need to review meticulously include the content of Python certificate program, whether the course is designed by industry professionals, facility to receive hands-on experience, job assistance after successful completion of the program, among others.\\nSubscribe to receive our top stories here.\\nWritten by\\n'},\n",
       "  {'author': 'Tanu N Prabhu',\n",
       "   'link': 'https://levelup.gitconnected.com/learn-the-python-len-function-49b20bb8edd9?source=tag_archive---------21-----------------------',\n",
       "   'title': 'Learn the Python len() Function - Level Up Coding',\n",
       "   'claps': 54,\n",
       "   'text': \"The len() Python function counts the number of items in an object. The object can be a string, tuple, dictionary, list, sets, array, and many more. The items here are the elements inside these objects, and the count represents the number of occurrences of these items in an object.\\nThe syntax of the len() function is:\\nHowever, in the later versions of Python (ie. Python v3.5.1+) the syntax is:\\nThe / indicates Positional-Only-Arguments. Don’t worry about it too much because that itself is a different concept. I’ll follow up with an article detailing that further.\\nThe len() function accepts exactly one argument. It does not take any keyword arguments. The argument should either be a sequence or a collection.\\nFailing to pass an argument or passing an invalid argument will raise a TypeError exception\\nNow you might be wondering what a sequence or collection is.\\nSequence: A group of items with deterministic ordering, meaning the order doesn't change. When you add items, the order would be retained, and you get the same order of items in return. Examples of a sequence are → strings, lists, tuples.\\nFor example, consider a list comprising the following elements as shown below:\\nThe order did not change in a sequence. This is what meant by deterministic order.\\nCollections: Unlike sequences, the collection has no deterministic ordering. Here, the order of items changes accordingly. Examples of collections are → Sets and Dictionaries.\\nConsider the below example of a set with the following elements:\\nAs seen above, the set function changed the order of the items. This is one of the main advantages of collections.\\nBelow is the clear chart which comprises examples of sequences and collections:\\nThe len() function returns the number of items in a container. For example, consider the container here to be a string of elements. In this case, len() returns the number of characters in the string.\\nBelow are examples and the usage of the len() function with different objects such as string, list, sets, tuple, and dictionary.\\nlen() does not work with integer values and the iterator values. Consider the below example:\\nIt also fails to work on iterators using for looping statements. Try it, I’m not joking.\\nOn converting the above example from integer to the string, len() works completely fine.\\nSee I told you, there was no problem when the value is of string type. If anybody of you guys knows the answer let me know, I am curious. By the way, the entire code can be found on my GitHub Repository below:\\nThis is a small article with a lot to learn about the len() function. Stay tuned for more updates. I hope you enjoyed reading the article. If you have any doubts regarding the content, then the comment section is all yours. Thank you guys for spending your precious time reading my article. Until then, Goodbye.\\nWritten by\\n\"},\n",
       "  {'author': 'Ankur Gangwar',\n",
       "   'link': 'https://medium.com/@ankur999gangwar/tool-for-getting-started-in-machine-learning-anaconda-92c815f2c0ef?source=tag_archive---------24-----------------------',\n",
       "   'title': 'Tool for getting started in machine learning : Anaconda',\n",
       "   'claps': 11,\n",
       "   'text': 'hello everyone,\\nthis blog is about what softwares you actually need to get started for machine learning or data science like stuff . There are many tools or IDEs out there like jupyter notebook , spyder , R-Studio etc.. and libraries also like numpy , pandas and scikit learn ( for Python ) and there are many others , so it can be a really boring task to download all the libraries or IDEs and install them in your local machine .\\nif you are a beginner in machine learning (or data science because fundamentals are same ) than there is a great package (or tool ) out there and that is ANACONDA .\\nthe great thing about anaconda is that it contains all the packages as well as libraries and IDEs that you need to get started for machine learning . There is another advantage and that is it is free for developing usage and available for Windows , Mac OS and Linux. there are several advantages with anconda package :\\nAnaconda can be downloaded from the official website , site is here : https://www.anaconda.com/distribution/\\nit can be installed on any of the three opearating systems :\\nfor windows : https://www.datacamp.com/community/tutorials/installing-anaconda-windows\\nfor Linux : https://www.digitalocean.com/community/tutorials/how-to-install-anaconda-on-ubuntu-18-04-quickstart\\nfor mac OS : https://www.datacamp.com/community/tutorials/installing-anaconda-mac-os-x\\nWritten by\\n'},\n",
       "  {'author': 'Mohammed Zaheeruddin Malick',\n",
       "   'link': 'https://medium.com/@mohammedzu/graph-coloring-in-pythonic-art-2c86a6da90cd?source=tag_archive---------26-----------------------',\n",
       "   'title': 'Graph Coloring in Pythonic Art - Mohammed Zaheeruddin Malick - Medium',\n",
       "   'claps': 50,\n",
       "   'text': 'Greedy BFS Graph coloring Algorithm in Python\\nPart IV and finale of the Holidays 2019 coding series... Happy 2020 Y’all\\nThis implementation illustrates Graph Coloring (An NP-Complete Problem) of US map using Greedy BFS.\\nPart I: https://medium.com/@mohammedzu/least-recently-used-lru-cache-in-pythonic-art-bda8a62fce36\\nPart 2:https://medium.com/@mohammedzu/least-frequently-used-lfu-cache-in-pythonic-art-fbaa63b34da9\\nPart 3:https://medium.com/@mohammedzu/directed-acyclic-graphs-dag-and-topological-sorting-b35cb869a66a\\nWritten by\\n'},\n",
       "  {'author': 'Margaryta Chepiga',\n",
       "   'link': 'https://medium.com/tumiya/releasing-macos-app-on-gcp-part-5-bbfab980cd38?source=tag_archive---------39-----------------------',\n",
       "   'title': 'Releasing macOS app on GCP — Part 5 - Tumiya - Medium',\n",
       "   'claps': 50,\n",
       "   'text': 'This is the last part of the series, you can find the rest below:\\nThis is the last part of the series, and it is regarding the issue that happened to me on Bitrise.\\nStep fails on bitrise, due to a non working Python executable.\\nThe weird thing with this issue is that, it doesn’t fail every time you run a pipeline. It’s random.\\nFirst time I run the pipeline, it succeeded. But then, I had to test some changes and trigger the pipeline several times in a short period of time. And that’s when I notices that it was passing once, and failing the next time 😟\\nNo matter what I tried nothing worked.\\nI set up the CLOUDSDK environmental variable, I digged into Bitrise VM’s, googled my way through. It seemed like no matter what I do it won’t work. I almost gave up. Until, Asad Mansoor, who saw my pain & struggle, suggested me to use one simple command. So just in case you run into same issue, here is a solution for you:\\nWe run the pipeline over 5 times to make sure it works every time. And it did.\\nWhy exactly did it happen?\\nI assume, that the python was not installed properly on any other machine. Since, simple reinstallation fixed it.\\nAlso keep in mind, you need to add the above line in the Script #1 before the commands that you already have there. So if you were following my series, your script will look like this now:\\nHope it helps, and saves you bunch of time!\\nWritten by\\n'},\n",
       "  {'author': 'Pangeran Bottor',\n",
       "   'link': 'https://medium.com/@pangeranbhp/must-be-a-string-is-incorrect-information-c0879c544a73?source=tag_archive---------43-----------------------',\n",
       "   'title': 'Must be a String is incorrect information. - Pangeran Bottor - Medium',\n",
       "   'claps': 19,\n",
       "   'text': 'The term must be a String while the definition can be any valid Python data.\\nMust be a String is incorrect information.For example, string, integer and tuple can be a dictionary key.\\nPlease refer to this:\\nHashability makes an object usable as a dictionary key and a set member, because these data structures use the hash value internally.(hashable term from https://docs.python.org/3/glossary.html)\\nWritten by\\n'}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [item for sublist in articles for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eden Au</td>\n",
       "      <td>342</td>\n",
       "      <td>https://towardsdatascience.com/4-numpy-tricks-...</td>\n",
       "      <td>NumPy is one of the most popular libraries in ...</td>\n",
       "      <td>4 NumPy Tricks Every Python Beginner should Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dominik Vacikar</td>\n",
       "      <td>28</td>\n",
       "      <td>https://medium.com/@chichikid/the-guys-who-wil...</td>\n",
       "      <td>Two weeks ago, I was sitting in a coffeeshop w...</td>\n",
       "      <td>The Guys Who Will Solve the Private Market - D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Farida Adamu</td>\n",
       "      <td>114</td>\n",
       "      <td>https://medium.com/@faridaadamu/hello-guys-goo...</td>\n",
       "      <td>Hello guys, good whatever-time-of-day it is in...</td>\n",
       "      <td>Introduction to Machine Learning - Farida Adam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kim Larsen</td>\n",
       "      <td>149</td>\n",
       "      <td>https://towardsdatascience.com/fire-your-bi-te...</td>\n",
       "      <td>The term “Business Intelligence” gained widesp...</td>\n",
       "      <td>Fire Your BI Team - Towards Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dana Alibrandi</td>\n",
       "      <td>459</td>\n",
       "      <td>https://blog.dash.org/announcing-the-release-o...</td>\n",
       "      <td>Today, Dash Core Group is proud to announce ou...</td>\n",
       "      <td>Announcing the Release of Dash Platform on Evo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  claps                                               link  \\\n",
       "0          Eden Au    342  https://towardsdatascience.com/4-numpy-tricks-...   \n",
       "1  Dominik Vacikar     28  https://medium.com/@chichikid/the-guys-who-wil...   \n",
       "2     Farida Adamu    114  https://medium.com/@faridaadamu/hello-guys-goo...   \n",
       "3       Kim Larsen    149  https://towardsdatascience.com/fire-your-bi-te...   \n",
       "4   Dana Alibrandi    459  https://blog.dash.org/announcing-the-release-o...   \n",
       "\n",
       "                                                text  \\\n",
       "0  NumPy is one of the most popular libraries in ...   \n",
       "1  Two weeks ago, I was sitting in a coffeeshop w...   \n",
       "2  Hello guys, good whatever-time-of-day it is in...   \n",
       "3  The term “Business Intelligence” gained widesp...   \n",
       "4  Today, Dash Core Group is proud to announce ou...   \n",
       "\n",
       "                                               title  \n",
       "0  4 NumPy Tricks Every Python Beginner should Le...  \n",
       "1  The Guys Who Will Solve the Private Market - D...  \n",
       "2  Introduction to Machine Learning - Farida Adam...  \n",
       "3           Fire Your BI Team - Towards Data Science  \n",
       "4  Announcing the Release of Dash Platform on Evo...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list of panda pkl to text file\n",
    "def convertPklToTxt(paths:list):\n",
    "    texts,titles = [],[]\n",
    "    for path in paths:\n",
    "        df = pd.read_pickle(path)\n",
    "        #clean up\n",
    "        #df['publication'] = \n",
    "        #df['title'] = \n",
    "        all_string = df['title'].copy(deep=True)\n",
    "        df['title'] = [x[0] for x in all_string.str.split(' - ')]\n",
    "        df['publication'] = [x[1] if len(x)>1 else None for x in all_string.str.split(' - ')]\n",
    "        df['text'] = df['text'].str.replace('\\nWritten by\\n',\"\\n\")\n",
    "        texts.append(df['text'])\n",
    "        titles.append(df['title'])\n",
    "    pd.concat(texts).to_csv('data/all_texts.txt',header=False,index=False)\n",
    "    pd.concat(titles).to_csv('data/all_titles.txt',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertPklToTxt(['data/raw_medium_articles_2017.pkl','data/raw_medium_articles_2018.pkl','data/raw_medium_articles_2019.pkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get('https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c')\n",
    "soup = BeautifulSoup(data.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.findAll('title')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claps  = soup.findAll('button')\n",
    "#unicodedata.normalize('NFKD', claps)\n",
    "for string in claps:\n",
    "    if 'claps' in string.get_text():\n",
    "        print(string.get_text().split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6224"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/raw_medium_articles_2017.pkl')\n",
    "df['title2']= [x[0] for x in df['title'].str.split(' - ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"Hi, I'm bob. What's your name? Don't have one? That's ok too!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "start_date = date(2013, 1, 1)\n",
    "end_date = date(2015, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for single_date in daterange(start_date, date.today()):\n",
    "    print(single_date.strftime(\"%Y/%m/%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
