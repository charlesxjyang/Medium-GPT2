{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Text-Generating Model w/ GPU",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlesxjyang/Medium-GPT2/blob/master/GPT_2_Text_Generating_Model_w_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "Lightly edited by [Charles Yang](http://charlesyang.io/)\n",
        "\n",
        "*Last updated: November 10th, 2019*\n",
        "*Last edited: January, 12th, 2020*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ac9621d7-4c81-4267-e3d3-453b1cc8263e"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "7ad9bafc-09ca-4c04-d466-387175ada579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan 12 21:18:19 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "315c478e-2ded-4e68-a2a9-f0b5e72a73a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 458Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 144Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 422Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:06, 209Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 266Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 110Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 161Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3ffb3d58-3a6f-407c-bf9a-4b68fe50381e"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"all_texts.txt\"\n",
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXAwyTO5xSgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_name = 'run1_355_final'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "9fbe4b65-d5fc-4c49-8a28-a37bc99f1c53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=200,\n",
        "              restore_from='fresh',\n",
        "              run_name=run_name,\n",
        "              print_every=10,\n",
        "              sample_every=100,\n",
        "              save_every=100\n",
        "              )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:21<00:00, 21.11s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 4526153 tokens\n",
            "Training...\n",
            "[10 | 17.10] loss=3.63 avg=3.63\n",
            "[20 | 25.94] loss=3.32 avg=3.48\n",
            "[30 | 34.76] loss=3.23 avg=3.39\n",
            "[40 | 43.62] loss=2.95 avg=3.28\n",
            "[50 | 52.46] loss=3.44 avg=3.31\n",
            "[60 | 61.30] loss=2.98 avg=3.26\n",
            "[70 | 70.13] loss=2.96 avg=3.21\n",
            "[80 | 78.95] loss=3.22 avg=3.21\n",
            "[90 | 87.75] loss=2.86 avg=3.17\n",
            "[100 | 96.59] loss=2.99 avg=3.15\n",
            "Saving checkpoint/run1_355_final/model-100\n",
            "======== SAMPLE 1 ========\n",
            " as you get older. The more you do this, the more you become aware of your own strength.\n",
            "You have found a new and exciting direction, but there is just one obstacle, the one you have to work out of your face every single day. You realize that you don’t have the passion, but you still have the desire, and you’ll be there in about a year or two. Then, it’s on another, perhaps not so good route.\n",
            "That was one of the reasons why I was so excited in the past: I could see myself in a decade or more as a climber. I wanted more than just riding, I wanted to be the best at it. And I’m here.\n",
            "I know I need to improve my fitness, because you may already be training more than me, but I know I have to be improving at the end of the day, no matter what, and I’m doing it. I’ve been watching this video and it’s helping me. I’ll talk about it, maybe you won’t, but I think you should get a copy. That’s it.\n",
            "At my current training level, I’m a bit behind in the weight, but I’m starting to come. I’ll give it another year, I’ll keep going, maybe my fitness will improve, maybe it will get even better. Maybe I’ll just keep riding.\n",
            "Until next time.\n",
            "\"\n",
            "\"…\n",
            "\"I wrote about the idea in my earlier blog post, but it seemed like the right time to go back and do it again. I want to share with you the steps to follow when building a new React component. I recommend you to skip the \"code sample\" section if you have any doubts before getting started.\n",
            "So in this article, we will use the first available React version in our project: 0.12.\n",
            "Since we need to build our React component, we want to take a look at the syntax and how it works. In particular, you need to know two things:\n",
            "1) What is the type of React component you need?\n",
            "2) Is your component React components or React state components?\n",
            "Before building your component, we want to define the state component. After defining the component, we want to set some initial values. This is very easy. The way you should do it in any React component is by creating a new state component, and extending it so that the state component is rendered whenever you need to update the state of the component.\n",
            "So in the first line of your component, you have to put in a value of React component. You can use any React component class to do this. A few React classes are included:\n",
            "React.Component — this class defines the React component.\n",
            "React.State — the type of your component to use as React state component.\n",
            "After you have created an instance of React component classes, you can update it using the update function provided by the state component.\n",
            "For example, let’s say we want to say goodbye to our current component and add our new state component:\n",
            "We just added another component:\n",
            "However, when we are done working on our previous component, our new component will render like:\n",
            "By the looks of it, our previous React component is gone. Now we want to replace our component by something different. For that, we need to render different React components for our component types.\n",
            "Let’s look at how exactly we can render different React components depending on how React components are defined.\n",
            "If you remember my previous posts, I included a new function component() which creates component() . This function takes any React component and calls the component method. What is the difference between a component() and component()?\n",
            "The reason is that when you create a React component, React doesn’t do anything to it. When components are created with Component , they just need to look like Components . Component is the class that extends Component so it acts as the standard way to define components. This means your React component would render like a Component with no extra attributes:\n",
            "However, if you have a Component class like this:\n",
            "You could write a component() function to do exactly the same thing as Component. That would work perfectly. The problem is that Component defines the attribute for React in component() , which isn’t going to have any effect on how React is rendered.\n",
            "So we have to extend React.Component so that it can render component() like we want it to do.\n",
            "That doesn’t look too big so you might say we have a way to do this. Let’s take a look.\n",
            "Let’s do our component() function like so:\n",
            "Now we have our component() function rendered by component() function instead of Component , which is great! But now we want to be able to update our component\n",
            "\n",
            "[110 | 133.46] loss=2.99 avg=3.14\n",
            "[120 | 142.28] loss=3.31 avg=3.15\n",
            "[130 | 151.11] loss=3.18 avg=3.15\n",
            "[140 | 159.95] loss=3.50 avg=3.18\n",
            "[150 | 168.80] loss=2.99 avg=3.17\n",
            "[160 | 177.64] loss=2.79 avg=3.14\n",
            "[170 | 186.48] loss=3.05 avg=3.14\n",
            "[180 | 195.30] loss=3.09 avg=3.13\n",
            "[190 | 204.14] loss=2.90 avg=3.12\n",
            "[200 | 212.98] loss=2.60 avg=3.09\n",
            "Saving checkpoint/run1_355_final/model-200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name=run_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "gpt2.copy_checkpoint_from_gdrive(run_name=run_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2e00f65d-5470-4e9e-ab81-25873f3e2c51"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name=run_name)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run1_355_final/model-200\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1_355_final/model-200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "d2fd6163-3984-4a39-8ea9-95c4fbc5ba15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "source": [
        "gpt2.generate(sess, run_name=run_name)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Product Details\n",
            "\n",
            "The new Guild Wars 2 is out, and we’ve got a brand new map (complete with an amazing new terrain-and-water-texture) to show you a few hours of the latest action.\n",
            "This map is from the Guild Wars 2: Rise of Iron DLC, and it’s pack-in-there-for-the-heroes-of-Iron-DLC-set. You can check it out here.\n",
            "Now, we’re not going to cover the maps and terrain, but you can check out the video below, which shows you the build-in-for-heroes.\n",
            "The map is not an official preview, but it’s worth a look.\n",
            "From the get-go, we’ve decided to focus on terrainy maps, with the newest terrain-and-water-texture. More on that soon.\n",
            "As you can see, this map is already packed-in for the heroes, which means we’re not going to cover the maps and terrain, but instead focus on the maps and terrain.\n",
            "The map is built on Guild Wars 2’s Guild Wars 2: Rise of Iron DLC pack. It contains three maps, and we’ve included two of the maps, Water and Ice.\n",
            "This map contains two maps, Water and Ice. Water features a new terrain-and-water-texture, and Ice features a brand new terrain-and-water-texture.\n",
            "As of now, we have no plans to cover any of the maps and terrain packs in this post.\n",
            "Once we’ve got all of the terrain and map set-in-for-heroes, we’ll be doing a full-on upcoming post, covering all the maps and terrain.\n",
            "Oh, and we’re still getting the Guild Wars 2: Rise of Iron DLC pack into the game, so that’s all we’re going to cover in this post.\n",
            "We’ve gone through this map pack-in-there-for-heroes of Iron DLC, and there’s a lot more to come.\n",
            "We’re still talking with the Guild Wars 2: Rise of Iron DLC team about all the terrain and map set-in-for-heroes in the game. Stay tuned.\n",
            "“We’re still looking for more ways to make the maps and terrain pack-in-there-for-heroes even more awesome.”\n",
            "After the maps are built, you can check out the terrain-and-water-texture by visiting this page.\n",
            "If you’re not already following Guild Wars 2: Rise of Iron in the Arena, and are interested in the DLC, you can check it out here.\n",
            "Guild Wars 2: Rise of Iron is the latest expansion for Guild Wars 2, and it includes new maps, terrain, and other features. It began as a small, one-shot, free expansion for the game (starting in October 2015), and has since been a full-fledged expansion.\n",
            "Guild Wars 2: Rise of Iron launched in November 2015, and the expansion has now been released in a major expansion, the Heart of Thorns, and it’s by far the most popular expansion.\n",
            "You can find out more information about Guild Wars 2: Rise of Iron, and download it from our official site.\n",
            "Guild Wars 2: Rise of Iron is available now in the Americas on PS4, Xbox One, and PC. For more information, check out the official site, and don’t forget to follow us on Twitter.\n",
            "\"\n",
            "\"I’ve been thinking a lot about algorithms and data science lately — the first step in a process of becoming a more effective data scientist. I’ve also been thinking a lot about how much data science is still taught in education — and how much more we need to do to improve the way we’ve done it.\n",
            "I’ve finally finished the first part of the set of data science textbooks I’ve been working on. I’m now taking the first step toward becoming a data scientist in the real world.\n",
            "Data science is a big part of my job as a data scientist. Even when I’m not trying to become a data scientist, I’m still learning how to use data science. I’ll be posting my progress on a daily basis. You can check out my daily progress on the blog — or you can just follow me to my Twitter (and, yes, follow me on Facebook).\n",
            "Please enable JavaScript to view the comments powered by Disqus.\n",
            "\"\n",
            "\"I’ve been working on a set of data science textbooks that will be added to the website on Wed, May 25th.\n",
            "I’ve started with all the examples for the examples in the first book. The last part is for the simple models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "038ad8c9-7efb-4c46-9501-b72bc4ab3bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              run_name=run_name,\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              prefix=\"Data science is \",\n",
        "              nsamples=15,\n",
        "              batch_size=1\n",
        "              )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data science is ・“data”. Data science is data, data, data. Data science is data, data, data. Data science is data, data, data. Data science is data, data, data. Data science is data, data, data. Data science is data, data, data. Data science is data, data, data. Data science is data, data, data. Data science is data, data, data. Data science is data, data, data. Data science is\n",
            "====================\n",
            "Data science is  a tool that allows us to value data in a way that is easier to understand and use.\n",
            "\"\n",
            "\"Dataset:\n",
            "For this dataset you can use the Dataset command\n",
            "I used the Dataset command to collect data on the amount of energy used by the solar panels. Dataset is a great tool for data analysis. You can see my code in the Dataset command.\n",
            "When you see the data, make sure to click the green check mark\n",
            "====================\n",
            "Data science is _________________\n",
            "\"\n",
            "\"This is a guest blog post by Karen Wallace, a writer for DataCamp.org.\n",
            "The last time I used the term data science in my blog post was April 20, 2017. There are many different ways of thinking about data about data, but in this blog post, I talk about data science in the most specific way possible.\n",
            "Data Science is a subset of Data Mining. Data mining is a process that collects information from the internet and maps it to specific\n",
            "====================\n",
            "Data science is  the science of code. It can be used and abused to build products, so i would like to encourage you to learn more. It is very hard to learn code from a textbook, but there are many great resources that teach you how to code. A good example of a good book is \n",
            "Code School . It is a great resource that has lots of great resources for you to learn.\n",
            "As you know, all of our authors aren’t developers, so this will be a\n",
            "====================\n",
            "Data science is  the study of how information is produced and processed, which can be done with the help of Big Data, Machine Learning, Data Science and Data Analytics.\n",
            "There is no standard way to do this, but the process of creating data sets is similar to that of creating a piece of music. The closer you are to a piece of music, the more information you can extract from it. You can use data extraction tools such as “Data Parser”, “Data Analysis”\n",
            "====================\n",
            "Data science is  a good fit for machine learning.\n",
            "*“””The value of machine learning is that it makes machine learning more practical than ever. Machine Learning in a nutshell”\n",
            "Machine learning can be defined as the systematic decomposition of data into its individual components and then combining them to form new data. Machine Learning has the potential to make data science more useful. The following are some examples of how data science can be used to make data science more relevant to data science.\n",
            "\n",
            "====================\n",
            "Data science is  the next big thing, and if you are interested in the facts, please check out https://data.gov.co/data/2015/data/predictive-data-and-analytical-data-1/\n",
            "[1] Data Science and Data Mining is a subset of Machine Learning and Machine Learning is an integrated concept with data mining, machine learning and data mining. Data Science is the process of gathering data and then using data mining techniques to make predictions.\n",
            "[2\n",
            "====================\n",
            "Data science is  a strong discipline, and we shouldn’t be surprised if not all of the data is created by machines. However, data scientists, as a profession, have historically done a good job of creating and sharing data in a way that creates a richer ecosystem. We are lucky that because of the data we have access to the full set of data, and have access to the tools we need to analyze data. The only problem is that when we are looking for the best data science tool, we\n",
            "====================\n",
            "Data science is  an academic discipline, but is important to many companies today. Even though data science has been around for a while, and is highly popular, it has been too difficult to get into.\n",
            "Data Science: A Beginner's Guide is a great series to get you started.\n",
            "Data Science: The Beginner's Guide is a great introduction to Data Science.\n",
            "Data Science: Data Science 101 is an excellent guide for newbies to Data Science.\n",
            "Data Science: Data Science 101 is a\n",
            "====================\n",
            "Data science is  a great tool for uncovering interesting artifacts. We do not know what is really happening, but we can use this information to make progress in our next step.\n",
            "While we are able to find some interesting artifacts, we are not able to go deep in the data and are left with only a small amount of information.\n",
            "As we know that the data is important, we can use it to make progress in our data science work.\n",
            "Let’s go through how we can use the\n",
            "====================\n",
            "Data science is  a science of data and it’s a science that is extremely popular with engineering\n",
            "Data science is a science of data and it’s a science that is extremely popular with engineering\n",
            "Data is  a science of data and it’s a science that is extremely popular with engineering\n",
            "Data is a science of data and it’s a science that is extremely popular with engineering\n",
            "Data is a science of data and it’\n",
            "====================\n",
            "Data science is  a great way to gain insights into how your data structure works and it can help you build better solutions to your data challenges.\n",
            "Data Structures\n",
            "Data structures are one of the most popular data structures in Data Science. The way a data structure is defined, the structure is how it is structured. The data structure is defined by the data scientist. Data scientists in data sciences are able to define data structures that are used to define data and data structures that are used to define data types.\n",
            "\n",
            "====================\n",
            "Data science is  a lot of things. I am an engineer, at a startup, in the software industry. I am also a designer and web developer. I am looking for a job at a startup to help in the development of a service.\n",
            "You might already know the IBM Watson Watson Science API is a tool that can crunch the data from Watson Analytics. It is an analytics engine that can analyze and understand the data you collect to find patterns. Watson Analytics is a collection of scientific data, like weather,\n",
            "====================\n",
            "Data science is a process of combining data with the knowledge that exists from a large number of data sources and information. This contributes to a good understanding of the data.\n",
            "There is a term called “colour” which is a combination of two words, “data” and ‘colour”. This is a term that focuses on the colours of data and the data, a deep blue. The colour is the mark of a data source, a data set, a data point or\n",
            "====================\n",
            "Data science is  a new discipline which is still in the developmental stage. It is in its infancy.\n",
            "What about data science ?\n",
            "Now we have to ask ourselves, what is data science ?\n",
            "Data science is data collection\n",
            "Data science is data analysis\n",
            "Data science is data visualization\n",
            "Data science is data visualization\n",
            "Data science is data analysis\n",
            "Data science is data visualization\n",
            "Data science is data collection, visualization.\n",
            "Data science is Data Analysis\n",
            "Data science is Data Science is Data Analysis\n",
            "Data science\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab_type": "code",
        "outputId": "172e83c3-27d4-468b-f37e-f032f99a93ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "model_name = \"1558M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 302Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 104Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 792Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 6.23Git [02:06, 49.3Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 477Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 170Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 169Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab_type": "code",
        "outputId": "ad97d381-9c5d-4949-d15a-2c0bfa3a69b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/1558M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/1558M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"Artificial General Intelligence will arrive\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=10,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}