{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "import multiprocessing as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medium_scraper import main,get_last_day_in_year,daterange,get_links_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "tags=['data','python']\n",
    "idx = []\n",
    "start_date = date(year,12,29)\n",
    "end_date = get_last_day_in_year(start_date)\n",
    "for tag in tags:\n",
    "    for a in daterange(start_date,end_date):\n",
    "        idx.append([tag,a])\n",
    "articles = []\n",
    "for tag,single_date in idx:\n",
    "    articles.append(get_links_articles(tag,single_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'author': 'Paul Stollery',\n",
       "   'link': 'https://medium.com/@PaulStollery/all-of-the-fucks-given-online-in-2016-58c60edd6e44?source=tag_archive---------0-----------------------',\n",
       "   'title': 'All of the fucks given in 2016 - Paul Stollery - Medium',\n",
       "   'claps': 186,\n",
       "   'text': 'The field of fucks was far from barren in 2016. In fact, just shy of a billion fucks were given across social media the news this year.\\nIt was, for lack of a better phrase, a very shitty year. For a start, most of the awesome people died. Things continued to go from bad to worse in Syria. Zika spread across the Americas. And Britain did the stupidest thing a country has done in a long, long time only for America to go ahead and top it.\\nIn fact, 2016 was so bad that when you type ‘cel’ into Google, the first thing it suggests as an autofill is this:\\nThe collective response to most of these things was: fuck. At the time of writing*, 946,158,697 fucks had been given (shared/commented/published in a blog or news article) in 2016. But what warranted the most fucks?\\nIt won’t surprise you to know that the biggest day for fucks given was November 9, triggered by the election of Donald Trump. What might surprise you though, is just how many fucks were shared.\\nAn average of 2,613,698 fucks were given per day in 2016. Most days — even the really shitty ones — saw between 2 and 4 millions fucks. On November 9, there were 7,638,384 — nearly three times the average.\\nWhat makes this all the more remarkable is that the second highest day only saw 3,518,781 mentions of the word fuck. Just look at the size of the spike:\\nTerms used\\nI typically despise word clouds, as they’re so often used for no other reason than to look pretty in a crappy Powerpoint presentation.\\nHowever the word ‘fuck’ can be used in a number of ways. It can be used to express joy, ‘fuck yeah’, sadness, ‘oh fuck’, anger, ‘fuck off’ or it can be used simply as a verb — no I’m not providing an example of this. In this case, a word cloud gives you a pretty good sense of people’s motivation for using the word.\\nAs you can see, WTF was the driving sentiment behind the use of the word.\\nAssociated people\\nDespite the number of fucks given on November 9, Trump wasn’t the person most commonly associated with the word ‘fuck’. As the below word cloud shows, ‘local hotties’ beat Trump to the number one spot. No points for guessing what’s going on there.\\nAside from the infamous ‘Local Hotties’, people associated with the word ‘fuck’ are largely politicians, Jesus and a chap named Andrew.\\nSweariest sex\\nWomen were significantly more sweary on social, with 58% of posts mentioning fuck coming from a woman. This is possibly due to the whole vagina grabbing leader of the free world thing.\\n‘Index’ in the above graph is a metric which is corrected for the overall number of men and women in the data pool (i.e. even when corrected for an uneven number of men and women, women remain the sweariest sex).\\nNotes on the data:\\nI tracked all mentions of ‘fuck’ on Twitter, Facebook, Instagram, Youtube, blogs, comments, forums and in the news using a tool called Netbase, which is a very powerful tool when it comes to monitoring social media.\\nIt’s worth noting here that different channels offer differing amounts of accuracy and reliability: Twitter and Tumblr are both really reliable as they’re robust as well as open, whereas data from channels like Facebook is incomplete due to the fact that most posts are private. Instagram’s data was sketchy and I’m always skeptical of news tracking on Netbase as it’s primarily designed for social media tracking.\\nI also noticed some issues with other channels — such as zero values on certain days on Instagram, which looking at other days seems very unlikely.\\nStill, when viewed as a whole, it paints an accurate picture of the year.\\nTo download the source data, click here.\\nWritten by\\n'},\n",
       "  {'author': 'Peter Wells',\n",
       "   'link': 'https://medium.com/@peterkwells/make-data-great-again-ab27ff9141df?source=tag_archive---------1-----------------------',\n",
       "   'title': 'Make data great again - Peter Wells - Medium',\n",
       "   'claps': 17,\n",
       "   'text': 'Data is becoming increasingly important to our societies. We live in an age of data abundance and, without many of us realising, data has become a new type of infrastructure and a critical one at that. The age of data abundance has led to brilliant new services and can help our societies tackle challenges such as climate change and population growth, but it also creates risks to privacy and concentrations of power.\\nSocieties need to be able to debate what this age of data abundance means for them. People need to make decisions about the relationship between individuals, communities, societies and data. We need to pick a future vision for our relationship with data and then make steps towards it. Many governments and societies are having this debate now.\\nIn my job I put forward the Open Data Institute’s position on those decisions while also trying to encourage a more public debate. I want a debate because I, and the lovely people I work with, want the decision to be made by societies around the world.\\nTo make this debate as broad and informed as possible, I need what I say to be understandable by as many people as possible. I try to use plain language and frequently test new language and concepts to see if they are understandable. Sometimes I test things through tweets or blogs, like this one, at other times by talking with people from differing backgrounds and perspectives.\\nBy testing, listening and learning I have made some of the language more accessible but I’ve also realised that something was more important than I first thought: politics. Both my politics and that of others.\\nLet me try and explain.\\nSometimes people say they want to help people make better choices about data. I did that a few times in this blog about an open future for data.\\nI was talking about the ideas in that blog with a left-wing British politican who stopped me mid-sentence and asked if I was a Blairite nowadays. No, I replied. “Then why are you using the language of Blair’s choice agenda?”, they asked.\\nFurther testing of the language caused another person to recoil and suggest that if I kept talking about choices I might be accused of being a secret Thatcherite pushing the theory of public choice. Hmm....\\nI’d used the word ‘choice’ because I thought it was plain language but it was clear that the decision risked putting in place a political barrier for some of the other ideas in the blog. This is a problem.\\nWhen thinking about and debating technology and data with other technologists it can be easy to fall into a trap of thinking that every decision can be based on empirical evidence, that there is a single right answer and that we can make that right answer a reality by designing and building the right technology. This is nonsense.\\nIn our debates about data we need to decide issues of access, ownership, regulation and the relationship between citizens and the state. These are political decisions.\\nWhilst we might have individual opinions about data we need a state and legal system to help put decisions into practice. States will allow technologists to innovate and try things out but there comes a time when existing legislation will be more strongly applied or new legislation will be put in place as society’s needs change. This happened and continues to happen with road traffic, it will happen with data.\\nBy broadening the debate we are helping that decision to be made democratically. Democracy might have seemed under strain in some countries in 2016 but as Churchill said:\\nIndeed it has been said that democracy is the worst form of Government except for all those other forms that have been tried from time to time\\nTo put it more simply politics and democracy is important and data, as with most things, is political.\\nWords are a tool political people use to reach our hearts. Sometimes those words are a catchy slogan. At other times it’s a frame: a guiding metaphor or image for a political argument.\\nPolitical slogans and language are designed to appeal to a group of people, build on existing beliefs and make them choose a particular path.\\nSome words carry a particular meaning in the present because they have been used in a political context in the past. Marx said it more poetically:\\nThe tradition of all dead generations weighs like a nightmare on the brains of the living.\\nThe word “choice” resonated amongst some people involved in British politics that I spoke to because of those traditions and their political history. It will have bought back nightmares for some and heavenly dreams for others.\\nOur societies and political systems are used to making political decisions about many types of resources, for example oil or water, but data has different qualities to the physical resources that are embedded in our political systems, debates and legislation.\\nTo give two regularly used examples: data is non-rivalrous, unlike a piece of cake many people can use data at the same time, and data benefits from network effects, it becomes more valuable as more people use and maintain it.\\nThese differences are one of the reasons the team at the Open Data Institute talk about data as analogous to roads:\\nData is infrastructure. Just like roads. Roads help us navigate to a location. Data helps us make a decision.\\nThe “data is roads” analogy breaks people out of the traditional mindset. It helps open their minds to thinking differently.\\nI think that, as with the web, these different qualities mean that a closed-open axis is a more useful way of thinking than the traditional left and right-wing political axis.\\nBut it will be harder to get people to think about the decisions along that closed-open axis if our words and ideas cause them to think of old left and right wing political battles.\\nData has many other different qualities to other resources. One that is becoming increasingly evident and important is that data is sometimes about identifiable people, sometimes it isn’t and sometimes it’s a bit complicated.\\nMuch of the current debate about data is dominated by personal data: the stuff which is about identifiable people. Many people believe that there is an asymmetry of power and privacy as data about us is controlled by governments and corporations.\\nTav Kotka, the Chief Information Officer of Estonia, recently gave a talk in which he broached the idea of adding a fifth freedom to the EU’s existing four freedoms for free movement of goods, workers, services and capital. The talk was mostly about personal data and the concept of personal data stores that could allow individuals to control how data about them is used.\\nWhilst I agree that more personal control over personal data is important the talk bought up memories of Margaret Thatcher and my teenage political nightmares. The talk did not mention society’s need to access and use that data. Taking back control of data by giving control to individuals misses out the challenges of digital inclusion and the role of other important parts of society like families, communities and nations. Different levels of control, rights and responsibilities are likely to need to given to these different groups. To give just one example vital medical research and national statistics need to use large amounts of personal data, this can’t be neglected or left solely to the decisions of individuals.\\nBut, as I realised, this time I was the one allowing my political history to do the interpretation for me and I was the one who wasn’t listening to the underlying argument. Tav Kotka was using language that built on his political history while talking in English to a Finnish audience. Even though I work for a global organisation my initial reaction was from a UK perspective. My bad.\\nThe EU is currently discussing complex concepts such as data control and data ownership through the free flow of data initiative. Major geopolitical organisations, like the EU, can have a large impact on countries outside their membership, the UK government has committed to following current EU data protection regulation after it exits the EU. That EU debate involves politicians from multiple countries, each with their own rich histories and perspectives. There are many other debates in countries around the world.\\nIf you want to help build a great future for data then as well as building new services you may want to get involved in either this or other multinational, national and local debates.\\nBut if you do, remember to think about politics: both other people’s politics and your own. That way you will be best placed to help people think about the decisions not in terms of traditional left and right-wing politics but instead in terms more suited to the different challenges and possibilities of data.\\nWritten by\\n'}],\n",
       " [],\n",
       " [{'author': 'Boy with Silver Wings',\n",
       "   'link': 'https://medium.com/talk-like/python-gui-with-kivy-an-introduction-83ab35e30fc6?source=tag_archive---------0-----------------------',\n",
       "   'title': 'Python GUI with Kivy: An Introduction - Talk Like - Medium',\n",
       "   'claps': 250,\n",
       "   'text': 'Starting a GUI Project with Python is pretty daunting task by itself. Libraries like Tkinter often mess with Programmer’s minds often creating thousands of lines of code even for simple interfaces. There is also the fact that most of these libraries come with limited layouts and the whole structure have to be rebuild even for a small non breaking change.\\nSo how do we go about creating applications that scale across multiple OS environments with Python. This is where libraries like Kivy and QT come in.\\nQT while being a great library, has some licensing issues for commercial products (Read more Here). It has some great add ons like QT Creator which eases GUI development to a drag and drop environment. If you don’t want licensing or just building a hobby project I would recommend that you try this library.\\nBut here let’s talk Kivy. Kivy is another great tool for creating GUIs for Windows, Linux, Android or iOS. It has a smooth interface and also some inbuilt components so we don’t have to build them all from scratch. (Looking at you, TKinter). Before any more pep talk, let’s look at a simple program that utilizes Kivy.\\nYou can see two programs here. Let us look at the first program. We begin with a class MyApp. If you are unfamiliar with writing Objet Oriented Python, you can read more Here. The first lines present the import statements for Kivy.\\nThe first is asking Kivy to import the Application Ecosystem to the program, absolutely necessary to instantiate the Kivy class.Then, from the uix package we are importing the Button class. You will see this a lot in Kivy programming as all widgets and layouts are defined within the uix package.\\nThis function essentially points to the UI of the actual application. As you can see, we have defined our Button inside this function. It can be thought of a render function that is executed by Kivy multiple times to draw the layout on screen.\\nThe Button is defined with a single option called text. There are other options like font size or color which you will find Here.\\nThis is the sample output you will find when you run the application.\\nBut this is very restrictive. Isn’t it? So Let’s split it. This is where the second gist comes in. Here, I will chose to split the code into a function. But this is just keeping things simple. To actually work in splitting the interface, you will have to split it into another class.\\nIn our sample code, will create a layout for GUI objects. We use BoxLayout which means things can be arranged in a box. The box can be either horizontal or vertical orientation, Box Layout comes with a default orientation of horizontal so we need not provide that option. There can be several layouts in Kivy Grid, Stack, Scatter to name a few. Read more Here. You can use\\nto attach the button to the layout.You can now get the same output as the other smaller code.\\nBut do remember, that one is smaller but this one is extensible.\\nHappy Coding!\\nWritten by\\n'},\n",
       "  {'author': 'Robert Roskam',\n",
       "   'link': 'https://medium.com/@raiderrobert/top-10-django-projects-started-2016-end-of-year-review-8e4efc88ac43?source=tag_archive---------2-----------------------',\n",
       "   'title': 'Top 10 Django Projects Started in 2016 — End of Year Review',\n",
       "   'claps': 122,\n",
       "   'text': 'Earlier this year, I wrote an article that was sort of a mid-year review of popular Django projects from Github. This article follows up on that one. My criteria is slightly different, so skip to the end of the article if you want to know how I selected these projects\\nEasily add a dashboard with widgets to django-admin. Basically, use this thing for adding simple reporting needs where you want some visualizations added easily, but don’t want to spend a lot of time designing a custom display.\\nIt uses chartist.js for building out the graphs, masonry.js for making grids, and sortable.js for dynamic ordering of tables. Compatible with Python 2 and 3; Django 1.8–1.10.\\nAn extensible, full-featured, moderately-opinionated workflow engine. Could be useful for rolling your own ERP. Also, features a pretty nice looking GUI.\\nCompatible with Python 2 and 3; Django 1.9. Important note: code base hasn’t had any commits since Aug 4th, 2016, nor have there been any issues closed or PRs made since April 2016, so I hope this project is still being maintained.\\nLets you keep a history of all changes to a particular model’s field. Store the field’s name, value, date and time of change, and the user that changed it.\\nWhile all this sounds close to django-reversion, please note that it isn’t a competing solution. Reversion is about tracking all the fields on a model.\\nField-history lets you track specific field or set of fields instead, and it’s interested in solving only that problem. In fact, the maintainers closed an issue that would make this package compete with reversion.\\nCompatible with Python 2.7/3.2+; Django 1.7+.\\nI was on the line about including this project since its use-case seems so narrow at this point. But here it is.\\ngraphene-django lets you implement a GraphQL API. What is GraphQL? To be overly simplistic, it’s a declarative JSON syntax that’s meant to feel like SQL: ask for data, not how to get the data, and get results back. Effectively, it’s an alternative to REST APIs that’s being pushed by Facebook.\\nSo why is this project called graphene-django? Graphene is the python reference implementation of GraphQL.\\nI think this library is seeing some inflated support from people wanting to push React in the Django community, because Facebook also released relay, which is the React package to support requesting data from GraphQL endpoints.\\nSo if your stack includes React, and you’re getting some strain from using a REST API, this project might be useful for you.\\nAppears to be compatible with Python 2.7/3.4–3.5; Django 1.6–1.10. (They don’t actually state that, but that’s what is passing on their builds.)\\nThis package is a backend for caching that uses disk. Pretty obvious. But it argues that it can be as performant as in-memory caching for the fetching operation (not setting or deleting, though.) Quite the claim, but here’s the benchmark below.\\nCompatible with Python 2.7/3.4–3.5. Only tested on Django 1.9 currently.\\nIf you want a REST API, but think that DRF has too much configuration, then this project was made for you. It literally adds a REST API onto django-admin endpoints with maybe 3 lines of code. So it’s extremely light.\\nThat said, the project has no setup.py file, so you’ll literally have to clone the repo and copy the files into you project.\\nAdditionally, there are no tests on this project yet, so compatibility is unknown. I’d expect probably Django 1.9 compatibility, but that’s a guess.\\nThis project provides two very simple features: (1) allow a user to purge sessions from other devices, and(2) allow an admin to force a password reset on a specific user.\\nThat’s all. Not very fancy, but extremely useful if you’re working on a project where the built-in auth features for Django are not sufficient for you.\\nCompatible with Python 2.7/3.4–3.5 and Django 1.8-1.9.\\nAnother narrow feature set here: effectively, this package provides protection against accidentally making materially changes in your SQL queries.\\nYou accomplish this by (1) writing a test using the provided context manager and (2) running that test locally, and(3) committing the resulting file to source control. Any subsequent running of that original test will compare the previously outputted file.\\nFor example, using this project would prevent you from accidentally adding a where clause or changing a sort or adding an entirely new query, because it is not comparing queryset results, just the SQL queries itself.\\nCompatible with Python 2.7/3.5 and Django 1.8–1.10.\\nAn unusual project, mypy-django is an implementation of the PEP-484 spec for the built-in Django components. Basically, if you care about static type checking, and you don’t mind using something experimental to help you, then you’ll care about this.\\nIf you’ve never heard of this, and you’re wondering why we’re talking about static types in Python, then this is worth you reading about at least.\\nCompatible with Python 3.5+ and Django 1.10.\\nWant to gather statistics about your own models, but don’t want to rely on a third-party service? That’s what this does.\\nThrough generic keys, If you want to do charts, reports, etc. for time series data, then this packages helps you to (1) create queries to extract that info and (2) handle the storage aspect for you.\\nVery important note: it will not provide you will a recurring task schedule, so don’t configure all this and wonder why your data isn’t getting recorded. You’ll need to run the queries inside of celery or something like it.\\nCompatibility seems a bit unclear. Python 2.7 and Django 1.9-1.10 are being tested, but it seems like they intend to test Python 3.4 as well, except their tests are misconfigured.\\nLast time, I just derived my list from this query on Github for projects started this year. Basically, I’m doing by the most popular stars first, and after that, I was primarily interested in code bases that were things that you could add to a Django project to provide either drop-in functionality or enable you to build features more easily.\\nConsequently, I eliminated anything that (1) wasn’t primarily python code, (2) was really just a starter template, and (3) was not English, because I can’t read Chinese or German or Spanish (you get the idea).\\nKeeping those existing criteria in mind, I added another level of filtering for this list. I included (4) no service integrations or (5) projects that presuppose usage of another large package(s).\\nWhat do I mean by service integrations? Basically, if the project calls an API, it’s service integration to me. So that’s things like django-zappa (which is now just zappa by the way) that lets you run django on aws lambda or django-stackoverflow that auto searches your debug error on Stackoverflow.\\nWhat do I mean presupposing using another large package? Basically, if in order to use this package, you need to go configure something else first like Django Channels (aka channels) or Django Rest Framework (aka DRF) or Django CMS. All those projects have their own little ecosystems . So for example, drf-url-filters is a great library that allows you to cleanly implement validations of the url queries, but it assumes you’re using Django Rest Framework, which is definitely not true for everyone.\\nOne thing I’d like to be super clear on: by excluding those things above from my list, I’m not saying those things lack value, but they ultimately this list could be easily dominated by those things, and I’d really like this list to be useful for Django specifically, not Django plus something else. That said, I may make separate lists in the future for the things I’ve excluded for right now from this one.\\nWritten by\\n'},\n",
       "  {'author': 'Clay Graham',\n",
       "   'link': 'https://medium.com/dronzebot/ansible-and-docker-py-path-issues-and-resolving-them-e3834d5bb79a?source=tag_archive---------5-----------------------',\n",
       "   'title': 'Ansible and Docker Py Path Issues and Resolving Them',\n",
       "   'claps': 18,\n",
       "   'text': 'I ran into a snag while trying to build out our CICD pipeline here at dronze.com, and I wanted to write a quick article that might help others. TL;DR is that ansible expects its docker-py installations to be in dist-packages for 2.7.\\nIf you google it you find long threads about ansible and docker versions, rolling back to 1.9 blah blah blah.\\nHere is most likely what you are dealing with. Ansible doesn’t know where to find docker-py, that is because in some ubuntu configurations python installs docker-py in site-packages (where ansible does not expect it) and in others in dist-packages (where it does). To resolve this make sure you tell ansible to look in both places on the target machine. This can be done at either the task or play level.\\nWritten by\\n'},\n",
       "  {'author': 'Arthur Carabott',\n",
       "   'link': 'https://medium.com/@acarabott/caffe-opencl-macos-sierra-amd-2e979518503?source=tag_archive---------7-----------------------',\n",
       "   'title': 'Caffe openCL macOS Sierra AMD - Arthur Carabott - Medium',\n",
       "   'claps': 21,\n",
       "   'text': 'No Nvidia/CUDA in the new macs, trying my luck with the openCL branch of Caffe\\nInstall http://brew.sh/ if you don’t have it\\nPython 2.7 and numpy if you ain’t got it\\nAs per http://caffe.berkeleyvision.org/install_osx.html\\nViennaCL\\nCUDA even though you can’t run it, you’ll need it installed to stop compilation complains\\nI’m cloning into a folder at ~/src feel free to use another location\\nCompilation time (from: http://caffe.berkeleyvision.org/installation.html#compilation)\\nOpen Makefile.config if your text editor, update the python include path on line 104\\nEdit: Was getting a nasty malloc error when running caffe, fix from this discussion is to uncomment USE_LEVELDB := 0 on line 52\\nAlso note that on line 13 USE_GREENTEA := 1 should be uncommented, confirms we are on the openCL branch\\nDouble check that the files it needs are actually there\\nIf you get blank lines, your Python 2.7 and/or numpy installation isn’t right, double check these.\\n(the -j flag will use multiple cores to compile, you can determine this by running sysctl -n hw.ncpu mine returns 8\\nAll tests passed!\\nTrying the LeNet MINST example from http://caffe.berkeleyvision.org/gathered/examples/mnist.html\\nNot convinced my GPU is being used...\\nWell at least it finished, took ~12 mins.\\nSo the real goal was to get this jaw dropper running....\\nThey have wrapped things up nicely in a custom Caffe repo, unfortunately this is for regular CUDA powered Caffe. Trying to get this working without CUDA...\\nFirst walls hit... haven’t managed to compile successfully yet. As they aren’t using the OpenCL branch I’ve had to port a bunch of their changes, you can find them here on the rtpose-opencl and rtpose-cpu branches: https://github.com/acarabott/caffe\\nWritten by\\n'},\n",
       "  {'author': 'Haydar Ali Ismail',\n",
       "   'link': 'https://medium.com/@haydar_ai/learning-data-science-day-3-pandas-sql-and-grammar-of-data-59774e794be7?source=tag_archive---------11-----------------------',\n",
       "   'title': 'Learning Data Science: Day 3 - Pandas, SQL, and Grammar of Data',\n",
       "   'claps': 11,\n",
       "   'text': 'On Day 1, we have learned a brief introduction about pandas. Today we are going to focus more on the grammar of data. Well, grammar not only exists in languages after all. Maybe after this there will be people who named as datagrammarnazi :))\\nIn this story, we will learn about database especially relational database and how Pandas and SQL working with the database. So let’s get going.\\nSome of us would think why bothering learning about the Grammar of Data? I already know SQL and so forth. Data Science is a pretty board topic and as far as I know it actually lies between the Data Engineering, Data Analysis, and Business Intelligence (somebody out there who might know this better can add or correct me if I wrong). So in order to learn Data Science, we have to learn at least the basics of each expertise including hacking skills, math & statistics knowledge, and substantive expertise. And because of that, we will need to learn about the database, grammar of data, and also analysis not just theoretically but also practically.\\nFor those of you who haven’t know, dplyr is an R package which provides a set of tools for efficiently manipulating data sets. Basically, dplyr is the next iteration of plyr , focusing on only data frames. dplyr is noticeably faster than its predecessor.\\nThis puts dplyr in the same domain of what Pandas try to achieve. I think it is all just preference or maybe at some point we might have to use both of them, I’m not quite sure right now. But, today we are going to focus on manipulating data sets on a database with pandas and SQL.\\nIn short, Rahul Dave has made a comparison between verbs between dplyr, pandas, and SQL that available below.\\nRahul Dave actually made a nice lecture about this topic and that’s where I study from today. The notebook itself provided in the link below.\\nThe notebook will guide you through a lot of data sets database manipulation and heavily focused on pandas and SQL. It also includes basic usage of SQLite. The topics include populating database, query, sort, select, assign, aggregate, delete, limit, indexes, and relationships just what can we expect from database class. Since it’s not a database story series I’m not going to talk a lot about that. The lecture also provided with a nice video course available in the link below.\\nThe video course itself is not mandatory to watch, you will be fine if you just go through the notebook. But it will give you a clearer explanation of what’s going on the notebook. I personally much prefer pandas, but even if you prefer a solution, it is better if you still understand the basic usage of the other solutions.\\nFor more resources about the comparisons between pandas and dplyr:\\nToday we learn about relational database and how to interact and manipulate it using either pandas or directly with SQL query. We also learn a bit about dplyr. But at this point, I think I’m going to go with pandas. What about you? Do you have thought about that or maybe prefer using dplyr with several noticeable advantages? Share with us on responses and we can discuss this. Don’t hesitate to tell me if I got something wrong, it will be a good information for all of us. For the next story I think I’m going to learn about Statistical Models but let’s see what I’m going to write tomorrow.\\nHere is a funny comic about SQL injection, thanks to Rahul Dave who includes this in the notebook. I guess I’m going to bookmark xkcd from now on.\\nFor people who haven’t read the other chapters of the series.\\nDay 0: Motivation\\nDay 1: Environment and Python\\nDay 2: Data Scraping and Effective Visualizations\\nDay 3: Pandas, SQL, and Grammar of Data - You are here\\nWritten by\\n'}],\n",
       " [{'author': 'Dibya Chakravorty',\n",
       "   'link': 'https://medium.com/broken-window/how-well-do-you-know-your-python-815ec34b5977?source=tag_archive---------1-----------------------',\n",
       "   'title': 'Python year in review 2016 - Broken Window - Medium',\n",
       "   'claps': 96,\n",
       "   'text': 'From moving to GitHub to the release of Python 3.6.0, 2016 has been an interesting year for the Python programming language. We had some uplifting moments like K Lars John’s keynote at Pycon and some downers like Zed Shaw’s article “The case against Python 3”. It was a good year for static typing, asynchronous programming and we finally got the long awaited f-strings. Here’s how I saw it all happening in the last 12 months.\\nPython’s year started with a bang with the language getting more traction than ever. Particularly, in December 2015, the keyword “Learn Python” overtook “Learn Java” for the very first time. Python maintained this lead throughout this year and it is ending 2016 as the clear winner. This is certainly great news for all of us.\\nNew year’s day was dramatic with Brett Cannon announcing in the Python core workflow mailing list that Python’s repository will be moving to GitHub. This caused a bit of a controversy as the open source GitLab was also under consideration. Brett Cannon defended his decision in more detail in one of his blog posts citing familiarity with the GitHub platform and Guido’s input as the major driving forces behind the decision . Guido later announced in Pycon that the move will happen by the end of this year, even though this doesn’t seem to be the case so far.\\nOne of the biggest defining events this year was the American elections. We have all heard the president elect’s campaign slogan “Make America great again”. In a funny twist to the campaign, a computer science major at Rice University created a Python derived language called TrumpScript with the slogan “Make Python great again”. The features of this language deviate from Python in ways that Mr. Trump would personally approve. For example:\\nFor more such gag reels, check out their awesome repository on GitHub, which has more than 5000 stars now.\\nThe discovery of the gravitational wave was one of the landmark science discoveries in 2016. The experiment detected waves coming from a black hole merger 1 billion year ago, confirming Einstein’s prediction.\\nPython has always been a favorite among the scientific community. The team of scientists involved in the discovery used Python extensively. Most notably, the discovery plots that appear in the groundbreaking paper are made in matplotlib, a Python library for plotting. For a complete discussion of Python’s role in this discovery, see this blog post.\\nAs all of you know, Python 3 came out in 2008 and was intentionally backwards incompatible with Python 2. While this move made the language better, more modern and future ready, it has also created major headaches in the past 8 years. Many major libraries had to be ported and the porting process is still going on.\\nAs if to rub salt into the wound, in a mail to the Python development mailing list, Victor Stinner announced that the next major version of Python will be Python 8 and it will break backwards compatibility again! But it was soon discovered that it was April first and the mail was nothing more than a sadistic April Fool’s prank. We all had a good laugh.\\nSince we are on the issue of Python 2 and 3, it is worthwhile mentioning that many operating systems had not adopted Python 3 at that time. This is one of the major reasons which was blocking Python 3 adoption in the community. In April, the new LTS of Ubuntu, codenamed Xenial Xerus, came out and it included Python 3 as the default Python distribution, answering many of our prayers.\\nThe debate between Python 2 and 3 is never ending, but the PSF had declared that Python 2.7 will reach end of life in 2020. As if to bolster this decision, a website called pythonclock.org went online in May. The website is counting down to the end of life of Python 2.7. The website recommends Pycon 2020 as the venue for this major event and talks about an epic after party celebrating Python 2’s contributions to the Python community.\\nMeanwhile in Pycon 2016, K Lars John gave an amazing keynote which received a standing ovation from the attendees. In this keynote, he talks about fractal dimensions and how it can be related to software development. It would do the keynote no justice to describe it in words. You should absolutely totally irrespective-of-what-you-are-doing-ly see it (be prepared to listen to some music too).\\nSince we cannot have a Python year in review without Guido prominently featuring in it, here is his Pycon 2016 speech, where he talks about his childhood and how he came up with the Python programming language. Must watch!\\nFollowed by one of his famous tweets this year about a crossword puzzle that expected the solver to know the origin of the name Python.\\nPython 3.5.2 was released in June. From what I understand, this is just a bugfix release with no major additional features.\\nPyPy is a fast and alternative implementation of the Python programming language. Lots of programmers who need speed in their code use PyPy instead of CPython as the compiler. One of the biggest pain points about using PyPy was that it had no Python 3 support so far. But in August, Mozilla decided to award $200,000 to Baroque Software to work on PyPy as part of its Mozilla Open Source Support (MOSS) initiative. Within the next year, this money will be used to to pay four core PyPy developers half-time to work on the missing features and on some of the big performance and cpyext issues. This should speed up the progress of catching up with Python 3.x significantly.\\nStatic type checking, like the one shown below, was available in Python since a long time. However, static types in Python was still not widely adopted, because the tool for checking the type annotations, mypy, was not ready for production use... until recently. In October, the Zulip team announced that they are now the largest open source project that has 100 % coverage of static type checking in their code. That is certainly no mean feat and I believe that many larger projects are going to follow in their footsteps.\\nI always think of November as the month of controversy. Zed Shaw, who wrote a very popular book called Learn Python the Hard Way, wrote a blog post titled “The case against Python 3”. In this post, he claimed that Python 3 is a dead language and no beginner should learn it. This caused a stir in the community. Many people felt compelled to write rebuttals. Most of his claims have been invalidated since then. But here is a list of rebuttals, in case you want to judge for yourself.\\nWith perfect timing, Semaphore CI also published a study of Python versions used in commercial projects in November. This study showed that nearly 70 % code in production runs on Python 2.7. I find this a bit disappointing for Python 3 even though the reasons for this trend are actually well shown.\\nIn any case, this statistics alone is not worrisome since the packages in PyPI are showing the exact opposite trend, something that I have talked about recently.\\nDecember was the month we have all been waiting for. Just days before Christmas, Python 3.6.0 was released with major new features.\\nWe now have f-strings, underscores in numeric literals, syntax for variable annotations, asynchronous generators and asynchronous comprehensions. You can see the full list of changes here.\\nAmong them, my favorite is f-strings, which allows me to sign off like this.\\nIf you enjoyed this article, please recommend it using the ❤ button so that other people can also discover it.\\nWritten by\\n'},\n",
       "  {'author': 'Peter TEMPFLI',\n",
       "   'link': 'https://medium.com/@tempflip/lane-detection-with-numpy-56b923245fc9?source=tag_archive---------2-----------------------',\n",
       "   'title': 'Lane detection with NumPy - Peter TEMPFLI - Medium',\n",
       "   'claps': 30,\n",
       "   'text': 'I can’t wait to start Udacity’s Self Driving Engineer course — but fortunately there is already so much stuff to do in advance. Sebastian Thrun created the brilliant AI for Robotics course, or there is also an exciting Introduction to CV course to attend.\\nThe folks from previous cohorts are also active — Mehdi Squalli shows us how to solve the “Detect Lane Lines project”.\\nI’m going to solve here a similar task — detect lanes on video frames, using NumPy and SciPy. My goal is not to achieve better performance or speed then with OpenCV. Rather, I’m going to implement some techniques learned at the Computer Vision course. This is the plan for the articles:\\nIn order to understand any picture, we need to convert them to some convenient form. When looking for lanes, we are trying to get rid of every detail, which is not relevant.\\nFirst of all, we need to do some blurring. It is a good way to get rid of small annoying details, but keep the larger ones (like lanes).\\nAs images are stored in computers as matrices, we can filter them using matrices, too. A blur filter works like this:\\nBasically, you are doing nothing else than a moving average in 2 dimensions. How does it make sense? The pixels with high values (the peaks) will get lower, and the extreme lows will get a bit higher — so you’ll end up with a lovely blurred image.\\nHow intense is your blur? It depends on the size of your filter (the ‘window’). The larger is the window, the more intense is the blur.\\nAnd here comes the cool thing with NumPy: you don’t need to implement yourself the multiplication loops. Just use the handy signal.correlate function.\\nUsing a matrix with only ones for blurring works, but it has some problems — the blur is not so nice, because it counts every pixel in the window to the final blurred (averaged) pixel value with the same weight. Your blur will be nicer and more smooth, if you count to the final value the closer pixels with a higher weight. So, instead of only ones, the values in your filter matrix will be higher in the center, and lower towards the edges. Something like this:\\nThis makes much more sense once plotted:\\nProbably you’ve already guessed — it’s called Gaussian Blur because the filter is a 2d Gaussian distribution. The intensity of your blur depends on 2 things: on the size of the filter, and on the sigma value of the Gaussian. There are a lot of ways to create this filter, that’s how I do it:\\nWhat is an edge? It can be a limit of an object; changing of some property (like color, texture, light and shadow); or some information about the position in the 3d space (like the end of the visible part of a ball).\\nOn an image, these edges usually marked as a sudden change in pixel value (color property edge is the most trivial example of this). Fortunately, there is a mathematic operation to find them: derive. A derivative of a function is another function which shows the slope of the original function on every position; so a derivative of an 2-d image will show us, how intense is the changing of the pixel value on a given position, compared to it’s neighbouring pixels.\\nAgain, we don’t need to implement this operation manually (however it’s not a big deal), but we can use filters. This is a horizontal edge filter:\\nThis filter takes the left-hand side pixel values, and subtract them from the right-hand side value. This way, we end up with the difference (or the pace of the changing, i.e the slope) for the given position.\\nThere is one caveat. We can make the correlation 2 ways:\\nThe first filter will show us the vertical edges and the second the horizontal ones. So in order to get all the edges on one image, we need to combine them:\\nIn the next post I’ll try to figure out, how to tell about an edge that it’s a lane. Stay tuned ;)\\nWritten by\\n'},\n",
       "  {'author': 'ipapi ~ ipapi.co',\n",
       "   'link': 'https://medium.com/@ipapi/weather-forecast-from-ip-address-9a1b8bd14970?source=tag_archive---------3-----------------------',\n",
       "   'title': 'Weather forecast from IP address - ipapi ~ ipapi.co - Medium',\n",
       "   'claps': 30,\n",
       "   'text': 'Recipe to get weather forecast from an IP address\\nWritten by\\n'},\n",
       "  {'author': 'Dr. GP Pulipaka',\n",
       "   'link': 'https://medium.com/@gp_pulipaka/curse-of-dimensionality-knn-breaks-down-part-two-with-code-examples-82d48adc6b61?source=tag_archive---------4-----------------------',\n",
       "   'title': 'Curse of Dimensionality- KNN Breaks Down: Part Two (With Code + Examples).',\n",
       "   'claps': 33,\n",
       "   'text': 'This is continuation of my earlier article Coding K-Nearest Neighbors Machine Learning Algorithm in Python as part of Python Series. K-Nearest Neighbors is an instance-based nonparametric algorithm that is subject to the curse of the dimensionality. In Computer Science, the situation of the curse of the dimensionality arises during dynamic programming, combinatorial problems, control theory, integer programming, and time-dependent problems. This happens when the number of states exponentially increases with respect to a tiny increase in the number of dimensions or parameters due to combinatory explosion. In 1957, Bellman coined the term curse of dimensionality. In this scenario many multivariate conundrums suffer from the curse of the dimensionality. It implies that the complexity of a z-variable conundrum is an exponential function in z. The curse of dimensionality can influence K-Nearest Neighbors based on the weakness of the model. If we were to create a 3-Nearest Neighbors model, the votes will be allocated as 2/3 for the wrong class. The curse of the dimensionality is defined on the unit balls of normed linear spaces for all the multivariate problems. The curse of the dimensionality is still applicable independently even for analytic functions. Katscher, Novak and Petras have studied the integration problem for monotone functions. Papageorgiou applied it for the integration problem with curse of the dimensionality on the convex functions. For the class Zemont of monotone function, it was proved as\\nFintd (Zemont) = Θ(m-1/e).\\nIn this, the convergence order is m-1/e. Therefore, the weaker tractability does not hold. The curse of the dimensionality has been proved on the convex functions as well. The curse of the dimensionality arises when the values can be arbitrarily large and the parameters are limited. K-Nearest Neighbors operates on the distance between the data points. The distance of the data points is inversely proportional to the exponential increase in the number of data points that leads to the curse of the dimensionality, which plays a key role in designing a classifier in the machine learning algorithm.\\nTo prove this, I’ve written code in Python and uploaded my code and dataset into Github repository. Check it out at https://github.com/GPSingularity/Machine-Learning-in-Python.\\nReferences\\nHinrichs, A., Novak, E., & Wo_niakowski, H. (2010). The Curse of Dimensionality for Monotone and Convex Functions of Many Variables. Retrieved December 30, 2016, from https://arxiv.org/abs/1011.3680\\nWritten by\\n'},\n",
       "  {'author': 'ipapi ~ ipapi.co',\n",
       "   'link': 'https://medium.com/@ipapi/what-is-my-ip-address-1656df3716c6?source=tag_archive---------7-----------------------',\n",
       "   'title': 'What is my IP address ? - ipapi ~ ipapi.co - Medium',\n",
       "   'claps': 52,\n",
       "   'text': 'Find your public (external) IP address in your favorite language with a secure API\\nWritten by\\n'},\n",
       "  {'author': 'Haydar Ali Ismail',\n",
       "   'link': 'https://medium.com/@haydar_ai/data-science-cheat-sheets-python-edition-f16b151f17b8?source=tag_archive---------8-----------------------',\n",
       "   'title': 'Data Science Cheat Sheets: Python edition - Haydar Ali Ismail - Medium',\n",
       "   'claps': 14,\n",
       "   'text': 'So you may expect that today I’m going to publish another day to learning Data Science. But, I’m so sad to disappoint you guys because I don’t think I’m able to publish it today. So, in exchange for that I’m going to give a curation of cheat sheets that I used to learn data science, especially in applying data science using Python. So let’s get started.\\nI’m not going to give you a bunch of cheat sheets available out there, I do choose several of them that are useful and easy to read. Hopefully tomorrow I can resume my journey for learning data science. Happy learning!\\nWritten by\\n'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [item for sublist in articles for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eden Au</td>\n",
       "      <td>342</td>\n",
       "      <td>https://towardsdatascience.com/4-numpy-tricks-...</td>\n",
       "      <td>NumPy is one of the most popular libraries in ...</td>\n",
       "      <td>4 NumPy Tricks Every Python Beginner should Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dominik Vacikar</td>\n",
       "      <td>28</td>\n",
       "      <td>https://medium.com/@chichikid/the-guys-who-wil...</td>\n",
       "      <td>Two weeks ago, I was sitting in a coffeeshop w...</td>\n",
       "      <td>The Guys Who Will Solve the Private Market - D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Farida Adamu</td>\n",
       "      <td>114</td>\n",
       "      <td>https://medium.com/@faridaadamu/hello-guys-goo...</td>\n",
       "      <td>Hello guys, good whatever-time-of-day it is in...</td>\n",
       "      <td>Introduction to Machine Learning - Farida Adam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kim Larsen</td>\n",
       "      <td>149</td>\n",
       "      <td>https://towardsdatascience.com/fire-your-bi-te...</td>\n",
       "      <td>The term “Business Intelligence” gained widesp...</td>\n",
       "      <td>Fire Your BI Team - Towards Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dana Alibrandi</td>\n",
       "      <td>459</td>\n",
       "      <td>https://blog.dash.org/announcing-the-release-o...</td>\n",
       "      <td>Today, Dash Core Group is proud to announce ou...</td>\n",
       "      <td>Announcing the Release of Dash Platform on Evo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  claps                                               link  \\\n",
       "0          Eden Au    342  https://towardsdatascience.com/4-numpy-tricks-...   \n",
       "1  Dominik Vacikar     28  https://medium.com/@chichikid/the-guys-who-wil...   \n",
       "2     Farida Adamu    114  https://medium.com/@faridaadamu/hello-guys-goo...   \n",
       "3       Kim Larsen    149  https://towardsdatascience.com/fire-your-bi-te...   \n",
       "4   Dana Alibrandi    459  https://blog.dash.org/announcing-the-release-o...   \n",
       "\n",
       "                                                text  \\\n",
       "0  NumPy is one of the most popular libraries in ...   \n",
       "1  Two weeks ago, I was sitting in a coffeeshop w...   \n",
       "2  Hello guys, good whatever-time-of-day it is in...   \n",
       "3  The term “Business Intelligence” gained widesp...   \n",
       "4  Today, Dash Core Group is proud to announce ou...   \n",
       "\n",
       "                                               title  \n",
       "0  4 NumPy Tricks Every Python Beginner should Le...  \n",
       "1  The Guys Who Will Solve the Private Market - D...  \n",
       "2  Introduction to Machine Learning - Farida Adam...  \n",
       "3           Fire Your BI Team - Towards Data Science  \n",
       "4  Announcing the Release of Dash Platform on Evo...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list of panda pkl to text file\n",
    "def convertPklToTxt(paths:list):\n",
    "    texts,titles = [],[]\n",
    "    for path in paths:\n",
    "        df = pd.read_pickle(path)\n",
    "        #clean up\n",
    "        #df['publication'] = \n",
    "        #df['title'] = \n",
    "        all_string = df['title'].copy(deep=True)\n",
    "        df['title'] = [x[0] for x in all_string.str.split(' - ')]\n",
    "        df['publication'] = [x[1] if len(x)>1 else None for x in all_string.str.split(' - ')]\n",
    "        df['text'] = df['text'].str.replace('\\nWritten by\\n',\"\\n\")\n",
    "        texts.append(df['text'])\n",
    "        titles.append(df['title'])\n",
    "    pd.concat(texts).to_csv('data/all_texts.txt',header=False,index=False)\n",
    "    pd.concat(titles).to_csv('data/all_titles.txt',header=False,index=False)\n",
    "    \n",
    "def get_total_number_of_articles(paths:list):\n",
    "    dfs = []\n",
    "    for path in paths:\n",
    "        df = pd.read_pickle(path)\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertPklToTxt(['data/raw_medium_articles_2015.pkl','data/raw_medium_articles_2016.pkl','data/raw_medium_articles_2017.pkl','data/raw_medium_articles_2018.pkl','data/raw_medium_articles_2019.pkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3860\n"
     ]
    }
   ],
   "source": [
    "get_total_number_of_articles(['data/raw_medium_articles_2015.pkl','data/raw_medium_articles_2016.pkl','data/raw_medium_articles_2017.pkl','data/raw_medium_articles_2018.pkl','data/raw_medium_articles_2019.pkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get('https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c')\n",
    "soup = BeautifulSoup(data.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.findAll('title')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claps  = soup.findAll('button')\n",
    "#unicodedata.normalize('NFKD', claps)\n",
    "for string in claps:\n",
    "    if 'claps' in string.get_text():\n",
    "        print(string.get_text().split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6224"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/raw_medium_articles_2017.pkl')\n",
    "df['title2']= [x[0] for x in df['title'].str.split(' - ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"Hi, I'm bob. What's your name? Don't have one? That's ok too!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "start_date = date(2013, 1, 1)\n",
    "end_date = date(2015, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for single_date in daterange(start_date, date.today()):\n",
    "    print(single_date.strftime(\"%Y/%m/%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
